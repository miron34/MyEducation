{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d701824e-53e4-42bd-b458-7e4b5b219088",
   "metadata": {},
   "source": [
    "# Наивный байесовский классификатор\n",
    "\n",
    "$$P(y | x_1,x_2,...,x_n) = \\frac{P(y) P(x_1,x_2,...,x_n|y)}{P(x_1,x_2,...,x_n)} $$\n",
    "\n",
    "В силу 'наивного' предположения о независимости признаков $x_1,x_2,..,x_n$ получаем:\n",
    "$$ P(y | x_1,x_2,...,x_n) = \\frac{P(y) \\prod\\limits_{i=1}^{n}P(x_i| y)}{P(x_1,x_2,...,x_n)} $$\n",
    "\n",
    "Откуда следует, что\n",
    "\n",
    "$$ P(y | x_1,x_2,...,x_n) \\propto P(y) \\prod\\limits_{i=1}^{n}P(x_i| y) $$\n",
    "\n",
    "И тогда:\n",
    "\n",
    "$$\\hat{y} = arg \\max_{y} P(y | x) $$\n",
    "\n",
    "$$\\hat{y} = arg \\max_{y} P(y) \\prod\\limits_{i=1}^{n}P(x_i| y) $$\n",
    "\n",
    "Поскольку мы ищем $argmax()$, то ничего не мешает нам логарифмировать выражение для более простого рассчета:\n",
    "\n",
    "$$\\hat{y} = arg \\max_{y} (\\ln(P(y)) + \\sum\\limits_{i=1}^{n} \\ln(P(x_i| y)) ) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e307b480-c62b-470b-9c04-7b571a0dc9d6",
   "metadata": {},
   "source": [
    "Вероятность $P(y)$ считается, исходя из частотных характеристик $y_{train}$: \n",
    "$$ P(y_{i}) = \\frac{count(y_{i})}{count(y_{train})}$$\n",
    "\n",
    "Но при рассчете условных вероятностей $P(x_i| y)$ будем учитывать 2 вещи. \\\n",
    "Во-первых: \n",
    "$$ P(x_i | y) = P(x_i =1 | y) x_i + (1 - P(x_i =1 | y)) (1 - x_i), $$\n",
    "Во-вторых, каждая условная вероятность $ p_y = P(x_i =1 | y)$ считается с учетом аддитивного сглаживания (сглаживания Лапласса). \\\n",
    "Поскольку мы не должны допустить возможность деления на 0:\n",
    "$$ p_y =  \\frac{\\alpha+\\sum_{i=1}^n \\mathbb{1}\\{y_i = y\\}x_{i,j}}{2\\alpha + \\sum_{i=1}^n\\mathbb{1}\\{y_i = y\\}}$$\n",
    "\n",
    "https://github.com/esokolov/ml-course-hse/blob/master/2021-spring/seminars/sem16-bayes.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5493fcb2-f1ef-4674-8e98-69a7a92d7ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups, make_classification\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81a09786-c922-4a1f-8139-8a120b838921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 156 ms\n",
      "Wall time: 157 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# мы выбрали такой датасет, поскольку BernoulliNaiveBayes используется:\n",
    "# - с положительными признаками (частотность слов всегда положительна)\n",
    "# - с бинарными признаками (ну тут без этого :D )\n",
    "data = fetch_20newsgroups(subset='all', data_home='data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb9f4048-d4af-4c24-baa4-79300cbf9197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training examples: (15076, 7649)\n",
      "# test examples: (3770, 7649)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['data'], data['target'],\n",
    "                                                   test_size = 0.2,\n",
    "                                                   stratify = data['target'],\n",
    "                                                   random_state=42)\n",
    "\n",
    "count = CountVectorizer(binary=True, lowercase=True, max_df=500, min_df=30)\n",
    "X_train = count.fit_transform(X_train)\n",
    "X_test = count.transform(X_test)\n",
    "\n",
    "print('# training examples:', X_train.shape)\n",
    "print('# test examples:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc915f4-fb8e-4636-a2f6-36885f013679",
   "metadata": {},
   "source": [
    "### Задание\n",
    "\n",
    "Необходимо реализовать наивный байесовский классификтор для нормального распределения.\n",
    "Сам код необходимо оформить и отправить боту в виде класса MyBernoulliNBClassifier в файле seminar03.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7608bd7-a1b0-4d72-9e14-344be353c5bb",
   "metadata": {},
   "source": [
    "Ваша реализация дожна поддерживать методы predict, predict_proba, score аналоично методам класса sklearn.naive_bayes.BernoulliNB при alpha=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c6e8b99-c6e8-4941-bccd-cb27063d57e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBernoulliNBClassifier:\n",
    "    def __init__(self, alpha=1, priors=None):\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # кол-во встречаний каждого таргетаcount number of occurrences for each label\n",
    "        y_counts = np.unique(y, return_counts=True)[1]\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        self.n_features = X.shape[1]\n",
    "        \n",
    "        # вычисляем P(y) - вероятность наблюдения каждого из таргетов\n",
    "        class_prior = y_counts / y_counts.sum()\n",
    "        self.log_class_prior = np.expand_dims(np.log(class_prior), axis = 1)\n",
    "        \n",
    "        # вычисляем P(x|y) - вероятность признака `х` при фикс таргете `у`\n",
    "        prob_x_given_y = np.zeros([self.n_classes, self.n_features])\n",
    "        \n",
    "        for i in range(self.n_classes):\n",
    "            \n",
    "            # берем объекты из `Х` с выбранным таргетом\n",
    "            row_mask = (y == i) \n",
    "            X_filtered = X[row_mask, :]\n",
    "            \n",
    "            # числитель: находим сумму по каждому признаку - `alpha` для сглаживания ( P(x and y) )\n",
    "            numerator = (X_filtered.sum(axis = 0) + self.alpha)          # точно ли тут сумма, а не count?\n",
    "            \n",
    "            # знаменатель: количество объектов с таргетом `y` ( P(y) )\n",
    "            denominator = (X_filtered.shape[0] + 2 * self.alpha)  \n",
    "            prob_x_given_y[i, :] = numerator / denominator\n",
    "        \n",
    "        # вычисляем логарифм вероятностей для P(x|y) and P(~x|y)\n",
    "        self.log_class_conditional_positive = np.log(prob_x_given_y)     # k x n matrix\n",
    "        self.log_class_conditional_negative = np.log(1 - prob_x_given_y) # k x n matrix\n",
    "    \n",
    "    def _joint_likelihoods(self, X):\n",
    "        if scipy.sparse.issparse(X):\n",
    "            X = X.todense()                                                   # m x d matrix\n",
    "        # вычисляем оба слагаемых в первом условии рассчета P(xi|y), затем находим его\n",
    "        log_probs_positive = self.log_class_conditional_positive.dot(X.T)     # n x m matrix\n",
    "        log_probs_negative = self.log_class_conditional_negative.dot(1 - X.T) # n x m matrix        \n",
    "        log_likelihoods = log_probs_positive + log_probs_negative             # n x m matrix\n",
    "        log_joint_likelihoods = (log_likelihoods + self.log_class_prior).T    # n x m matrix\n",
    "        return np.asarray(log_joint_likelihoods)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        jll = self._joint_likelihoods(X)\n",
    "        return self.classes_[np.argmax(jll, axis=1)]\n",
    "\n",
    "    def predict_log_proba(self, X):\n",
    "        jll = self._joint_likelihoods(X)\n",
    "        # нормализауем забытым знаменателем P(x) = P(f_1, ..., f_n)\n",
    "        log_prob_x = scipy.special.logsumexp(jll, axis=1)\n",
    "        return jll - np.atleast_2d(log_prob_x).T\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return np.exp(self.predict_log_proba(X))\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return accuracy_score(y, self.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88b30d23-7945-4769-8bc6-18c2a98b8ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.8010610079575596\n",
      "macro f1 score: 0.8046815233397144\n"
     ]
    }
   ],
   "source": [
    "# предсказания нашей модели\n",
    "bnb = MyBernoulliNBClassifier(alpha=1)\n",
    "bnb.fit(X_train, y_train)\n",
    "y_pred = bnb.predict(X_test)\n",
    "y_pred_proba = bnb.predict_proba(X_test)\n",
    "y_pred_log_proba = bnb.predict_log_proba(X_test)\n",
    "\n",
    "print('test accuracy:', bnb.score(X_test, y_test))\n",
    "print('macro f1 score:', f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db0da74e-4bcf-4ef5-b46d-e1f50601f34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.8010610079575596\n",
      "macro f1 score: 0.8046815233397144\n"
     ]
    }
   ],
   "source": [
    "# предсказания склерн-модели\n",
    "clf = BernoulliNB(alpha=1, force_alpha=True)\n",
    "clf.fit(X_train, y_train)\n",
    "check = clf.predict(X_test)\n",
    "check_proba = clf.predict_proba(X_test)\n",
    "check_log_proba = clf.predict_log_proba(X_test)\n",
    "\n",
    "print('test accuracy:', clf.score(X_test, y_test))\n",
    "print('macro f1 score:', f1_score(y_test, check, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6955baf6-defb-4544-ab18-458f6f1ae2f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "round_num = 7\n",
    "\n",
    "display((y_pred != check).sum())\n",
    "display((np.round(y_pred_proba, round_num) != np.round(check_proba, round_num)).sum())\n",
    "display((np.round(y_pred_log_proba, round_num) != np.round(check_log_proba, round_num)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26d457f-56eb-4c71-9f99-96573dcdeda4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
