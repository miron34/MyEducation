{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Семинар 6. Семантическая сегментация, детекция."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmJo7IfF_58i"
   },
   "source": [
    "Скачаем и рахархивируем заранее все данные, которые понадобятся на семинаре + установим недостающий модуль"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MY2kFBuI_uK_",
    "outputId": "d7e73f24-db48-438e-d684-b07b34cb3865",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/tc1qo73rrm3gt3m/CARVANA.zip  # Carvana dataset\n",
    "!wget https://www.dropbox.com/s/k886cusbuc1afnq/imagenet-mini.zip  # mini image-net dataset\n",
    "!wget https://gist.githubusercontent.com/yrevar/942d3a0ac09ec9e5eb3a/raw/238f720ff059c1f82f368259d1ca4ffa5dd8f9f5/imagenet1000_clsidx_to_labels.txt  # классы имаджнета\n",
    "!unzip -q CARVANA.zip\n",
    "!unzip -q imagenet-mini.zip\n",
    "!rm -rf ./train/.DS_Store\n",
    "!rm -rf ./train_masks/.DS_Store\n",
    "!pip install colour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7upRqguxAo90",
    "outputId": "169b4058-db96-443a-db2f-56a9cb92964a"
   },
   "outputs": [],
   "source": [
    "!ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubc0wbOl_iUQ"
   },
   "source": [
    "<h2>Часть 1. Семантическая сегментация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jq_ppf6t_iUQ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from os.path import isfile, join\n",
    "from typing import Dict, List, Tuple, Union\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from colour import Color\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision import datasets, models, transforms \n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ff_VyjsEaym"
   },
   "source": [
    "<h> В этой части семинара мы напишем свой U-net с нуля, без мам, пап и кредитов,\n",
    "обучим и попробуем его на задачу распознавания точной маски автомобиля"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_jCFjB2_iUT"
   },
   "outputs": [],
   "source": [
    "class Carvana(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        transform: transforms.Compose = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        :param root: путь к папке с данными\n",
    "        :param transform: transforms of the images and labels\n",
    "        \"\"\"\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.transform = transform\n",
    "        (self.data_path, self.labels_path) = ([], [])\n",
    "\n",
    "        def load_images(path: str) -> List[str]:\n",
    "            \"\"\"\n",
    "            Возвращает список с путями до всех изображений\n",
    "\n",
    "            :param path: путь к папке с данными\n",
    "            :return: лист с путями до всех изображений\n",
    "            \"\"\"\n",
    "            images_dir = [join(path, f) for f in os.listdir(path)\n",
    "                          if isfile(join(path, f))]\n",
    "            images_dir.sort()\n",
    "\n",
    "            return images_dir\n",
    "\n",
    "        self.data_path = load_images(self.root + \"/train\")\n",
    "        self.labels_path = load_images(self.root + \"/train_masks\")\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        :param index: sample index\n",
    "        :return: tuple (img, target) with the input data and its label\n",
    "        \"\"\"\n",
    "        img = Image.open(self.data_path[index])\n",
    "        target = Image.open(self.labels_path[index])\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            target = self.transform(target)\n",
    "            target = (target > 0).float()\n",
    "\n",
    "        return (img, target)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_path)\n",
    "\n",
    "    \n",
    "def im_show(img_list: List[Tuple[str, str]]) -> None:\n",
    "    \"\"\"\n",
    "    Plots images with correspinding segmentation mask\n",
    "    \n",
    "    :param img_list: list of paths to images\n",
    "    \"\"\"\n",
    "\n",
    "    to_PIL = transforms.ToPILImage()\n",
    "    if len(img_list) > 9:\n",
    "        raise Exception(\"len(img_list) must be smaller than 10\")\n",
    "        \n",
    "    fig, axes = plt.subplots(len(img_list), 2, figsize=(16, 16))\n",
    "    fig.tight_layout()\n",
    "\n",
    "    for (idx, sample) in enumerate(img_list):\n",
    "        a = axes[idx][0].imshow(np.array(to_PIL(sample[0])))\n",
    "        b = axes[idx][1].imshow(np.array(to_PIL(sample[1])))\n",
    "        for ax in axes[idx]:\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "            \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLiA3F2SMQiA"
   },
   "source": [
    "Загрузим датасет при помощи определенного выше класса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eVBdhAWMLlzy"
   },
   "outputs": [],
   "source": [
    "train_dataset = Carvana(\n",
    "    root=\".\",\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FKUcQZpLMTy_"
   },
   "source": [
    "Посмотрим несколько примеров изображений и масок автомобиля"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2VIj0iLOLou-",
    "outputId": "87886f75-29c0-4bee-8902-a6001cfab05b"
   },
   "outputs": [],
   "source": [
    "img_list = []\n",
    "for i in range(4):\n",
    "    img, label = train_dataset[np.random.randint(0, 100)]\n",
    "    img_list.append((img, label))\n",
    "\n",
    "im_show(img_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LKLjoUgsLqr1"
   },
   "source": [
    "Дополните недостающий код таким образом, чтобы получилась архитектура U-net сети (https://arxiv.org/pdf/1505.04597.pdf). \n",
    "Обратите внимание, что при проходе \"вниз\" количество каналов каждого блока __увеличивается в два раза__. \n",
    "Ситуация с проходом \"вверх\" противоположна, количество каналлов __уменьшается вдвое__. \n",
    "Также, начинаем мы __не с 64 каналов__, как на схеме ниже, __а с 16 каналов__. \n",
    "\n",
    "При проходе вниз в нашей реализации предлагается __дойти до 128 каналов__, чтобы сэкономить время обучения, в оригинальной статье было до 1024. (2 блока, вместо 4х)\n",
    "В целом, при отличии количества каналлов от указанных выше чисел, модель все равно будет работать, но для удобства проведения семинара лучше всем договориться об одних числах.\n",
    "\n",
    "<img src=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png\">\n",
    "\n",
    "Подсказка: каждый блок по пути \"вниз\" представляет из себя двойную свертку с батч нормом и активацией (при создании `Relu` слоя не забывайте `inplace=True`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j-8ltVJe_iUV"
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, kernel_size=3, padding=1, stride=1):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv =  # <your code here>\n",
    "        self.bn = nn.BatchNorm2d(out_size)\n",
    "        self.relu =  # <your code here>\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))\n",
    "\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.down_1 = nn.Sequential(\n",
    "            ConvBlock(3, 16),\n",
    "            ConvBlock(16, 32, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "        self.down_2 =  # <your code here>\n",
    "\n",
    "        self.middle = ConvBlock(128, 128, kernel_size=1, padding=0)\n",
    "\n",
    "        self.up_2 =  # <your code here>\n",
    "\n",
    "        self.up_1 =  # <your code here>\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            ConvBlock(32, 16),\n",
    "            ConvBlock(16, 1, kernel_size=1, padding=0)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        down1 =  # <your code here>\n",
    "        out = F.max_pool2d(down1, kernel_size=2, stride=2)\n",
    "\n",
    "        down2 =  # <your code here>\n",
    "        out = F.max_pool2d(down2, kernel_size=2, stride=2)\n",
    "\n",
    "        out = self.middle(out)\n",
    "\n",
    "        out = nn.functional.interpolate(out, scale_factor=2)  # интерполяцией увеличиваем размер фильтра вдвое\n",
    "        out = torch.cat([down2, out], 1)\n",
    "        out = self.up_2(out)\n",
    "\n",
    "        out =  # <your code here>\n",
    "        out =  # <your code here>\n",
    "        out =  # <your code here>\n",
    "\n",
    "        out = nn.functional.interpolate(out, scale_factor=2)\n",
    "        return self.output(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tE58rn_ZMohS"
   },
   "source": [
    "Определяем функцию для обучения одной эпохи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b5ZF0LeS_iUb"
   },
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, epoch, num_epochs, device):\n",
    "    model.train()\n",
    "\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for i, (images, labels) in pbar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = torch.sigmoid(model(images))\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        accuracy = ((outputs > 0.5) == labels).float().mean()\n",
    "            \n",
    "        pbar.set_description(\n",
    "            f\"Loss: {round(loss.item(), 4)} \"\n",
    "            f\"Accuracy: {round(accuracy.item() * 100, 4)}\"\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4RnYf1lS_iUd",
    "outputId": "4d929f03-4843-4634-95d4-c29a105d0119",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "model = Unet().to(device)\n",
    "criterion = torch.nn.BCELoss()  # https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html\n",
    "optimizer = torch.optim.RMSprop(\n",
    "    model.parameters(),\n",
    "    weight_decay=1e-4,\n",
    "    lr=1e-4,\n",
    "    momentum=0.9\n",
    ")\n",
    "\n",
    "num_epochs = 1\n",
    "for epoch in range(0, num_epochs):\n",
    "    train(train_loader, model, criterion, epoch, num_epochs, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G07YWsb2FDff"
   },
   "source": [
    "Посмотрим на результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5aytNIaaDa4S",
    "outputId": "757aa449-0c67-4f8e-ba69-c460902f374d"
   },
   "outputs": [],
   "source": [
    "img_list = []\n",
    "for i in range(4):\n",
    "    img, label = train_dataset[np.random.randint(0, 100)]\n",
    "    pred = model(img.unsqueeze(0).cuda()).detach().squeeze(0).cpu()\n",
    "    img_list.append((img, pred))\n",
    "\n",
    "im_show(img_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVvff-qgkdvT"
   },
   "source": [
    "Еще несколько эпох и никакие гринскрины нужны не будут..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DjnzaZ9vNqX7"
   },
   "outputs": [],
   "source": [
    "# освободим память\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sr09Rv6F_iUf"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQv54rfdELFv"
   },
   "source": [
    "<h2>Часть 2. mean Average Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBJgNh0WELFv"
   },
   "source": [
    "Для определения метрики mean Average Precision понадобятся проделать небольшой путь и вспомнить пару понятий."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHuMyUHKELFw"
   },
   "source": [
    "Вспоминаем второй класс ИАДа:\n",
    "<br>\n",
    "$$ Precision = {TP \\over TP + FP} $$\n",
    "<br>\n",
    "$$ Recall = {TP \\over TP + FN} $$\n",
    "<br>\n",
    "<br>TP - True Positive\n",
    "<br>FP - False Positive\n",
    "<br>FN - False Negative\n",
    "<br> <br> __Precision__ - доля объектов, названных классификатором положительными и при этом действительно являющимися положительными, среди всех объектов, названных классификатором положительными.\n",
    "\n",
    "__Recall__ - доля объектов, названных классификатором положительными и при этом действительно являющимися положительными, среди всех истинно положительных объектов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLB8y0--ELFx"
   },
   "source": [
    "<h4>Вспоминаем лекцию 6:\n",
    "Intersection over Union</h4>\n",
    "<img src=https://www.pyimagesearch.com/wp-content/uploads/2016/09/iou_equation.png>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pI5Y3JDlELFx"
   },
   "source": [
    "В задаче детекции метки TP, FP, FN (чаще всего) выдаются по следующей логике:\n",
    "\n",
    "- метка TP выдается в случае если IoU > 0.5 и класс определен правильно\n",
    "- метка FP выдается в случае если IoU <= 0.5 и/или если Bounding Box'ы дублируются\n",
    "- метка FN выдается если если IoU > 0.5, но неправильно определен класс и/или если бокса нет совсем"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQqzHZmwTTT3"
   },
   "source": [
    "<h3> Чтобы было нагляднее: </h3>\n",
    "\n",
    "* зеленый цвет - истинный бокс и класс\n",
    "* синий - наши предсказания\n",
    "\n",
    "<h4>True Positive </h4> У нас два волка, оба определены своим классом и боксы очевидно имеют IoU больше 0.5.\n",
    "<img src=\"TruePositiveVolks.jpg\">\n",
    "\n",
    "<h4>False Positive </h4> Несмотря на то, что класс определен правильно и бокс в целом выглядит логично на своем месте, IoU слишком мал, поэтому такая детекция получает метку FP.\n",
    "<img src=\"FalsePositiveVolk.jpg\">\n",
    "\n",
    "<h4>False Negative </h4> Потому что несмотря на хорошее пересечение предсказанного бокса с целевым, класс с высокой уверенностью определен неправильно.\n",
    "<img src=\"FalseNegativeVolk.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8TbTIshELFx"
   },
   "source": [
    "<h3> PR-кривая </h3>\n",
    "\n",
    "Далее, для подсчёта mAP, нужно построить PR-кривую. \n",
    "Напомним, что это кривая, у которой по оси Y - значение Precision, а по оси X - значение recall. \n",
    "Эти значения считаются при переборе пороговых вероятностей, начиная с которых объект помечается положительным классом.\n",
    "\n",
    "Для задачи бинарной классификации мы когда-то строили такую кривую, теперь рассмотрим чуть более сложный случай, где у нас три класса: Волк, Лев и Тигр + решается задача детекции, а не классификации.\n",
    "\n",
    "\n",
    "Внизу представлена таблица с игрушечными данными по предсказаниям модели. \n",
    "Допустим из 7 объектов, в датасете у нас только 3 волка. \n",
    "В данном случае мы называли объект \"действительно волком\" если он имел правильный класс и IoU не менее 0.5. \n",
    "Таким образом имеем задачу вида one vs all, где интересующим нас классом будет являться именно \"волк\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwawmoyeELFy"
   },
   "source": [
    "|Номер строки| Уверенность в том что волк (истинный класс)    |IoU не менее 0.5?   |  Precision  |  Recall |\n",
    "|------------|------------------------|----------------------|-------------|---------| \n",
    "|1           |0.92 (Волк)     |   True    |    1.0     |    0.33 |\n",
    "|2           |0.83 (Волк)     |   True    |    1.0     |    0.67 | \n",
    "|3           |0.77 (Волк)     |   False   |    0.67    |    0.67 | \n",
    "|4           |0.71 (Лев)      |   False   |    0.50    |    ...  |\n",
    "|5           |0.67 (Тигр)     |   False   |    0.40    |    .... | \n",
    "|6           |0.54 (Волк)     |   True    |    0.50    |    .... | \n",
    "|7           |0.47 (Тигр)     |   False   |    0.50    |    1.0  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehDXMLbjELFy"
   },
   "source": [
    "<h6>Посчитаем Precision и Recall для порога в 0.9:</h6>\n",
    "Здесь все легко, взяли порог в 0.9 и называем волками всех, у кого уверенность в классе \"Волк\" больше 0.9. Один TP, отсутсвуют FP и два FN (2 и 6 строчки). \n",
    "$$Precision ={  1 \\over 1 + 0} = 1.0$$\n",
    "\n",
    "$$Recall = {1 \\over 1 + 2} = 0.33$$\n",
    "\n",
    "<h6>для порога в 0.8:</h6> \n",
    "Здесь тоже без дополнительных сложностей. С таким порогом, во второй строчке у нас нашелся еще один TP, соотоветственно убавился один FN. Остальное осталось так же.\n",
    "$$Precision ={  2 \\over 2 + 0} = 1.0$$\n",
    "\n",
    "$$Recall = {2 \\over 2 + 1} = 0.67$$\n",
    "\n",
    "<h6>для порога в 0.75:</h6>  \n",
    "А вот при пороге в 0.75 в третьей строчке замечаем, что несмотря на то, что истинный класс действительно \"Волк\", чем мы и называем данный объект - IoU c истинным боксом меньше 0.5, поэтому присваиваем метку FP.\n",
    "$$Precision ={  2 \\over 2 + 1} = 0.67$$\n",
    "\n",
    "$$Recall = {2 \\over 2 + 1} = 0.67$$\n",
    "\n",
    "<h6>Дальше сами :)</h6>\n",
    "<h6>Задание: Посчитать недостающие в таблице значения precision и recall и сравнить с изображенной PR кривой</h6>\n",
    "\n",
    "PR кривая будет вылядеть следуюшим образом:\n",
    "<img src=\"pr_uno.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62fZzJWCELF4"
   },
   "source": [
    "<h3> Монотонная PR-кривая </h3>\n",
    "\n",
    "**Average Precision (AP)** стандартно **определяется как AUC-PR**, то есть как площадь по PR кривой. Из-за того, что Precision и Recall находятся в отрезке от 0 до 1, AP так же определена на этом отрезке. Чем ближе к 1, тем качественнее модель.\n",
    "\n",
    "Для удобства вычислений, и чуть большей устойчивости к перестановке - вместо того, чтобы терпеть возникшую немонотонность, для всех совпадающих значений recall'a берется максимальный справа от текущей точки precision, то есть график изменится следующим образом:\n",
    "\n",
    "<img src=\"pr_dos.png\">\n",
    "\n",
    "Технология та же - для вычисления AP считается AUC под красной кривой:\n",
    "\n",
    "$$AP = 1 * 0.67 + (1 - 0.67) * 0.5 = 0.835 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZlQ8t0jMELF8"
   },
   "source": [
    "<h3>k-point interpolation</h3>\n",
    "\n",
    "В какой-то момент люди решили, что просто площади теперь не круто и в популярном соревновании PASCAL VOC2008 для вычисления Average Precision использовалась 11-point interpolation. \n",
    "\n",
    "По-простому: брались 11 значений монотонной PR функции, в точках 0, 0.1, 0.2, ..., 0.9, 1.0 и усреднялись. <br>Для любителей формул:\n",
    "\n",
    "$$AP = {1 \\over11} * \\sum P(r), r \\in [0.1, 0.2, ..., 0.9, 1.0]$$\n",
    "$P(r)$ - значение Precision при определенном Recall.\n",
    "\n",
    "Графически это все выглядит следующим образом:\n",
    "\n",
    "<img src=\"pr_tres.png\">\n",
    "\n",
    "Посчитаем АР данным способом:\n",
    "$$AP = {1 \\over {11}} (1 * 7 + 3 * 0.5) = 0.77$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0dN1KII-ELF_"
   },
   "source": [
    "<h3>Подсчёт mAP</h3>\n",
    "\n",
    "Для более <font color=\"green\">fresh </font> PASCAl VOC соревнований (2011 - 2012) было принято решение считать **Average Precision** как обычную __площадь под монотонной PR кривой__. Этого определения мы и будем придерживаться далее.\n",
    "\n",
    "Все предыдущие графики, наша первая табличка и значения AP считались для одного класса \"Волк\". Понятно, что подобные значения можно посчитать для каждого класса в выборке. И каждый раз будет принцип one vs all, где различается нужный класс и \"все остальные\"\n",
    "\n",
    "\n",
    "Метрика __mean Average Precison__ считается как среднее между __AP__ каждого класса, те:\n",
    "\n",
    "$$mAP = \\sum_{c \\in C} AP(c)$$\n",
    "где С - множество классов.\n",
    "\n",
    "<br>Ниже - пример того как считать среднее :))\n",
    "\n",
    "*для Average Precision льва и тигра значения взяты случайно\n",
    "\n",
    "|AP(Волк)| AP(Лев) | AP(Тигр)|mAp    |\n",
    "|--------|---------|---------|-------|\n",
    "|0.835   |0.83     | 0.77    | 0.81  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQAG0kBGELF_"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JiIqDhk_ELGA"
   },
   "source": [
    "<h2>Часть 3. Детекция объектов на изображениях</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сегодня на семинаре мы попробуем обучить модель детекции. \n",
    "Мы не будем полностью реализовывать архитектуру сети, а возьмем предобученную Faster R-CNN из pytorch и сделаем fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icjhclUNIgCk"
   },
   "source": [
    "Для начала загрузим данные. Мы будем пользоваться датасетом [Penn-Fudan](https://www.cis.upenn.edu/~jshi/ped_html/). Он содержит 170 изображений с разметкой сегментации и детекции. Мы воспользуемся последней"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NaGQea5hIgCn",
    "outputId": "2b25078d-617d-4ccc-cec2-cd4ac140d703"
   },
   "outputs": [],
   "source": [
    "! curl https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip > PennFudanPed.zip\n",
    "! unzip -q PennFudanPed.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZtNWxqoMESqw"
   },
   "outputs": [],
   "source": [
    "from PIL import ImageDraw\n",
    "from sklearn.metrics import auc\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U1g32XYFFYTb"
   },
   "outputs": [],
   "source": [
    "class PennFudanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root: str, transforms_list: transforms = None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms_list\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # note that we haven't converted the mask to RGB,\n",
    "        # because each color corresponds to a different instance\n",
    "        # with 0 being background\n",
    "        mask = Image.open(mask_path)\n",
    "        mask = np.array(mask)\n",
    "        \n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = np.unique(mask)\n",
    "        \n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "\n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            \n",
    "        # convert everything into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        \n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "\n",
    "        target = {\"boxes\": boxes, \"labels\": labels, \"image_id\": image_id}\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "\n",
    "def get_transform(train=False):\n",
    "    transforms_list = []\n",
    "    if train:\n",
    "        transforms.append(transforms.RandomHorizontalFlip(0.5))\n",
    "    transforms_list.append(transforms.ToTensor())\n",
    "    \n",
    "    return transforms.Compose(transforms_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aYRhV4hfGpq9"
   },
   "outputs": [],
   "source": [
    "dataset = PennFudanDataset(\"PennFudanPed/\", get_transform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 553
    },
    "id": "2PJGAZYs6mQE",
    "outputId": "1e91df1b-127b-43ca-c387-4639ac092220"
   },
   "outputs": [],
   "source": [
    "image, labels = next(iter(dataset))\n",
    "image = transforms.ToPILImage()(image)\n",
    "\n",
    "draw = ImageDraw.Draw(image)\n",
    "for box in labels[\"boxes\"]:\n",
    "    draw.rectangle([(box[0], box[1]), (box[2], box[3])])\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VABmgMQ3IgDI"
   },
   "source": [
    "Теперь подгрузим модель. Среди моделей детекции в pytorch есть только `Faster R-CNN`. Так как мы детектим только 2 класса - пешеходов и фон, нужно изменить выходной слой, предсказывающий классы изображений. Для этого в torchvision есть блок `FastRCNNPredictor`.\n",
    "\n",
    "<h4>Схема модели Fast R-CNN</h4>\n",
    "<img src=https://pytorch.org/tutorials/_static/img/tv_tutorial/tv_image03.png>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kSUjeN0M7EWk"
   },
   "outputs": [],
   "source": [
    "def get_detection_model(num_classes=2):\n",
    "    model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    \n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBvGLhxOIgDP"
   },
   "source": [
    "Теперь давайте напишем подсчет IoU, которая пригодится нам для отбора кандидатов.\n",
    "\n",
    "<h4>Intersection over Union </h4>\n",
    "<img src=https://www.pyimagesearch.com/wp-content/uploads/2016/09/iou_equation.png>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fx3qsM_fIgDP"
   },
   "outputs": [],
   "source": [
    "def intersection_over_union(dt_bbox: np.array, gt_bbox: np.array) -> float:\n",
    "    \"\"\"\n",
    "    Intersection over Union between two bboxes\n",
    "    (x0, y0) - coordinates of the upper left bbox corner\n",
    "    (x1, y1) - coordinates of the down right bbox corner\n",
    "    \n",
    "    :param dt_bbox: list or numpy array of size (4,) [x0, y0, x1, y1]\n",
    "    :param gt_bbox: list or numpy array of size (4,) [x0, y0, x1, y1]\n",
    "    :return : intersection over union\n",
    "    \"\"\"\n",
    "    # <your code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mV4wTZpMNotf",
    "outputId": "01e21e5b-c601-4bae-aaf8-b7b69d8faf91"
   },
   "outputs": [],
   "source": [
    "dt_bbox = [0, 0, 2, 2]\n",
    "gt_bbox = [1, 1, 3, 3]\n",
    "intersection_over_union(dt_bbox, gt_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Pkva3QFDDed"
   },
   "outputs": [],
   "source": [
    "def evaluate_sample(target_pred: torch.Tensor, target_true, iou_threshold=0.5):\n",
    "    # правильные прямоугольники\n",
    "    gt_bboxes = target_true[\"boxes\"].numpy()\n",
    "    gt_labels = target_true[\"labels\"].numpy()\n",
    "\n",
    "    # предсказания модели\n",
    "    dt_bboxes = target_pred[\"boxes\"].numpy()\n",
    "    dt_labels = target_pred[\"labels\"].numpy()\n",
    "    dt_scores = target_pred[\"scores\"].numpy()\n",
    "\n",
    "    results = []\n",
    "    # для каждого прямоугольника из предсказания находим максимально близкий прямоугольник среди ответов\n",
    "    for detection_id in range(len(dt_labels)):\n",
    "        dt_bbox = dt_bboxes[detection_id, :]\n",
    "        dt_label = dt_labels[detection_id]\n",
    "        dt_score = dt_scores[detection_id]\n",
    "\n",
    "        detection_result_dict = {\"score\": dt_score}\n",
    "\n",
    "        max_IoU = 0\n",
    "        max_gt_id = -1\n",
    "        for gt_id in range(len(gt_labels)):\n",
    "            gt_bbox = gt_bboxes[gt_id, :]\n",
    "            gt_label = gt_labels[gt_id]\n",
    "\n",
    "            if gt_label != dt_label:\n",
    "                continue\n",
    "\n",
    "            if intersection_over_union(dt_bbox, gt_bbox) > max_IoU:\n",
    "                max_IoU = intersection_over_union(dt_bbox, gt_bbox)\n",
    "                max_gt_id = gt_id\n",
    "\n",
    "        if max_gt_id >= 0 and max_IoU >= iou_threshold:\n",
    "            # для прямоугольника detection_id нашли правильный ответ, который имеет IoU больше 0.5 \n",
    "            detection_result_dict[\"TP\"] = 1\n",
    "            # удаляем эти прямоугольники из данных, чтобы больше не матчить с ними\n",
    "            gt_labels = np.delete(gt_labels, max_gt_id, axis=0)\n",
    "            gt_bboxes = np.delete(gt_bboxes, max_gt_id, axis=0)\n",
    "\n",
    "        else:\n",
    "            detection_result_dict[\"TP\"] = 0\n",
    "\n",
    "        results.append(detection_result_dict)\n",
    "\n",
    "    # возвращаем результат, для кажого прямоугольника говорим, смогли ли сматчить его с чем то из ответов\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "roHlrGtVIgDV"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, device):\n",
    "    results = []\n",
    "    model.eval()\n",
    "    nbr_boxes = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (images, targets_true) in enumerate(test_loader):\n",
    "            images = list(image.to(device).float() for image in images)\n",
    "            targets_pred = model(images)\n",
    "\n",
    "            targets_true = [{k: v.cpu().float() for k, v in t.items()} for t in targets_true]\n",
    "            targets_pred = [{k: v.cpu().float() for k, v in t.items()} for t in targets_pred]\n",
    "\n",
    "            for i in range(len(targets_true)):\n",
    "                target_true = targets_true[i]\n",
    "                target_pred = targets_pred[i]\n",
    "                nbr_boxes += target_true[\"labels\"].shape[0]\n",
    "\n",
    "                # матчим ответы с правильными боксами\n",
    "                results.extend(evaluate_sample(target_pred, target_true))\n",
    "\n",
    "    results = sorted(results, key=lambda k: k[\"score\"], reverse=True)\n",
    "\n",
    "    # считаем точность и полноту, чтобы потом посчитать mAP как auc\n",
    "    acc_TP = np.zeros(len(results))\n",
    "    acc_FP = np.zeros(len(results))\n",
    "    recall = np.zeros(len(results))\n",
    "    precision = np.zeros(len(results))\n",
    "\n",
    "    if results[0][\"TP\"] == 1:\n",
    "        acc_TP[0] = 1\n",
    "    else:\n",
    "        acc_FP[0] = 1\n",
    "\n",
    "    for i in range(1, len(results)):\n",
    "        acc_TP[i] = results[i][\"TP\"] + acc_TP[i - 1]\n",
    "        acc_FP[i] = (1 - results[i][\"TP\"]) + acc_FP[i - 1]\n",
    "\n",
    "        precision[i] = acc_TP[i] / (acc_TP[i] + acc_FP[i])\n",
    "        recall[i] = acc_TP[i] / nbr_boxes\n",
    "\n",
    "    return auc(recall, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fsqKh0gaIgDa"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, data_loader, device):\n",
    "    model.train()\n",
    "    global_loss = 0\n",
    "    for images, targets in tqdm(data_loader, leave=False, desc=\"Batch number\"):\n",
    "        images = list(image.to(device).float() for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        dict_loss = model(images, targets)\n",
    "        losses = sum(loss for loss in dict_loss.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        global_loss += float(losses.cpu().detach().numpy())\n",
    "\n",
    "    return global_loss\n",
    "\n",
    "\n",
    "def train(model, train_loader, test_loader, optimizer, device, num_epochs=5):\n",
    "    for epoch in trange(num_epochs, leave=True, desc=f\"Epoch number\"):\n",
    "        train_one_epoch(model, optimizer, train_loader, device)\n",
    "        mAP = evaluate(model, test_loader, device=device)\n",
    "\n",
    "        print(f\"mAP after epoch {epoch + 1} is {mAP:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PEzKc4eR9BHn"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "dataset = PennFudanDataset(\"PennFudanPed\", get_transform(train=False))\n",
    "\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "\n",
    "dataset_train = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "data_loader_train = torch.utils.data.DataLoader(\n",
    "    dataset_train, \n",
    "    batch_size=4, \n",
    "    shuffle=True, \n",
    "    pin_memory=True,\n",
    "    num_workers=4, \n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "dataset_test = torch.utils.data.Subset(dataset, indices[-50:])\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, \n",
    "    batch_size=1, \n",
    "    shuffle=False, \n",
    "    num_workers=4, \n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 868,
     "referenced_widgets": [
      "0dff8103f53b4b38adb088af2f2d8956",
      "29bb1547ec9545ce85f339693c2af46b",
      "4f90da335c9a4cbf8d10fe1dc3c78813",
      "13277a01bbb94b6ca6c24d60bf29fcfd",
      "1788c9caf91b4c2bb8d2ef0d291f4b14",
      "7b7e261e948d4bfbb25df72521f25331",
      "12f7d44ba70b4b8fb9113164bb597fd9",
      "88b26b728294483681449ffba56cac49"
     ]
    },
    "id": "0vpF4zUbIgDt",
    "outputId": "ebbf2160-74bb-4d7e-dc56-7d8ad3d332aa"
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model = get_detection_model(num_classes)\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "    \n",
    "train(model, data_loader_train, data_loader_test, optimizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0DlzqfsIgDv"
   },
   "source": [
    "Давайте посмотрим, как наша модель научилась выделять людей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "id": "lZ8_iGQzIgDx",
    "outputId": "c7ec4729-846f-441b-9d45-8fa5eab2b4fd"
   },
   "outputs": [],
   "source": [
    "image, labels = next(iter(dataset_test))\n",
    "model.eval()\n",
    "pred = model(image.unsqueeze(0).to(device))[0]\n",
    "\n",
    "image = transforms.ToPILImage()(image)\n",
    "\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "for box in labels[\"boxes\"]:\n",
    "    draw.rectangle([(box[0], box[1]), (box[2], box[3])])\n",
    "    \n",
    "for box in pred[\"boxes\"]:\n",
    "    draw.rectangle([(box[0], box[1]), (box[2], box[3])], outline=\"red\")\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qrj1cdioj3I3",
    "outputId": "82d982e3-5452-4633-a19e-24ac25f3f3ec"
   },
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "02VoP-2aMggf",
    "outputId": "c5ed9d3c-f23a-4c78-85dd-16c50ecae0b8"
   },
   "outputs": [],
   "source": [
    "for l_box in labels[\"boxes\"]:\n",
    "    print(intersection_over_union(pred[\"boxes\"][0].cpu().detach().numpy(), l_box.numpy()))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "sem_06_solved.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "16fd80de06cf4fe7ad751626b0464edd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "25f64ca28e40485bbe05267742ff4a57": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_16fd80de06cf4fe7ad751626b0464edd",
      "placeholder": "​",
      "style": "IPY_MODEL_55ce6e52c7c24dad89f4cdaa829f7a46",
      "value": " 507M/507M [00:06&lt;00:00, 76.7MB/s]"
     }
    },
    "3341c0402ee54b198d6e98c503db53bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "383292a7696a4ea6bd705a079b3d4a6e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "55ce6e52c7c24dad89f4cdaa829f7a46": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "66a33a75cd354566a10357147f425d50": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7282de6cafa3432dbcb0c94cc438f32e",
       "IPY_MODEL_25f64ca28e40485bbe05267742ff4a57"
      ],
      "layout": "IPY_MODEL_383292a7696a4ea6bd705a079b3d4a6e"
     }
    },
    "7282de6cafa3432dbcb0c94cc438f32e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_845a8e8552fe43cd8907e2f0f1b53c9c",
      "max": 531503671,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3341c0402ee54b198d6e98c503db53bf",
      "value": 531503671
     }
    },
    "845a8e8552fe43cd8907e2f0f1b53c9c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
