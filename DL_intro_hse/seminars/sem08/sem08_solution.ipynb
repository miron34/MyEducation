{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "sem08_solution.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W8R8WgZceEk"
      },
      "source": [
        "# Character-Level LSTM\n",
        "На этом семинаре поговорим про рекуррентные нейронные сети (Recurrent Neural Networ, RNN). Мы обучим модель на тексте книги \"Анна Каренина\", после чего попробуем генерировать новый текст.\n",
        "\n",
        "**Модель сможет генерировать новый текст на основе текста \"Анны Карениной\"!**\n",
        "\n",
        "Можно посмотреть полезную [статью про RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) и [реализацию в Torch](https://github.com/karpathy/char-rnn). \n",
        "\n",
        "Ообщая архитектура RNN:\n",
        "\n",
        "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/assets/charseq.jpeg?raw=1\" width=\"500\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqUOE2flceEl"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wHfCDyzceEl"
      },
      "source": [
        "## Загрузим данные\n",
        "\n",
        "Загрузим текстовый файл \"Анны Карениной\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b34kfqIOceEl"
      },
      "source": [
        "# open text file and read in data as `text`\n",
        "with open('anna.txt', 'r') as f:\n",
        "    text = f.read()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp1Ljc4mceEl"
      },
      "source": [
        "Посмотрим первые 100 символов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VctmLQfceEl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6907a065-d44b-4a0a-c0a3-2973137dce25"
      },
      "source": [
        "text[:100]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iC21bopceEl"
      },
      "source": [
        "### Токенизация\n",
        "\n",
        "В ячейках ниже создадим два **словаря** для преобразования символов в целые числа и обратно. Кодирование символов как целых чисел упрощает их использование в качестве входных данных в сети."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYVlmnxLceEl"
      },
      "source": [
        "# encode the text and map each character to an integer and vice versa\n",
        "\n",
        "# we create two dictionaries:\n",
        "# 1. int2char, which maps integers to characters\n",
        "# 2. char2int, which maps characters to unique integers\n",
        "chars = tuple(set(text))\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "\n",
        "# encode the text\n",
        "encoded = np.array([char2int[ch] for ch in text])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJIzwzSwceEl"
      },
      "source": [
        "Посмотрим как символы закодировались целыми числами"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WK1MYr_9ceEl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b46ed90c-8162-40f8-b664-d7335a077eb4"
      },
      "source": [
        "encoded[:100]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([28, 29, 45, 56, 54, 43, 37, 77, 65,  1,  1,  1, 25, 45, 56, 56, 47,\n",
              "       77, 38, 45, 74, 30, 64, 30, 43, 63, 77, 45, 37, 43, 77, 45, 64, 64,\n",
              "       77, 45, 64, 30, 21, 43, 51, 77, 43, 72, 43, 37, 47, 77, 70, 79, 29,\n",
              "       45, 56, 56, 47, 77, 38, 45, 74, 30, 64, 47, 77, 30, 63, 77, 70, 79,\n",
              "       29, 45, 56, 56, 47, 77, 30, 79, 77, 30, 54, 63, 77, 46, 32, 79,  1,\n",
              "       32, 45, 47, 34,  1,  1, 66, 72, 43, 37, 47, 54, 29, 30, 79])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azltQy-gceEl"
      },
      "source": [
        "## Предобработка данных\n",
        "\n",
        "Как можно видеть на изображении char-RNN выше, сеть ожидает **one-hot encoded** входа, что означает, что каждый символ преобразуется в целое число (через созданный словарь), а затем преобразуется в вектор-столбец, где только соответствующий ему целочисленный индекс будет иметь значение 1, а остальная часть вектора будет заполнена нулями. Давайте создадим для этого функцию."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnahALhiceEl"
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "    \n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "    \n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "    \n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "    \n",
        "    return one_hot"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3lTdLKfceEl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e5f7db8-4bda-4d4b-86cb-bd3dd442d582"
      },
      "source": [
        "# check that the function works as expected\n",
        "test_seq = np.array([[3, 5, 1]])\n",
        "one_hot = one_hot_encode(test_seq, 8)\n",
        "\n",
        "print(one_hot)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YyL91CuceEl"
      },
      "source": [
        "## Создаем mini-batch'и\n",
        "\n",
        "\n",
        "Создатдим мини-батчи для обучения. На простом примере они будут выглядеть так:\n",
        "\n",
        "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/assets/sequence_batching@1x.png?raw=1\" width=500px>\n",
        "<br>\n",
        "\n",
        "Возьмем закодированные символы (переданные как параметр `arr`) и разделим их на несколько последовательностей, заданных параметром `batch_size`. Каждая из наших последовательностей будет иметь длину `seq_length`.\n",
        "\n",
        "### Создани батчей\n",
        "\n",
        "**1. Первое, что нам нужно сделать, это отбросить часть текста, чтобы у нас были только полные мини-батчи**\n",
        "\n",
        "Каждый батч содержит $ N\\times M $ символов, где $ N $ - это размер батча (количество последовательностей в батче), а $ M $ - длина `seq_length` или количество шагов в последовательности. Затем, чтобы получить общее количество батчей $ K $, которое мы можем сделать из массива `arr`, нужно разделить длину `arr` на количество символов в батче. Когда мы узнаем количество батчей, можно получить общее количество символов, которые нужно сохранить, из `arr`: $ N * M * K $.\n",
        "\n",
        "**2. После этого нам нужно разделить `arr` на $N$ батчей** \n",
        "\n",
        "Это можно сделать с помощью `arr.reshape(size)`, где `size` - это кортеж, содержащий размеры измененного массива. Мы знаем, что нам нужно $ N $ последовательностей в батче, поэтому сделаем его размером первого измерения. Для второго измерения можем использовать «-1» в качестве заполнителя, он заполнит массив соответствующими данными. После этого должен остаться массив $N\\times(M * K)$.\n",
        "\n",
        "**3. Теперь, когда у нас есть этот массив, мы можем перебирать его, чтобы получить наши мини-батчи**\n",
        "\n",
        "Идея состоит в том, что каждая партия представляет собой окно $ N\\times M $ в массиве $ N\\times (M * K) $. Для каждого последующего батча окно перемещается на `seq_length`. Мы также хотим создать как входной, так и выходной массивы. Это окно можно сделать с помощью `range`, чтобы делать шаги размером `n_steps` от $ 0 $ до `arr.shape [1]`, общее количество токенов в каждой последовательности. Таким образом, целые числа, которые получены из диапазона, всегда указывают на начало батча, и каждое окно имеет ширину `seq_length`.\n",
        "\n",
        "> **TODO:** Допишите функцию для создания батчей: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ECftYejnvpx"
      },
      "source": [
        "# ОТВЕТЫ\n",
        "def get_batches(arr, batch_size, seq_length):\n",
        "    '''Create a generator that returns batches of size\n",
        "       batch_size x seq_length from arr.\n",
        "       \n",
        "       Arguments\n",
        "       ---------\n",
        "       arr: Array you want to make batches from\n",
        "       batch_size: Batch size, the number of sequences per batch\n",
        "       seq_length: Number of encoded chars in a sequence\n",
        "    '''\n",
        "    \n",
        "    batch_size_total = batch_size * seq_length\n",
        "    # total number of batches we can make\n",
        "    n_batches = len(arr)//batch_size_total\n",
        "    \n",
        "    # Keep only enough characters to make full batches\n",
        "    arr = arr[:n_batches * batch_size_total]\n",
        "    # Reshape into batch_size rows\n",
        "    arr = arr.reshape((batch_size, -1))\n",
        "    \n",
        "    # iterate through the array, one sequence at a time\n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        # The features\n",
        "        x = arr[:, n:n+seq_length]\n",
        "        # The targets, shifted by one\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9uKOvbqceEl"
      },
      "source": [
        "### Протестируем\n",
        "\n",
        "Теперь создадим несколько наборов данных, и проверим, что происходит, когда мы создаем батчи."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtKlLXi1ceEl"
      },
      "source": [
        "batches = get_batches(encoded, 8, 50)\n",
        "x, y = next(batches)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rg5MUTqqceEl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0965cef5-806e-4339-ad23-949928915384"
      },
      "source": [
        "# printing out the first 10 items in a sequence\n",
        "print('x\\n', x[:10, :10])\n",
        "print('\\ny\\n', y[:10, :10])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x\n",
            " [[28 29 45 56 54 43 37 77 65  1]\n",
            " [63 46 79 77 54 29 45 54 77 45]\n",
            " [43 79 39 77 46 37 77 45 77 38]\n",
            " [63 77 54 29 43 77 62 29 30 43]\n",
            " [77 63 45 32 77 29 43 37 77 54]\n",
            " [62 70 63 63 30 46 79 77 45 79]\n",
            " [77 76 79 79 45 77 29 45 39 77]\n",
            " [49 81 64 46 79 63 21 47 34 77]]\n",
            "\n",
            "y\n",
            " [[29 45 56 54 43 37 77 65  1  1]\n",
            " [46 79 77 54 29 45 54 77 45 54]\n",
            " [79 39 77 46 37 77 45 77 38 46]\n",
            " [77 54 29 43 77 62 29 30 43 38]\n",
            " [63 45 32 77 29 43 37 77 54 43]\n",
            " [70 63 63 30 46 79 77 45 79 39]\n",
            " [76 79 79 45 77 29 45 39 77 63]\n",
            " [81 64 46 79 63 21 47 34 77 68]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_qHIAEIceEl"
      },
      "source": [
        "Если вы правильно реализовали get_batches, результат должен выглядеть примерно так:\n",
        "```\n",
        "x\n",
        " [[25  8 60 11 45 27 28 73  1  2]\n",
        " [17  7 20 73 45  8 60 45 73 60]\n",
        " [27 20 80 73  7 28 73 60 73 65]\n",
        " [17 73 45  8 27 73 66  8 46 27]\n",
        " [73 17 60 12 73  8 27 28 73 45]\n",
        " [66 64 17 17 46  7 20 73 60 20]\n",
        " [73 76 20 20 60 73  8 60 80 73]\n",
        " [47 35 43  7 20 17 24 50 37 73]]\n",
        "\n",
        "y\n",
        " [[ 8 60 11 45 27 28 73  1  2  2]\n",
        " [ 7 20 73 45  8 60 45 73 60 45]\n",
        " [20 80 73  7 28 73 60 73 65  7]\n",
        " [73 45  8 27 73 66  8 46 27 65]\n",
        " [17 60 12 73  8 27 28 73 45 27]\n",
        " [64 17 17 46  7 20 73 60 20 80]\n",
        " [76 20 20 60 73  8 60 80 73 17]\n",
        " [35 43  7 20 17 24 50 37 73 36]]\n",
        " ```\n",
        " хотя точные цифры могут отличаться. Убедитесь, что данные сдвинуты на один шаг для `y`!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jouxv0L2ceEl"
      },
      "source": [
        "---\n",
        "## Зададим архитектуру\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/assets/charRNN.png?raw=1\" width=500px>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7s5eRaoceEl"
      },
      "source": [
        "### Структура модели\n",
        "\n",
        "В `__init__` предлагаемая структура выглядит следующим образом:\n",
        "* Создаваём и храним необходимые словари (уже релизовано)\n",
        "* Определяем слой LSTM, который принимает в качестве параметров: размер ввода (количество символов), размер скрытого слоя `n_hidden`, количество слоев` n_layers`, вероятность drop-out'а `drop_prob` и логическое значение batch_first (True)\n",
        "* Определяем слой drop-out с помощью drop_prob\n",
        "* Определяем полносвязанный слой с параметрами: размер ввода `n_hidden` и размер выхода - количество символов\n",
        "* Наконец, инициализируем веса\n",
        "\n",
        "Обратите внимание, что некоторые параметры были названы и указаны в функции `__init__`, их нужно сохранить и использовать, выполняя что-то вроде` self.drop_prob = drop_prob`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Plm1atCuceEl"
      },
      "source": [
        "---\n",
        "### Входы-выходы LSTM\n",
        "\n",
        "Вы можете создать [LSTM layer](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM) следующим образом\n",
        "\n",
        "```python\n",
        "self.lstm = nn.LSTM(input_size, n_hidden, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "```\n",
        "\n",
        "где `input_siz`e - это количество символов, которые эта ячейка ожидает видеть в качестве последовательного ввода, а `n_hidde`n - это количество элементов в скрытых слоях ячейки. Можно добавить drop-out, добавив параметр `dropout` с заданной вероятностью. Наконец, в функции `forward` мы можем складывать ячейки LSTM в слои, используя `.view`.\n",
        "\n",
        "Также требуется создать начальное скрытое состояние всех нулей:\n",
        "\n",
        "```python\n",
        "self.init_hidden()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlTnDntHceEl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05590da1-3430-4f2f-9bc4-397a4835b77e"
      },
      "source": [
        "# check if GPU is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU!')\n",
        "else: \n",
        "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No GPU available, training on CPU; consider making n_epochs very small.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPq1EA38rBqn"
      },
      "source": [
        "# ОТВЕТЫ\n",
        "class CharRNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
        "                               drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        \n",
        "        # creating character dictionaries\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "        \n",
        "        ## TODO: define the LSTM\n",
        "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        ## TODO: define a dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        ## TODO: define the final, fully-connected output layer\n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "      \n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "        ''' Forward pass through the network. \n",
        "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
        "                \n",
        "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "        \n",
        "        ## TODO: pass through a dropout layer\n",
        "        out = self.dropout(r_output)\n",
        "        \n",
        "        # Stack up LSTM outputs using view\n",
        "        # you may need to use contiguous to reshape the output\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "        \n",
        "        ## TODO: put x through the fully-connected layer\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        # return the final output and the hidden state\n",
        "        return out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "        \n",
        "        return hidden"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IrBRlEPceEl"
      },
      "source": [
        "## Обучим модель\n",
        "\n",
        "Во время обучения нужно установить количество эпох, скорость обучения и другие параметры.\n",
        "\n",
        "Используем оптимизатор Adam и кросс-энтропию, считаем loss и, как обычно, выполняем back propagation.\n",
        "\n",
        "Пара подробностей об обучении:\n",
        "> * Во время цикла мы отделяем скрытое состояние от его истории; на этот раз установив его равным новой переменной * tuple *, потому что скрытое состояние LSTM, является кортежем скрытых состояний.\n",
        "* Мы используем [`clip_grad_norm_`](https://pytorch.org/docs/stable/_modules/torch/nn/utils/clip_grad.html) чтобы избавиться от взрывающихся градиентов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lv8VkRI0ceEl"
      },
      "source": [
        "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "    ''' Training a network \n",
        "    \n",
        "        Arguments\n",
        "        ---------\n",
        "        \n",
        "        net: CharRNN network\n",
        "        data: text data to train the network\n",
        "        epochs: Number of epochs to train\n",
        "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "        seq_length: Number of character steps per mini-batch\n",
        "        lr: learning rate\n",
        "        clip: gradient clipping\n",
        "        val_frac: Fraction of data to hold out for validation\n",
        "        print_every: Number of steps for printing training and validation loss\n",
        "    \n",
        "    '''\n",
        "    net.train()\n",
        "    \n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "        \n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "            \n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "            if(train_on_gpu):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "            \n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "            \n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "            loss.backward()\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "            \n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                    \n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    \n",
        "                    inputs, targets = x, y\n",
        "                    if(train_on_gpu):\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "                \n",
        "                    val_losses.append(val_loss.item())\n",
        "                \n",
        "                net.train() # reset to train mode after iterationg through validation data\n",
        "                \n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt0q4KGEceEm"
      },
      "source": [
        "## Определим модель\n",
        "\n",
        "Теперь мы можем создать модель с заданными гиперпараметрами. Определим размеры мини-батчей."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykMcIloEr3G7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "300411a6-9dc4-4f34-aa4a-267f4769342a"
      },
      "source": [
        "# ОТВЕТЫ\n",
        "# define and print the net\n",
        "n_hidden=512\n",
        "n_layers=2\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHy6mECuceEm"
      },
      "source": [
        "### Установим гиперпараметры"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hTkNrWEsjgI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1c7738d-e3b6-4b03-b98a-c71b57a27954"
      },
      "source": [
        "# ОТВЕТЫ\n",
        "\n",
        "batch_size = 128\n",
        "seq_length = 100\n",
        "n_epochs = 20 # start smaller if you are just testing initial behavior\n",
        "\n",
        "# train the model\n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/20... Step: 10... Loss: 3.2806... Val Loss: 3.2198\n",
            "Epoch: 1/20... Step: 20... Loss: 3.1579... Val Loss: 3.1370\n",
            "Epoch: 1/20... Step: 30... Loss: 3.1429... Val Loss: 3.1244\n",
            "Epoch: 1/20... Step: 40... Loss: 3.1163... Val Loss: 3.1199\n",
            "Epoch: 1/20... Step: 50... Loss: 3.1419... Val Loss: 3.1181\n",
            "Epoch: 1/20... Step: 60... Loss: 3.1205... Val Loss: 3.1166\n",
            "Epoch: 1/20... Step: 70... Loss: 3.1073... Val Loss: 3.1153\n",
            "Epoch: 1/20... Step: 80... Loss: 3.1257... Val Loss: 3.1124\n",
            "Epoch: 1/20... Step: 90... Loss: 3.1234... Val Loss: 3.1056\n",
            "Epoch: 1/20... Step: 100... Loss: 3.0975... Val Loss: 3.0904\n",
            "Epoch: 1/20... Step: 110... Loss: 3.0659... Val Loss: 3.0460\n",
            "Epoch: 1/20... Step: 120... Loss: 2.9597... Val Loss: 2.9701\n",
            "Epoch: 1/20... Step: 130... Loss: 2.8846... Val Loss: 2.8460\n",
            "Epoch: 2/20... Step: 140... Loss: 2.8687... Val Loss: 2.8193\n",
            "Epoch: 2/20... Step: 150... Loss: 2.7384... Val Loss: 2.6945\n",
            "Epoch: 2/20... Step: 160... Loss: 2.6366... Val Loss: 2.5944\n",
            "Epoch: 2/20... Step: 170... Loss: 2.5418... Val Loss: 2.5287\n",
            "Epoch: 2/20... Step: 180... Loss: 2.5086... Val Loss: 2.4850\n",
            "Epoch: 2/20... Step: 190... Loss: 2.4590... Val Loss: 2.4458\n",
            "Epoch: 2/20... Step: 200... Loss: 2.4612... Val Loss: 2.4347\n",
            "Epoch: 2/20... Step: 210... Loss: 2.4167... Val Loss: 2.3854\n",
            "Epoch: 2/20... Step: 220... Loss: 2.3799... Val Loss: 2.3550\n",
            "Epoch: 2/20... Step: 230... Loss: 2.3609... Val Loss: 2.3311\n",
            "Epoch: 2/20... Step: 240... Loss: 2.3433... Val Loss: 2.3039\n",
            "Epoch: 2/20... Step: 250... Loss: 2.2786... Val Loss: 2.2778\n",
            "Epoch: 2/20... Step: 260... Loss: 2.2583... Val Loss: 2.2509\n",
            "Epoch: 2/20... Step: 270... Loss: 2.2533... Val Loss: 2.2280\n",
            "Epoch: 3/20... Step: 280... Loss: 2.2521... Val Loss: 2.2048\n",
            "Epoch: 3/20... Step: 290... Loss: 2.2154... Val Loss: 2.1778\n",
            "Epoch: 3/20... Step: 300... Loss: 2.1885... Val Loss: 2.1533\n",
            "Epoch: 3/20... Step: 310... Loss: 2.1678... Val Loss: 2.1399\n",
            "Epoch: 3/20... Step: 320... Loss: 2.1361... Val Loss: 2.1165\n",
            "Epoch: 3/20... Step: 330... Loss: 2.1033... Val Loss: 2.0936\n",
            "Epoch: 3/20... Step: 340... Loss: 2.1074... Val Loss: 2.0815\n",
            "Epoch: 3/20... Step: 350... Loss: 2.1018... Val Loss: 2.0560\n",
            "Epoch: 3/20... Step: 360... Loss: 2.0267... Val Loss: 2.0359\n",
            "Epoch: 3/20... Step: 370... Loss: 2.0616... Val Loss: 2.0153\n",
            "Epoch: 3/20... Step: 380... Loss: 2.0316... Val Loss: 2.0003\n",
            "Epoch: 3/20... Step: 390... Loss: 2.0050... Val Loss: 1.9852\n",
            "Epoch: 3/20... Step: 400... Loss: 1.9742... Val Loss: 1.9695\n",
            "Epoch: 3/20... Step: 410... Loss: 1.9859... Val Loss: 1.9584\n",
            "Epoch: 4/20... Step: 420... Loss: 1.9744... Val Loss: 1.9412\n",
            "Epoch: 4/20... Step: 430... Loss: 1.9563... Val Loss: 1.9245\n",
            "Epoch: 4/20... Step: 440... Loss: 1.9524... Val Loss: 1.9146\n",
            "Epoch: 4/20... Step: 450... Loss: 1.8813... Val Loss: 1.8972\n",
            "Epoch: 4/20... Step: 460... Loss: 1.8716... Val Loss: 1.8860\n",
            "Epoch: 4/20... Step: 470... Loss: 1.9031... Val Loss: 1.8762\n",
            "Epoch: 4/20... Step: 480... Loss: 1.8744... Val Loss: 1.8652\n",
            "Epoch: 4/20... Step: 490... Loss: 1.8808... Val Loss: 1.8468\n",
            "Epoch: 4/20... Step: 500... Loss: 1.8779... Val Loss: 1.8376\n",
            "Epoch: 4/20... Step: 510... Loss: 1.8620... Val Loss: 1.8209\n",
            "Epoch: 4/20... Step: 520... Loss: 1.8663... Val Loss: 1.8138\n",
            "Epoch: 4/20... Step: 530... Loss: 1.8200... Val Loss: 1.8040\n",
            "Epoch: 4/20... Step: 540... Loss: 1.7834... Val Loss: 1.7921\n",
            "Epoch: 4/20... Step: 550... Loss: 1.8351... Val Loss: 1.7830\n",
            "Epoch: 5/20... Step: 560... Loss: 1.7965... Val Loss: 1.7692\n",
            "Epoch: 5/20... Step: 570... Loss: 1.7828... Val Loss: 1.7604\n",
            "Epoch: 5/20... Step: 580... Loss: 1.7660... Val Loss: 1.7531\n",
            "Epoch: 5/20... Step: 590... Loss: 1.7556... Val Loss: 1.7429\n",
            "Epoch: 5/20... Step: 600... Loss: 1.7545... Val Loss: 1.7330\n",
            "Epoch: 5/20... Step: 610... Loss: 1.7290... Val Loss: 1.7298\n",
            "Epoch: 5/20... Step: 620... Loss: 1.7308... Val Loss: 1.7186\n",
            "Epoch: 5/20... Step: 630... Loss: 1.7528... Val Loss: 1.7113\n",
            "Epoch: 5/20... Step: 640... Loss: 1.7242... Val Loss: 1.7022\n",
            "Epoch: 5/20... Step: 650... Loss: 1.7002... Val Loss: 1.6918\n",
            "Epoch: 5/20... Step: 660... Loss: 1.6862... Val Loss: 1.6870\n",
            "Epoch: 5/20... Step: 670... Loss: 1.7062... Val Loss: 1.6814\n",
            "Epoch: 5/20... Step: 680... Loss: 1.7057... Val Loss: 1.6708\n",
            "Epoch: 5/20... Step: 690... Loss: 1.6821... Val Loss: 1.6652\n",
            "Epoch: 6/20... Step: 700... Loss: 1.6862... Val Loss: 1.6576\n",
            "Epoch: 6/20... Step: 710... Loss: 1.6605... Val Loss: 1.6533\n",
            "Epoch: 6/20... Step: 720... Loss: 1.6513... Val Loss: 1.6408\n",
            "Epoch: 6/20... Step: 730... Loss: 1.6637... Val Loss: 1.6373\n",
            "Epoch: 6/20... Step: 740... Loss: 1.6295... Val Loss: 1.6314\n",
            "Epoch: 6/20... Step: 750... Loss: 1.6221... Val Loss: 1.6261\n",
            "Epoch: 6/20... Step: 760... Loss: 1.6597... Val Loss: 1.6236\n",
            "Epoch: 6/20... Step: 770... Loss: 1.6276... Val Loss: 1.6185\n",
            "Epoch: 6/20... Step: 780... Loss: 1.6217... Val Loss: 1.6088\n",
            "Epoch: 6/20... Step: 790... Loss: 1.5963... Val Loss: 1.6057\n",
            "Epoch: 6/20... Step: 800... Loss: 1.6235... Val Loss: 1.6015\n",
            "Epoch: 6/20... Step: 810... Loss: 1.6149... Val Loss: 1.5989\n",
            "Epoch: 6/20... Step: 820... Loss: 1.5643... Val Loss: 1.5914\n",
            "Epoch: 6/20... Step: 830... Loss: 1.6110... Val Loss: 1.5844\n",
            "Epoch: 7/20... Step: 840... Loss: 1.5693... Val Loss: 1.5783\n",
            "Epoch: 7/20... Step: 850... Loss: 1.5761... Val Loss: 1.5758\n",
            "Epoch: 7/20... Step: 860... Loss: 1.5665... Val Loss: 1.5675\n",
            "Epoch: 7/20... Step: 870... Loss: 1.5826... Val Loss: 1.5648\n",
            "Epoch: 7/20... Step: 880... Loss: 1.5710... Val Loss: 1.5596\n",
            "Epoch: 7/20... Step: 890... Loss: 1.5759... Val Loss: 1.5566\n",
            "Epoch: 7/20... Step: 900... Loss: 1.5447... Val Loss: 1.5556\n",
            "Epoch: 7/20... Step: 910... Loss: 1.5276... Val Loss: 1.5520\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfZxvNoDceEm"
      },
      "source": [
        "## Checkpoint\n",
        "\n",
        "После обучения сохраним модель, чтобы можно было загрузить ее позже. Здесь сохраняются параметры, необходимые для создания той же архитектуры, гиперпараметры скрытого слоя и токены."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6RXl5VAceEm"
      },
      "source": [
        "# change the name, for saving multiple files\n",
        "model_name = 'rnn_x_epoch.net'\n",
        "\n",
        "checkpoint = {'n_hidden': net.n_hidden,\n",
        "              'n_layers': net.n_layers,\n",
        "              'state_dict': net.state_dict(),\n",
        "              'tokens': net.chars}\n",
        "\n",
        "with open(model_name, 'wb') as f:\n",
        "    torch.save(checkpoint, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2sJhx5iceEm"
      },
      "source": [
        "---\n",
        "## Делаем предсказания\n",
        "\n",
        "Теперь, когда мы обучили модель, сделаем предсказание следующих символов! Для предсказания мы передаем последний символ, и сеть предсказывает следующий символ, который мы потом передаем снова на вхол и получаем еще один предсказанный символ и так далее...\n",
        "\n",
        "Наши прогнозы основаны на категориальном распределении вероятностей по всем возможным символам. Мы можем ограничить число символов, чтобы сделать получаемый предсказанный текст более разумным, рассматривая только некоторые наиболее вероятные символы $K$. Это не позволит сети выдавать нам совершенно абсурдные прогнозы, а также позволит внести некоторый шум и случайность в выбранный текст. Узнать больше [можно здесь](https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEIRW_B2ceEm"
      },
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "        ''' Given a character, predict the next character.\n",
        "            Returns the predicted character and the hidden state.\n",
        "        '''\n",
        "        \n",
        "        # tensor inputs\n",
        "        x = np.array([[net.char2int[char]]])\n",
        "        x = one_hot_encode(x, len(net.chars))\n",
        "        inputs = torch.from_numpy(x)\n",
        "        \n",
        "        if(train_on_gpu):\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        # detach hidden state from history\n",
        "        h = tuple([each.data for each in h])\n",
        "        # get the output of the model\n",
        "        out, h = net(inputs, h)\n",
        "\n",
        "        # get the character probabilities\n",
        "        p = F.softmax(out, dim=1).data\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu() # move to cpu\n",
        "        \n",
        "        # get top characters\n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(net.chars))\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "        \n",
        "        # select the likely next character with some element of randomness\n",
        "        p = p.numpy().squeeze()\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "        \n",
        "        # return the encoded value of the predicted char and the hidden state\n",
        "        return net.int2char[char], h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OG38j3gQceEm"
      },
      "source": [
        "### Priming и генерирование текста\n",
        "\n",
        "Нужно задать скрытое состояние, чтобы сеть не генерировала произвольные символы. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9vpB5gRceEm"
      },
      "source": [
        "def sample(net, size, prime='The', top_k=None):\n",
        "        \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "    \n",
        "    net.eval() # eval mode\n",
        "    \n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "    \n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqmFA9eEceEm"
      },
      "source": [
        "print(sample(net, 1000, prime='Anna', top_k=5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "942mjdQHceEm"
      },
      "source": [
        "## Loading a checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xt9ldUuSceEm"
      },
      "source": [
        "# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\n",
        "with open('rnn_x_epoch.net', 'rb') as f:\n",
        "    checkpoint = torch.load(f)\n",
        "    \n",
        "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
        "loaded.load_state_dict(checkpoint['state_dict'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ut6R3zDcceEm"
      },
      "source": [
        "# Sample using a loaded model\n",
        "print(sample(loaded, 2000, top_k=5, prime=\"And Levin said\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rhp9qC8neCk"
      },
      "source": [
        "### Полезные ссылки:\n",
        "\n",
        "\n",
        "*   [Блог-пост Christopher'а Olah'а по LSTM](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5Twt8oBnell"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}