{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"sem08_solution.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Character-Level LSTM\nНа этом семинаре поговорим про рекуррентные нейронные сети (Recurrent Neural Networ, RNN). Мы обучим модель на тексте книги \"Анна Каренина\", после чего попробуем генерировать новый текст.\n\n**Модель сможет генерировать новый текст на основе текста \"Анны Карениной\"!**\n\nМожно посмотреть полезную [статью про RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) и [реализацию в Torch](https://github.com/karpathy/char-rnn). \n\nОобщая архитектура RNN:\n\n<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/assets/charseq.jpeg?raw=1\" width=\"500\">","metadata":{"id":"1W8R8WgZceEk"}},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F","metadata":{"id":"sqUOE2flceEl","execution":{"iopub.status.busy":"2023-02-14T12:02:12.586369Z","iopub.execute_input":"2023-02-14T12:02:12.586828Z","iopub.status.idle":"2023-02-14T12:02:14.607311Z","shell.execute_reply.started":"2023-02-14T12:02:12.586738Z","shell.execute_reply":"2023-02-14T12:02:14.606225Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Загрузим данные\n\nЗагрузим текстовый файл \"Анны Карениной\".","metadata":{"id":"_wHfCDyzceEl"}},{"cell_type":"code","source":"with open('../input/charrnn/data/data/anna.txt', 'r') as f:\n    text = f.read()","metadata":{"id":"b34kfqIOceEl","execution":{"iopub.status.busy":"2023-02-14T12:02:21.967737Z","iopub.execute_input":"2023-02-14T12:02:21.968583Z","iopub.status.idle":"2023-02-14T12:02:22.032219Z","shell.execute_reply.started":"2023-02-14T12:02:21.968546Z","shell.execute_reply":"2023-02-14T12:02:22.031249Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Посмотрим первые 100 символов:","metadata":{"id":"Jp1Ljc4mceEl"}},{"cell_type":"code","source":"text[:100]","metadata":{"id":"7VctmLQfceEl","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"6907a065-d44b-4a0a-c0a3-2973137dce25","execution":{"iopub.status.busy":"2023-02-14T12:02:22.369125Z","iopub.execute_input":"2023-02-14T12:02:22.369493Z","iopub.status.idle":"2023-02-14T12:02:22.378972Z","shell.execute_reply.started":"2023-02-14T12:02:22.369463Z","shell.execute_reply":"2023-02-14T12:02:22.377937Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"},"metadata":{}}]},{"cell_type":"markdown","source":"### Токенизация\n\nВ ячейках ниже создадим два **словаря** для преобразования символов в целые числа и обратно. Кодирование символов как целых чисел упрощает их использование в качестве входных данных в сети.","metadata":{"id":"4iC21bopceEl"}},{"cell_type":"code","source":"# кодирование символов в инт и обратно\nchars = tuple(set(text))\nint2char = dict(enumerate(chars))\nchar2int = {ch: ii for ii, ch in int2char.items()}\n\n# encode the text\nencoded = np.array([char2int[ch] for ch in text])","metadata":{"id":"tYVlmnxLceEl","execution":{"iopub.status.busy":"2023-02-14T12:02:22.818351Z","iopub.execute_input":"2023-02-14T12:02:22.818708Z","iopub.status.idle":"2023-02-14T12:02:23.129784Z","shell.execute_reply.started":"2023-02-14T12:02:22.818677Z","shell.execute_reply":"2023-02-14T12:02:23.128790Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Посмотрим как символы закодировались целыми числами","metadata":{"id":"oJIzwzSwceEl"}},{"cell_type":"code","source":"encoded[:100]","metadata":{"id":"WK1MYr_9ceEl","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b46ed90c-8162-40f8-b664-d7335a077eb4","execution":{"iopub.status.busy":"2023-02-14T12:02:23.834448Z","iopub.execute_input":"2023-02-14T12:02:23.834849Z","iopub.status.idle":"2023-02-14T12:02:23.844195Z","shell.execute_reply.started":"2023-02-14T12:02:23.834814Z","shell.execute_reply":"2023-02-14T12:02:23.843063Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"array([50, 38, 10, 18, 56, 21, 61, 82, 59, 57, 57, 57, 29, 10, 18, 18,  6,\n       82, 37, 10,  4,  3, 60,  3, 21, 77, 82, 10, 61, 21, 82, 10, 60, 60,\n       82, 10, 60,  3, 55, 21, 24, 82, 21, 32, 21, 61,  6, 82, 66, 17, 38,\n       10, 18, 18,  6, 82, 37, 10,  4,  3, 60,  6, 82,  3, 77, 82, 66, 17,\n       38, 10, 18, 18,  6, 82,  3, 17, 82,  3, 56, 77, 82, 27, 40, 17, 57,\n       40, 10,  6, 35, 57, 57, 16, 32, 21, 61,  6, 56, 38,  3, 17])"},"metadata":{}}]},{"cell_type":"markdown","source":"## Предобработка данных\n\nКак можно видеть на изображении char-RNN выше, сеть ожидает **one-hot encoded** входа, что означает, что каждый символ преобразуется в целое число (через созданный словарь), а затем преобразуется в вектор-столбец, где только соответствующий ему целочисленный индекс будет иметь значение 1, а остальная часть вектора будет заполнена нулями. Давайте создадим для этого функцию.","metadata":{"id":"azltQy-gceEl"}},{"cell_type":"code","source":"def one_hot_encode(arr, n_labels):\n    # создаем пустой массив\n    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)    \n    # делаем ohe\n    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n    # reshape в исходную форму \n    one_hot = one_hot.reshape((*arr.shape, n_labels))\n    \n    return one_hot","metadata":{"id":"OnahALhiceEl","execution":{"iopub.status.busy":"2023-02-14T12:02:24.401733Z","iopub.execute_input":"2023-02-14T12:02:24.402414Z","iopub.status.idle":"2023-02-14T12:02:24.408131Z","shell.execute_reply.started":"2023-02-14T12:02:24.402379Z","shell.execute_reply":"2023-02-14T12:02:24.407095Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"test_seq = np.array([[3, 5, 1,9]])\none_hot = one_hot_encode(test_seq, 10)\n\nprint(one_hot)","metadata":{"id":"L3lTdLKfceEl","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4e5f7db8-4bda-4d4b-86cb-bd3dd442d582","execution":{"iopub.status.busy":"2023-02-14T12:02:24.672118Z","iopub.execute_input":"2023-02-14T12:02:24.672442Z","iopub.status.idle":"2023-02-14T12:02:24.679597Z","shell.execute_reply.started":"2023-02-14T12:02:24.672414Z","shell.execute_reply":"2023-02-14T12:02:24.678502Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"[[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Создаем mini-batch'и\n\n\nСоздатдим мини-батчи для обучения. На простом примере они будут выглядеть так:\n\n<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/assets/sequence_batching@1x.png?raw=1\" width=500px>\n<br>\n\nВозьмем закодированные символы (переданные как параметр `arr`) и разделим их на несколько последовательностей, заданных параметром `batch_size`. Каждая из наших последовательностей будет иметь длину `seq_length`.\n\n### Создани батчей\n\n**1. Первое, что нам нужно сделать, это отбросить часть текста, чтобы у нас были только полные мини-батчи**\n\nКаждый батч содержит $ N\\times M $ символов, где $ N $ - это размер батча (количество последовательностей в батче), а $ M $ - длина `seq_length` или количество шагов в последовательности. Затем, чтобы получить общее количество батчей $ K $, которое мы можем сделать из массива `arr`, нужно нацело разделить длину `arr` на количество символов в батче ($N * M$). Когда мы узнаем количество батчей, можно получить общее количество символов, которые нужно сохранить, из `arr`: $ N * M * K $.\n\n**2. После этого нам нужно разделить `arr` на $N$ батчей** \n\nЭто можно сделать с помощью `arr.reshape(size)`, где `size` - это кортеж, содержащий размеры измененного массива. Мы знаем, что нам нужно $ N $ последовательностей в батче, поэтому сделаем его размером первого измерения. Для второго измерения можем использовать «-1» в качестве заполнителя, он заполнит массив соответствующими данными. После этого должен остаться массив $N\\times(M * K)$.\n\n**3. Теперь, когда у нас есть этот массив, мы можем перебирать его, чтобы получить наши мини-батчи**\n\nИдея состоит в том, что каждая партия представляет собой окно $ N\\times M $ в массиве $ N\\times (M * K) $. Для каждого последующего батча окно перемещается на `seq_length`. Мы также хотим создать как входной, так и выходной массивы. Это окно можно сделать с помощью `range`, чтобы делать шаги размером `n_steps` от $ 0 $ до `arr.shape [1]`, общее количество токенов в каждой последовательности. Таким образом, целые числа, которые получены из диапазона, всегда указывают на начало батча, и каждое окно имеет ширину `seq_length`.\n\n> **TODO:** Допишите функцию для создания батчей: ","metadata":{"id":"9YyL91CuceEl"}},{"cell_type":"code","source":"def get_batches(arr, batch_size, seq_length):\n    '''Создаем генератор, возвращающий батчи\n       размером (batch_size x seq_length) из arr\n       \n       ---------\n       arr: массив, для которого будем делать батчи\n       batch_size: размер батча - кол-во последовательностей в батче\n       seq_length: кол-во символов в каждой послед батча\n    '''\n    chars_in_batch = batch_size * seq_length # кол-во символов в батче (во всех послед)\n    n_batches = len(arr) // chars_in_batch   # общее кол-во батчей\n    arr = arr[:n_batches * chars_in_batch]   # берем ту часть arr, чтобы были целые батчи\n    arr = arr.reshape((batch_size, -1))      # разделяем кол-во последовательностей = batch_size \n    \n    for n in range(0, arr.shape[1], seq_length):\n        x = arr[:, n:n+seq_length]           # закодированные символы (фичи)\n        y = np.zeros_like(x)                 # символы со сдвигом на 1 (таргеты)\n        try:\n            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n        except IndexError:\n            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n        yield x, y","metadata":{"id":"2ECftYejnvpx","execution":{"iopub.status.busy":"2023-02-14T12:02:25.198492Z","iopub.execute_input":"2023-02-14T12:02:25.198877Z","iopub.status.idle":"2023-02-14T12:02:25.207241Z","shell.execute_reply.started":"2023-02-14T12:02:25.198845Z","shell.execute_reply":"2023-02-14T12:02:25.206092Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Протестируем\n\nТеперь создадим несколько наборов данных, и проверим, что происходит, когда мы создаем батчи.","metadata":{"id":"s9uKOvbqceEl"}},{"cell_type":"code","source":"batches = get_batches(encoded, 8, 50)\nx, y = next(batches)","metadata":{"id":"qtKlLXi1ceEl","execution":{"iopub.status.busy":"2023-02-14T12:02:25.679368Z","iopub.execute_input":"2023-02-14T12:02:25.680396Z","iopub.status.idle":"2023-02-14T12:02:25.685278Z","shell.execute_reply.started":"2023-02-14T12:02:25.680351Z","shell.execute_reply":"2023-02-14T12:02:25.684396Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# printing out the first 10 items in a sequence\nprint('x\\n', x[:10, :10])\nprint('\\ny\\n', y[:10, :10])","metadata":{"id":"Rg5MUTqqceEl","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0965cef5-806e-4339-ad23-949928915384","execution":{"iopub.status.busy":"2023-02-14T12:02:25.918533Z","iopub.execute_input":"2023-02-14T12:02:25.918907Z","iopub.status.idle":"2023-02-14T12:02:25.925418Z","shell.execute_reply.started":"2023-02-14T12:02:25.918873Z","shell.execute_reply":"2023-02-14T12:02:25.924343Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"x\n [[50 38 10 18 56 21 61 82 59 57]\n [77 27 17 82 56 38 10 56 82 10]\n [21 17 45 82 27 61 82 10 82 37]\n [77 82 56 38 21 82 14 38  3 21]\n [82 77 10 40 82 38 21 61 82 56]\n [14 66 77 77  3 27 17 82 10 17]\n [82  9 17 17 10 82 38 10 45 82]\n [44  1 60 27 17 77 55  6 35 82]]\n\ny\n [[38 10 18 56 21 61 82 59 57 57]\n [27 17 82 56 38 10 56 82 10 56]\n [17 45 82 27 61 82 10 82 37 27]\n [82 56 38 21 82 14 38  3 21 37]\n [77 10 40 82 38 21 61 82 56 21]\n [66 77 77  3 27 17 82 10 17 45]\n [ 9 17 17 10 82 38 10 45 82 77]\n [ 1 60 27 17 77 55  6 35 82 74]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Если вы правильно реализовали get_batches, результат должен выглядеть примерно так:\n```\nx\n [[25  8 60 11 45 27 28 73  1  2]\n [17  7 20 73 45  8 60 45 73 60]\n [27 20 80 73  7 28 73 60 73 65]\n [17 73 45  8 27 73 66  8 46 27]\n [73 17 60 12 73  8 27 28 73 45]\n [66 64 17 17 46  7 20 73 60 20]\n [73 76 20 20 60 73  8 60 80 73]\n [47 35 43  7 20 17 24 50 37 73]]\n\ny\n [[ 8 60 11 45 27 28 73  1  2  2]\n [ 7 20 73 45  8 60 45 73 60 45]\n [20 80 73  7 28 73 60 73 65  7]\n [73 45  8 27 73 66  8 46 27 65]\n [17 60 12 73  8 27 28 73 45 27]\n [64 17 17 46  7 20 73 60 20 80]\n [76 20 20 60 73  8 60 80 73 17]\n [35 43  7 20 17 24 50 37 73 36]]\n ```\n хотя точные цифры могут отличаться. Убедитесь, что данные сдвинуты на один шаг для `y`!!!","metadata":{"id":"R_qHIAEIceEl"}},{"cell_type":"markdown","source":"---\n## Зададим архитектуру\n\n\n<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/assets/charRNN.png?raw=1\" width=500px>","metadata":{"id":"Jouxv0L2ceEl"}},{"cell_type":"markdown","source":"### Структура модели\n\nВ `__init__` предлагаемая структура выглядит следующим образом:\n* Создаваём и храним необходимые словари (уже релизовано)\n* Определяем слой LSTM, который принимает в качестве параметров: размер ввода (количество символов), размер скрытого слоя `n_hidden`, количество слоев` n_layers`, вероятность drop-out'а `drop_prob` и логическое значение batch_first (True)\n* Определяем слой drop-out с помощью drop_prob\n* Определяем полносвязанный слой с параметрами: размер ввода `n_hidden` и размер выхода - количество символов\n* Наконец, инициализируем веса\n\nОбратите внимание, что некоторые параметры были названы и указаны в функции `__init__`, их нужно сохранить и использовать, выполняя что-то вроде` self.drop_prob = drop_prob`.","metadata":{"id":"E7s5eRaoceEl"}},{"cell_type":"markdown","source":"---\n### Входы-выходы LSTM\n\nВы можете создать [LSTM layer](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM) следующим образом\n\n```python\nself.lstm = nn.LSTM(input_size, n_hidden, n_layers, \n                            dropout=drop_prob, batch_first=True)\n```\n\nгде `input_size` - это количество символов, которые эта ячейка ожидает видеть в качестве последовательного ввода, а `n_hidden` - это количество элементов в скрытых слоях ячейки. Можно добавить drop-out, добавив параметр `dropout` с заданной вероятностью. Наконец, в функции `forward` мы можем складывать ячейки LSTM в слои, используя `.view`.\n\nТакже требуется создать начальное скрытое состояние всех нулей:\n\n```python\nself.init_hidden()\n```","metadata":{"id":"Plm1atCuceEl"}},{"cell_type":"code","source":"# check if GPU is available\ntrain_on_gpu = torch.cuda.is_available()\nif(train_on_gpu):\n    print('Training on GPU!')\nelse: \n    print('No GPU available, training on CPU; consider making n_epochs very small.')","metadata":{"id":"HlTnDntHceEl","colab":{"base_uri":"https://localhost:8080/"},"outputId":"05590da1-3430-4f2f-9bc4-397a4835b77e","execution":{"iopub.status.busy":"2023-02-14T12:02:27.070822Z","iopub.execute_input":"2023-02-14T12:02:27.071177Z","iopub.status.idle":"2023-02-14T12:02:27.138222Z","shell.execute_reply.started":"2023-02-14T12:02:27.071147Z","shell.execute_reply":"2023-02-14T12:02:27.136904Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Training on GPU!\n","output_type":"stream"}]},{"cell_type":"code","source":"class CharRNN(nn.Module):\n    \n    def __init__(self, tokens, n_hidden=256, n_layers=2,\n                               drop_prob=0.5, lr=0.001):\n        super().__init__()\n        self.drop_prob = drop_prob\n        self.n_layers = n_layers\n        self.n_hidden = n_hidden\n        self.lr = lr\n        \n        # создание словарей для символов\n        self.chars = tokens\n        self.int2char = dict(enumerate(self.chars))\n        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n        \n        # определяем LSTM\n        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n                            dropout=drop_prob, batch_first=True)\n        \n        # определяем дропаут слой\n        self.dropout = nn.Dropout(drop_prob)\n        \n        # опрделяем финальный FC слой\n        self.fc = nn.Linear(n_hidden, len(self.chars))\n      \n    \n    def forward(self, x, hidden):\n        ''' Прямой проход нейросети'''\n                \n        # получаем output и новый промежуточный слой\n        r_output, hidden = self.lstm(x, hidden)\n        \n        # пропускаем выходы через дропаут\n        out = self.dropout(r_output)\n        \n        # делаем решейп - получаем число строк = числу объектов в батче, в каждой строке - выходы по конкретному символу\n        out = out.contiguous().view(-1, self.n_hidden)\n        \n        # выходы скрытых состояний пропускаем через фк-слой\n        out = self.fc(out)\n        \n        # вовзвращаем итоговые логиты и финальное скрытое состояние\n        return out, hidden\n    \n    \n    def init_hidden(self, batch_size):\n        ''' Инициализация скрытого состояния '''\n        # создание двух тензоров, размером: n_layers x batch_size x n_hidden,\n        # initialized to zero, for hidden state and cell state of LSTM\n        weight = next(self.parameters()).data\n        \n        if (train_on_gpu):\n            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n        else:\n            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n        \n        return hidden","metadata":{"id":"VPq1EA38rBqn","execution":{"iopub.status.busy":"2023-02-14T12:02:27.310742Z","iopub.execute_input":"2023-02-14T12:02:27.311102Z","iopub.status.idle":"2023-02-14T12:02:27.322135Z","shell.execute_reply.started":"2023-02-14T12:02:27.311073Z","shell.execute_reply":"2023-02-14T12:02:27.321077Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Пример работы нашей модели с батчами \nДля понимания структуры модели и процесса обучения сети","metadata":{}},{"cell_type":"code","source":"# берем батч с кол-вом послед-й = 3 и длиной = 6\nx, y = next(get_batches(encoded, 3, 6))\nx, y","metadata":{"execution":{"iopub.status.busy":"2023-02-14T12:02:27.787821Z","iopub.execute_input":"2023-02-14T12:02:27.788195Z","iopub.status.idle":"2023-02-14T12:02:27.796408Z","shell.execute_reply.started":"2023-02-14T12:02:27.788152Z","shell.execute_reply":"2023-02-14T12:02:27.795285Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"(array([[50, 38, 10, 18, 56, 21],\n        [66,  1, 51, 21, 14, 56],\n        [60, 21, 82, 27, 32, 21]]),\n array([[38, 10, 18, 56, 21, 61],\n        [ 1, 51, 21, 14, 56, 35],\n        [21, 82, 27, 32, 21, 61]]))"},"metadata":{}}]},{"cell_type":"code","source":"# делаем ohe кодирование каждого символа\nohe_x = torch.from_numpy(one_hot_encode(x, len(chars)))\nohe_x, ohe_x.shape","metadata":{"execution":{"iopub.status.busy":"2023-02-14T12:02:28.568414Z","iopub.execute_input":"2023-02-14T12:02:28.568799Z","iopub.status.idle":"2023-02-14T12:02:28.582208Z","shell.execute_reply.started":"2023-02-14T12:02:28.568734Z","shell.execute_reply":"2023-02-14T12:02:28.580978Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"(tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n \n         [[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 1., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n \n         [[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 1.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]]]),\n torch.Size([3, 6, 83]))"},"metadata":{}}]},{"cell_type":"code","source":"# рандомно инициализируем начальное скрытое состояние для LSTM-слоя\n# скрытое сост описывается в данной реализации двумя параметрами h,c\nh0 = torch.randn(2, 3, 10)\nc0 = torch.randn(2, 3, 10)\n\n# создаем \"голый\" LSTM-слой с размером входа = числу уник символов, размер скрытого слоя = 10, кол-во LTSM-слоев = 2 \njust_lstm = nn.LSTM(len(chars), 10, 2, batch_first=True)\n\n# получаем выходы крайнего слоя сети(в нашем случае-второго) и скрытые состояния для каждого элемента каждой послед-ти в каждом батче(h,c)\noutput, (hn, cn) = just_lstm(ohe_x,(h0, c0))\noutput, output.shape","metadata":{"execution":{"iopub.status.busy":"2023-02-14T12:02:29.580010Z","iopub.execute_input":"2023-02-14T12:02:29.580394Z","iopub.status.idle":"2023-02-14T12:02:29.662953Z","shell.execute_reply.started":"2023-02-14T12:02:29.580363Z","shell.execute_reply":"2023-02-14T12:02:29.661965Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"(tensor([[[ 0.0343,  0.0047, -0.3562, -0.0677, -0.2162,  0.0644, -0.0560,\n           -0.1192,  0.2907,  0.4693],\n          [ 0.0432,  0.0379, -0.0285, -0.0083, -0.1016, -0.0305, -0.0924,\n           -0.2044,  0.1201,  0.2923],\n          [ 0.0087,  0.0746,  0.0887,  0.0246,  0.0055, -0.0856, -0.1197,\n           -0.1254,  0.0343,  0.1540],\n          [-0.0202,  0.0978,  0.1564,  0.0359,  0.0554, -0.0872, -0.1346,\n           -0.0587, -0.0166,  0.0855],\n          [-0.0340,  0.1181,  0.2068,  0.0520,  0.0730, -0.0846, -0.1353,\n           -0.0087, -0.0439,  0.0781],\n          [-0.0434,  0.1345,  0.2414,  0.0688,  0.0774, -0.0722, -0.1293,\n            0.0297, -0.0418,  0.0921]],\n \n         [[-0.0091, -0.0742,  0.2105, -0.3617,  0.0749,  0.1683, -0.3839,\n           -0.0665,  0.0220,  0.1851],\n          [-0.0189, -0.0059,  0.2139, -0.1150,  0.1075, -0.0083, -0.1848,\n            0.0128,  0.0209,  0.1614],\n          [-0.0252,  0.0555,  0.2466, -0.0017,  0.0964, -0.0559, -0.1822,\n            0.0360,  0.0073,  0.1066],\n          [-0.0338,  0.0991,  0.2645,  0.0545,  0.0890, -0.0589, -0.1653,\n            0.0528,  0.0031,  0.1040],\n          [-0.0417,  0.1237,  0.2644,  0.0748,  0.0788, -0.0667, -0.1509,\n            0.0577, -0.0087,  0.0992],\n          [-0.0409,  0.1412,  0.2729,  0.0890,  0.0762, -0.0717, -0.1383,\n            0.0621, -0.0221,  0.1063]],\n \n         [[-0.1369,  0.2184,  0.4262,  0.0523,  0.2052, -0.1442,  0.1614,\n           -0.1429, -0.1488, -0.1307],\n          [-0.1572,  0.2020,  0.3766,  0.0590,  0.2011, -0.1528,  0.0653,\n           -0.0438, -0.1340, -0.0171],\n          [-0.1367,  0.1825,  0.3344,  0.0625,  0.1844, -0.1086, -0.0118,\n            0.0275, -0.1157,  0.0473],\n          [-0.1182,  0.1707,  0.3100,  0.0658,  0.1606, -0.0915, -0.0526,\n            0.0548, -0.0993,  0.0670],\n          [-0.1038,  0.1604,  0.2891,  0.0645,  0.1374, -0.0788, -0.0784,\n            0.0623, -0.1032,  0.0777],\n          [-0.0951,  0.1607,  0.2851,  0.0721,  0.1141, -0.0682, -0.0889,\n            0.0705, -0.0818,  0.0931]]], grad_fn=<TransposeBackward0>),\n torch.Size([3, 6, 10]))"},"metadata":{}}]},{"cell_type":"code","source":"# containing the final hidden state for each element in the sequenc\nhn, hn.shape","metadata":{"execution":{"iopub.status.busy":"2023-02-14T12:02:30.034070Z","iopub.execute_input":"2023-02-14T12:02:30.034442Z","iopub.status.idle":"2023-02-14T12:02:30.045478Z","shell.execute_reply.started":"2023-02-14T12:02:30.034411Z","shell.execute_reply":"2023-02-14T12:02:30.044447Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"(tensor([[[-0.0731, -0.0884, -0.1309, -0.1100, -0.2039, -0.0496, -0.0475,\n            0.0463, -0.1645,  0.1421],\n          [-0.1208, -0.0712, -0.2103, -0.0777, -0.1898, -0.0607, -0.0109,\n           -0.0080, -0.1912,  0.1071],\n          [-0.0627, -0.1520, -0.2189, -0.1021, -0.2187, -0.0550, -0.0602,\n            0.0023, -0.1318,  0.1787]],\n \n         [[-0.0434,  0.1345,  0.2414,  0.0688,  0.0774, -0.0722, -0.1293,\n            0.0297, -0.0418,  0.0921],\n          [-0.0409,  0.1412,  0.2729,  0.0890,  0.0762, -0.0717, -0.1383,\n            0.0621, -0.0221,  0.1063],\n          [-0.0951,  0.1607,  0.2851,  0.0721,  0.1141, -0.0682, -0.0889,\n            0.0705, -0.0818,  0.0931]]], grad_fn=<StackBackward0>),\n torch.Size([2, 3, 10]))"},"metadata":{}}]},{"cell_type":"code","source":"# containing the final cell state for each element in the sequence\ncn, cn.shape","metadata":{"execution":{"iopub.status.busy":"2023-02-14T12:02:31.113232Z","iopub.execute_input":"2023-02-14T12:02:31.113941Z","iopub.status.idle":"2023-02-14T12:02:31.122233Z","shell.execute_reply.started":"2023-02-14T12:02:31.113903Z","shell.execute_reply":"2023-02-14T12:02:31.121289Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"(tensor([[[-0.1519, -0.1802, -0.2485, -0.2007, -0.4782, -0.1164, -0.0759,\n            0.0839, -0.3283,  0.2355],\n          [-0.2064, -0.1238, -0.4940, -0.1292, -0.3691, -0.1296, -0.0164,\n           -0.0136, -0.4459,  0.2223],\n          [-0.1278, -0.3141, -0.4277, -0.1842, -0.5101, -0.1262, -0.0969,\n            0.0041, -0.2626,  0.2978]],\n \n         [[-0.1110,  0.2744,  0.4516,  0.1397,  0.1516, -0.1289, -0.2816,\n            0.0805, -0.0744,  0.1881],\n          [-0.1053,  0.2948,  0.5190,  0.1841,  0.1525, -0.1308, -0.3028,\n            0.1693, -0.0393,  0.2137],\n          [-0.2433,  0.3269,  0.5481,  0.1504,  0.2305, -0.1223, -0.1883,\n            0.1929, -0.1429,  0.1900]]], grad_fn=<StackBackward0>),\n torch.Size([2, 3, 10]))"},"metadata":{}}]},{"cell_type":"code","source":"# делаем решейп - получаем число строк = числу объектов в батче, в каждой строке - выходы по конкретному символу\noutput = output.contiguous().view(-1, 10)\noutput, output.shape","metadata":{"execution":{"iopub.status.busy":"2023-02-14T12:02:32.753626Z","iopub.execute_input":"2023-02-14T12:02:32.754378Z","iopub.status.idle":"2023-02-14T12:02:32.764931Z","shell.execute_reply.started":"2023-02-14T12:02:32.754339Z","shell.execute_reply":"2023-02-14T12:02:32.763685Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"(tensor([[ 0.0343,  0.0047, -0.3562, -0.0677, -0.2162,  0.0644, -0.0560, -0.1192,\n           0.2907,  0.4693],\n         [ 0.0432,  0.0379, -0.0285, -0.0083, -0.1016, -0.0305, -0.0924, -0.2044,\n           0.1201,  0.2923],\n         [ 0.0087,  0.0746,  0.0887,  0.0246,  0.0055, -0.0856, -0.1197, -0.1254,\n           0.0343,  0.1540],\n         [-0.0202,  0.0978,  0.1564,  0.0359,  0.0554, -0.0872, -0.1346, -0.0587,\n          -0.0166,  0.0855],\n         [-0.0340,  0.1181,  0.2068,  0.0520,  0.0730, -0.0846, -0.1353, -0.0087,\n          -0.0439,  0.0781],\n         [-0.0434,  0.1345,  0.2414,  0.0688,  0.0774, -0.0722, -0.1293,  0.0297,\n          -0.0418,  0.0921],\n         [-0.0091, -0.0742,  0.2105, -0.3617,  0.0749,  0.1683, -0.3839, -0.0665,\n           0.0220,  0.1851],\n         [-0.0189, -0.0059,  0.2139, -0.1150,  0.1075, -0.0083, -0.1848,  0.0128,\n           0.0209,  0.1614],\n         [-0.0252,  0.0555,  0.2466, -0.0017,  0.0964, -0.0559, -0.1822,  0.0360,\n           0.0073,  0.1066],\n         [-0.0338,  0.0991,  0.2645,  0.0545,  0.0890, -0.0589, -0.1653,  0.0528,\n           0.0031,  0.1040],\n         [-0.0417,  0.1237,  0.2644,  0.0748,  0.0788, -0.0667, -0.1509,  0.0577,\n          -0.0087,  0.0992],\n         [-0.0409,  0.1412,  0.2729,  0.0890,  0.0762, -0.0717, -0.1383,  0.0621,\n          -0.0221,  0.1063],\n         [-0.1369,  0.2184,  0.4262,  0.0523,  0.2052, -0.1442,  0.1614, -0.1429,\n          -0.1488, -0.1307],\n         [-0.1572,  0.2020,  0.3766,  0.0590,  0.2011, -0.1528,  0.0653, -0.0438,\n          -0.1340, -0.0171],\n         [-0.1367,  0.1825,  0.3344,  0.0625,  0.1844, -0.1086, -0.0118,  0.0275,\n          -0.1157,  0.0473],\n         [-0.1182,  0.1707,  0.3100,  0.0658,  0.1606, -0.0915, -0.0526,  0.0548,\n          -0.0993,  0.0670],\n         [-0.1038,  0.1604,  0.2891,  0.0645,  0.1374, -0.0788, -0.0784,  0.0623,\n          -0.1032,  0.0777],\n         [-0.0951,  0.1607,  0.2851,  0.0721,  0.1141, -0.0682, -0.0889,  0.0705,\n          -0.0818,  0.0931]], grad_fn=<ViewBackward0>),\n torch.Size([18, 10]))"},"metadata":{}}]},{"cell_type":"code","source":"# применяем ФК-слой к выходам по каждому элементу \n# и получаем логиты вероятностей след элементов\njust_fc = nn.Linear(10, len(chars))\nres = just_fc(output)\nres, res.shape","metadata":{"execution":{"iopub.status.busy":"2023-02-14T12:09:15.861832Z","iopub.execute_input":"2023-02-14T12:09:15.862721Z","iopub.status.idle":"2023-02-14T12:09:15.872415Z","shell.execute_reply.started":"2023-02-14T12:09:15.862674Z","shell.execute_reply":"2023-02-14T12:09:15.871220Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"(tensor([[ 0.0835,  0.1542,  0.0207,  ..., -0.3010, -0.3587, -0.1807],\n         [ 0.1259,  0.1088,  0.1051,  ..., -0.2967, -0.3181, -0.2657],\n         [ 0.1095,  0.0956,  0.1968,  ..., -0.3081, -0.2664, -0.3092],\n         ...,\n         [ 0.0431,  0.0641,  0.3614,  ..., -0.3191, -0.2067, -0.4160],\n         [ 0.0370,  0.0698,  0.3448,  ..., -0.3261, -0.2007, -0.4000],\n         [ 0.0372,  0.0670,  0.3266,  ..., -0.3379, -0.2049, -0.3922]],\n        grad_fn=<AddmmBackward0>),\n torch.Size([18, 83]))"},"metadata":{}}]},{"cell_type":"code","source":"# ответы для каждого из 3(batch size) * 6(seq_len) = 18 элементов \ntorch.from_numpy(y).view(3*6)","metadata":{"execution":{"iopub.status.busy":"2023-02-14T12:09:16.141801Z","iopub.execute_input":"2023-02-14T12:09:16.142443Z","iopub.status.idle":"2023-02-14T12:09:16.150365Z","shell.execute_reply.started":"2023-02-14T12:09:16.142410Z","shell.execute_reply":"2023-02-14T12:09:16.149400Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"tensor([38, 10, 18, 56, 21, 61,  1, 51, 21, 14, 56, 35, 21, 82, 27, 32, 21, 61])"},"metadata":{}}]},{"cell_type":"code","source":"# считаем лосс по батчу, далее бэк-проп\nnn.CrossEntropyLoss()(res, torch.from_numpy(y).view(3*6))","metadata":{"execution":{"iopub.status.busy":"2023-02-14T12:09:16.531124Z","iopub.execute_input":"2023-02-14T12:09:16.531807Z","iopub.status.idle":"2023-02-14T12:09:16.541325Z","shell.execute_reply.started":"2023-02-14T12:09:16.531747Z","shell.execute_reply":"2023-02-14T12:09:16.540265Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"tensor(4.4358, grad_fn=<NllLossBackward0>)"},"metadata":{}}]},{"cell_type":"markdown","source":"### То же самое внутри нашего франкенштейна\nВнутри модели идет и слой лстм, и фк-слой и решейпинг","metadata":{}},{"cell_type":"code","source":"net = CharRNN(chars, n_hidden=10, n_layers=2, drop_prob=0)","metadata":{"execution":{"iopub.status.busy":"2023-02-14T12:31:50.436080Z","iopub.execute_input":"2023-02-14T12:31:50.436442Z","iopub.status.idle":"2023-02-14T12:31:50.443142Z","shell.execute_reply.started":"2023-02-14T12:31:50.436413Z","shell.execute_reply":"2023-02-14T12:31:50.441966Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"output, (hn, cn) = net(ohe_x,(h0, c0))","metadata":{"execution":{"iopub.status.busy":"2023-02-14T12:31:51.485882Z","iopub.execute_input":"2023-02-14T12:31:51.486791Z","iopub.status.idle":"2023-02-14T12:31:51.493725Z","shell.execute_reply.started":"2023-02-14T12:31:51.486711Z","shell.execute_reply":"2023-02-14T12:31:51.492691Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"output, output.shape","metadata":{"execution":{"iopub.status.busy":"2023-02-14T12:31:51.778190Z","iopub.execute_input":"2023-02-14T12:31:51.778532Z","iopub.status.idle":"2023-02-14T12:31:51.788435Z","shell.execute_reply.started":"2023-02-14T12:31:51.778504Z","shell.execute_reply":"2023-02-14T12:31:51.787447Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"(tensor([[-0.1803, -0.2769,  0.4006,  ..., -0.1604, -0.2651, -0.3944],\n         [-0.2628, -0.2175,  0.4230,  ..., -0.0868, -0.2442, -0.3540],\n         [-0.2785, -0.2026,  0.4353,  ..., -0.0254, -0.2635, -0.3452],\n         ...,\n         [-0.3254, -0.1689,  0.4939,  ...,  0.0110, -0.2737, -0.3406],\n         [-0.3181, -0.1756,  0.4895,  ...,  0.0188, -0.2767, -0.3465],\n         [-0.3127, -0.1810,  0.4802,  ...,  0.0180, -0.2770, -0.3476]],\n        grad_fn=<AddmmBackward0>),\n torch.Size([18, 83]))"},"metadata":{}}]},{"cell_type":"code","source":"# считаем лосс по батчу, далее бэк-проп\nnn.CrossEntropyLoss()(output, torch.from_numpy(y).view(3*6))","metadata":{"execution":{"iopub.status.busy":"2023-02-14T12:31:52.538978Z","iopub.execute_input":"2023-02-14T12:31:52.539900Z","iopub.status.idle":"2023-02-14T12:31:52.547624Z","shell.execute_reply.started":"2023-02-14T12:31:52.539852Z","shell.execute_reply":"2023-02-14T12:31:52.546545Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"tensor(4.4621, grad_fn=<NllLossBackward0>)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Обучим модель\n\nВо время обучения нужно установить количество эпох, скорость обучения и другие параметры.\n\nИспользуем оптимизатор Adam и кросс-энтропию, считаем loss и, как обычно, выполняем back propagation.\n\nПара подробностей об обучении:\n> * Во время цикла мы отделяем скрытое состояние от его истории; на этот раз установив его равным новой переменной * tuple *, потому что скрытое состояние LSTM, является кортежем скрытых состояний.\n* Мы используем [`clip_grad_norm_`](https://pytorch.org/docs/stable/_modules/torch/nn/utils/clip_grad.html) чтобы избавиться от взрывающихся градиентов.","metadata":{"id":"5IrBRlEPceEl"}},{"cell_type":"code","source":"def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n    ''' Функция тренировки модели\n        ---------\n        \n        net: CharRNN - класс модели\n        data: текст для обучения модели text data to train the network\n        epochs: кол-во эпох\n        batch_size: кол-во последовательностей в мини-батче\n        seq_length: кол-во символов в каждой последовательности в мини-батче\n        lr: learning rate\n        clip: gradient clipping\n        val_frac: доля валидационного датасета // Fraction of data to hold out for validation\n        print_every: кол-во шагов для вывода логов // Number of steps for printing training and validation loss\n    \n    '''\n    net.train()\n    \n    opt = torch.optim.Adam(net.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss()\n    \n    # разделяем на трени и вал выборки\n    val_idx = int(len(data)*(1-val_frac))\n    data, val_data = data[:val_idx], data[val_idx:]\n    \n    if(train_on_gpu):\n        net.cuda()\n    \n    counter = 0\n    n_chars = len(net.chars) # уник символы текста\n    for e in range(epochs):\n        # инициализируем скрытое состояние\n        h = net.init_hidden(batch_size)\n        \n        for x, y in get_batches(data, batch_size, seq_length):\n            counter += 1\n            \n            # One-hot кодирование и перевод в тензоры\n            x = one_hot_encode(x, n_chars)\n            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n            \n            if(train_on_gpu):\n                inputs, targets = inputs.cuda(), targets.cuda()\n\n            # Creating new variables for the hidden state, otherwise\n            # we'd backprop through the entire training history\n            h = tuple([each.data for each in h])\n\n            # обнуляем старые градиенты\n            net.zero_grad()\n            \n            # прямой проход модели по батчу\n            output, h = net(inputs, h)\n            \n            # считаем лосс и делаем бэк-проп\n            loss = criterion(output, targets.view(batch_size*seq_length).long())\n            loss.backward()\n            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n            nn.utils.clip_grad_norm_(net.parameters(), clip)\n            opt.step()\n            \n            # loss stats\n            if counter % print_every == 0:\n                # Get validation loss\n                val_h = net.init_hidden(batch_size)\n                val_losses = []\n                net.eval()\n                for x, y in get_batches(val_data, batch_size, seq_length):\n                    # One-hot encode our data and make them Torch tensors\n                    x = one_hot_encode(x, n_chars)\n                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n                    \n                    # Creating new variables for the hidden state, otherwise\n                    # we'd backprop through the entire training history\n                    val_h = tuple([each.data for each in val_h])\n                    \n                    inputs, targets = x, y\n                    if(train_on_gpu):\n                        inputs, targets = inputs.cuda(), targets.cuda()\n\n                    output, val_h = net(inputs, val_h)\n                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n                \n                    val_losses.append(val_loss.item())\n                \n                net.train() # reset to train mode after iterationg through validation data\n                \n                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n                      \"Step: {}...\".format(counter),\n                      \"Loss: {:.4f}...\".format(loss.item()),\n                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))","metadata":{"id":"lv8VkRI0ceEl","execution":{"iopub.status.busy":"2023-02-14T12:42:43.055525Z","iopub.execute_input":"2023-02-14T12:42:43.055917Z","iopub.status.idle":"2023-02-14T12:42:43.072104Z","shell.execute_reply.started":"2023-02-14T12:42:43.055883Z","shell.execute_reply":"2023-02-14T12:42:43.071086Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"## Определим модель\n\nТеперь мы можем создать модель с заданными гиперпараметрами. Определим размеры мини-батчей.","metadata":{"id":"Gt0q4KGEceEm"}},{"cell_type":"code","source":"n_hidden=512\nn_layers=2\n\nnet = CharRNN(chars, n_hidden, n_layers)\nprint(net)","metadata":{"id":"ykMcIloEr3G7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"300411a6-9dc4-4f34-aa4a-267f4769342a","execution":{"iopub.status.busy":"2023-02-14T12:43:03.549449Z","iopub.execute_input":"2023-02-14T12:43:03.549826Z","iopub.status.idle":"2023-02-14T12:43:03.602351Z","shell.execute_reply.started":"2023-02-14T12:43:03.549792Z","shell.execute_reply":"2023-02-14T12:43:03.599845Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"CharRNN(\n  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n  (dropout): Dropout(p=0.5, inplace=False)\n  (fc): Linear(in_features=512, out_features=83, bias=True)\n)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Установим гиперпараметры","metadata":{"id":"XHy6mECuceEm"}},{"cell_type":"code","source":"batch_size = 128\nseq_length = 100\nn_epochs = 20 \n\ntrain(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=20)","metadata":{"id":"8hTkNrWEsjgI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d1c7738d-e3b6-4b03-b98a-c71b57a27954","execution":{"iopub.status.busy":"2023-02-14T12:47:20.997910Z","iopub.execute_input":"2023-02-14T12:47:20.998555Z","iopub.status.idle":"2023-02-14T12:50:44.025530Z","shell.execute_reply.started":"2023-02-14T12:47:20.998522Z","shell.execute_reply":"2023-02-14T12:50:44.024517Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Epoch: 1/20... Step: 20... Loss: 3.1508... Val Loss: 3.1298\nEpoch: 1/20... Step: 40... Loss: 3.1124... Val Loss: 3.1205\nEpoch: 1/20... Step: 60... Loss: 3.1153... Val Loss: 3.1153\nEpoch: 1/20... Step: 80... Loss: 3.1172... Val Loss: 3.1065\nEpoch: 1/20... Step: 100... Loss: 3.0701... Val Loss: 3.0573\nEpoch: 1/20... Step: 120... Loss: 2.9059... Val Loss: 2.8791\nEpoch: 2/20... Step: 140... Loss: 2.7197... Val Loss: 2.6579\nEpoch: 2/20... Step: 160... Loss: 2.5703... Val Loss: 2.5201\nEpoch: 2/20... Step: 180... Loss: 2.4748... Val Loss: 2.4459\nEpoch: 2/20... Step: 200... Loss: 2.4105... Val Loss: 2.3865\nEpoch: 2/20... Step: 220... Loss: 2.3480... Val Loss: 2.3239\nEpoch: 2/20... Step: 240... Loss: 2.3079... Val Loss: 2.2726\nEpoch: 2/20... Step: 260... Loss: 2.2221... Val Loss: 2.2230\nEpoch: 3/20... Step: 280... Loss: 2.2259... Val Loss: 2.1782\nEpoch: 3/20... Step: 300... Loss: 2.1687... Val Loss: 2.1354\nEpoch: 3/20... Step: 320... Loss: 2.1208... Val Loss: 2.1007\nEpoch: 3/20... Step: 340... Loss: 2.0991... Val Loss: 2.0651\nEpoch: 3/20... Step: 360... Loss: 2.0265... Val Loss: 2.0332\nEpoch: 3/20... Step: 380... Loss: 2.0294... Val Loss: 1.9987\nEpoch: 3/20... Step: 400... Loss: 1.9775... Val Loss: 1.9656\nEpoch: 4/20... Step: 420... Loss: 1.9765... Val Loss: 1.9396\nEpoch: 4/20... Step: 440... Loss: 1.9521... Val Loss: 1.9184\nEpoch: 4/20... Step: 460... Loss: 1.8880... Val Loss: 1.8929\nEpoch: 4/20... Step: 480... Loss: 1.8945... Val Loss: 1.8682\nEpoch: 4/20... Step: 500... Loss: 1.8846... Val Loss: 1.8443\nEpoch: 4/20... Step: 520... Loss: 1.8824... Val Loss: 1.8212\nEpoch: 4/20... Step: 540... Loss: 1.7959... Val Loss: 1.8011\nEpoch: 5/20... Step: 560... Loss: 1.8047... Val Loss: 1.7821\nEpoch: 5/20... Step: 580... Loss: 1.7698... Val Loss: 1.7634\nEpoch: 5/20... Step: 600... Loss: 1.7547... Val Loss: 1.7444\nEpoch: 5/20... Step: 620... Loss: 1.7529... Val Loss: 1.7285\nEpoch: 5/20... Step: 640... Loss: 1.7410... Val Loss: 1.7150\nEpoch: 5/20... Step: 660... Loss: 1.6929... Val Loss: 1.6962\nEpoch: 5/20... Step: 680... Loss: 1.7200... Val Loss: 1.6805\nEpoch: 6/20... Step: 700... Loss: 1.6920... Val Loss: 1.6646\nEpoch: 6/20... Step: 720... Loss: 1.6698... Val Loss: 1.6533\nEpoch: 6/20... Step: 740... Loss: 1.6383... Val Loss: 1.6421\nEpoch: 6/20... Step: 760... Loss: 1.6652... Val Loss: 1.6312\nEpoch: 6/20... Step: 780... Loss: 1.6310... Val Loss: 1.6185\nEpoch: 6/20... Step: 800... Loss: 1.6287... Val Loss: 1.6108\nEpoch: 6/20... Step: 820... Loss: 1.5925... Val Loss: 1.5982\nEpoch: 7/20... Step: 840... Loss: 1.5747... Val Loss: 1.5877\nEpoch: 7/20... Step: 860... Loss: 1.5848... Val Loss: 1.5775\nEpoch: 7/20... Step: 880... Loss: 1.5940... Val Loss: 1.5696\nEpoch: 7/20... Step: 900... Loss: 1.5720... Val Loss: 1.5596\nEpoch: 7/20... Step: 920... Loss: 1.5601... Val Loss: 1.5552\nEpoch: 7/20... Step: 940... Loss: 1.5526... Val Loss: 1.5452\nEpoch: 7/20... Step: 960... Loss: 1.5598... Val Loss: 1.5356\nEpoch: 8/20... Step: 980... Loss: 1.5294... Val Loss: 1.5282\nEpoch: 8/20... Step: 1000... Loss: 1.5274... Val Loss: 1.5186\nEpoch: 8/20... Step: 1020... Loss: 1.5293... Val Loss: 1.5147\nEpoch: 8/20... Step: 1040... Loss: 1.5245... Val Loss: 1.5094\nEpoch: 8/20... Step: 1060... Loss: 1.5016... Val Loss: 1.5041\nEpoch: 8/20... Step: 1080... Loss: 1.5112... Val Loss: 1.4970\nEpoch: 8/20... Step: 1100... Loss: 1.4945... Val Loss: 1.4910\nEpoch: 9/20... Step: 1120... Loss: 1.5085... Val Loss: 1.4820\nEpoch: 9/20... Step: 1140... Loss: 1.4928... Val Loss: 1.4746\nEpoch: 9/20... Step: 1160... Loss: 1.4634... Val Loss: 1.4769\nEpoch: 9/20... Step: 1180... Loss: 1.4568... Val Loss: 1.4702\nEpoch: 9/20... Step: 1200... Loss: 1.4483... Val Loss: 1.4650\nEpoch: 9/20... Step: 1220... Loss: 1.4563... Val Loss: 1.4550\nEpoch: 9/20... Step: 1240... Loss: 1.4413... Val Loss: 1.4534\nEpoch: 10/20... Step: 1260... Loss: 1.4568... Val Loss: 1.4501\nEpoch: 10/20... Step: 1280... Loss: 1.4583... Val Loss: 1.4391\nEpoch: 10/20... Step: 1300... Loss: 1.4401... Val Loss: 1.4412\nEpoch: 10/20... Step: 1320... Loss: 1.4141... Val Loss: 1.4337\nEpoch: 10/20... Step: 1340... Loss: 1.4045... Val Loss: 1.4280\nEpoch: 10/20... Step: 1360... Loss: 1.4042... Val Loss: 1.4301\nEpoch: 10/20... Step: 1380... Loss: 1.4460... Val Loss: 1.4215\nEpoch: 11/20... Step: 1400... Loss: 1.4416... Val Loss: 1.4166\nEpoch: 11/20... Step: 1420... Loss: 1.4422... Val Loss: 1.4113\nEpoch: 11/20... Step: 1440... Loss: 1.4406... Val Loss: 1.4119\nEpoch: 11/20... Step: 1460... Loss: 1.3884... Val Loss: 1.4066\nEpoch: 11/20... Step: 1480... Loss: 1.4040... Val Loss: 1.4030\nEpoch: 11/20... Step: 1500... Loss: 1.3764... Val Loss: 1.4005\nEpoch: 11/20... Step: 1520... Loss: 1.3922... Val Loss: 1.3967\nEpoch: 12/20... Step: 1540... Loss: 1.3962... Val Loss: 1.3905\nEpoch: 12/20... Step: 1560... Loss: 1.4056... Val Loss: 1.3854\nEpoch: 12/20... Step: 1580... Loss: 1.3418... Val Loss: 1.3929\nEpoch: 12/20... Step: 1600... Loss: 1.3656... Val Loss: 1.3871\nEpoch: 12/20... Step: 1620... Loss: 1.3588... Val Loss: 1.3860\nEpoch: 12/20... Step: 1640... Loss: 1.3545... Val Loss: 1.3815\nEpoch: 12/20... Step: 1660... Loss: 1.3829... Val Loss: 1.3791\nEpoch: 13/20... Step: 1680... Loss: 1.3663... Val Loss: 1.3762\nEpoch: 13/20... Step: 1700... Loss: 1.3410... Val Loss: 1.3682\nEpoch: 13/20... Step: 1720... Loss: 1.3320... Val Loss: 1.3714\nEpoch: 13/20... Step: 1740... Loss: 1.3370... Val Loss: 1.3700\nEpoch: 13/20... Step: 1760... Loss: 1.3333... Val Loss: 1.3665\nEpoch: 13/20... Step: 1780... Loss: 1.3119... Val Loss: 1.3664\nEpoch: 13/20... Step: 1800... Loss: 1.3452... Val Loss: 1.3621\nEpoch: 14/20... Step: 1820... Loss: 1.3277... Val Loss: 1.3592\nEpoch: 14/20... Step: 1840... Loss: 1.3024... Val Loss: 1.3529\nEpoch: 14/20... Step: 1860... Loss: 1.3390... Val Loss: 1.3527\nEpoch: 14/20... Step: 1880... Loss: 1.3363... Val Loss: 1.3572\nEpoch: 14/20... Step: 1900... Loss: 1.3279... Val Loss: 1.3513\nEpoch: 14/20... Step: 1920... Loss: 1.3175... Val Loss: 1.3486\nEpoch: 14/20... Step: 1940... Loss: 1.3502... Val Loss: 1.3472\nEpoch: 15/20... Step: 1960... Loss: 1.3099... Val Loss: 1.3429\nEpoch: 15/20... Step: 1980... Loss: 1.2976... Val Loss: 1.3449\nEpoch: 15/20... Step: 2000... Loss: 1.2790... Val Loss: 1.3416\nEpoch: 15/20... Step: 2020... Loss: 1.3179... Val Loss: 1.3448\nEpoch: 15/20... Step: 2040... Loss: 1.3053... Val Loss: 1.3367\nEpoch: 15/20... Step: 2060... Loss: 1.2972... Val Loss: 1.3361\nEpoch: 15/20... Step: 2080... Loss: 1.2954... Val Loss: 1.3366\nEpoch: 16/20... Step: 2100... Loss: 1.2878... Val Loss: 1.3337\nEpoch: 16/20... Step: 2120... Loss: 1.2944... Val Loss: 1.3329\nEpoch: 16/20... Step: 2140... Loss: 1.2800... Val Loss: 1.3280\nEpoch: 16/20... Step: 2160... Loss: 1.2886... Val Loss: 1.3341\nEpoch: 16/20... Step: 2180... Loss: 1.2852... Val Loss: 1.3259\nEpoch: 16/20... Step: 2200... Loss: 1.2744... Val Loss: 1.3242\nEpoch: 16/20... Step: 2220... Loss: 1.2852... Val Loss: 1.3265\nEpoch: 17/20... Step: 2240... Loss: 1.2719... Val Loss: 1.3229\nEpoch: 17/20... Step: 2260... Loss: 1.2706... Val Loss: 1.3204\nEpoch: 17/20... Step: 2280... Loss: 1.2853... Val Loss: 1.3188\nEpoch: 17/20... Step: 2300... Loss: 1.2491... Val Loss: 1.3183\nEpoch: 17/20... Step: 2320... Loss: 1.2598... Val Loss: 1.3162\nEpoch: 17/20... Step: 2340... Loss: 1.2643... Val Loss: 1.3121\nEpoch: 17/20... Step: 2360... Loss: 1.2789... Val Loss: 1.3151\nEpoch: 18/20... Step: 2380... Loss: 1.2576... Val Loss: 1.3099\nEpoch: 18/20... Step: 2400... Loss: 1.2767... Val Loss: 1.3036\nEpoch: 18/20... Step: 2420... Loss: 1.2505... Val Loss: 1.3031\nEpoch: 18/20... Step: 2440... Loss: 1.2372... Val Loss: 1.3016\nEpoch: 18/20... Step: 2460... Loss: 1.2529... Val Loss: 1.2992\nEpoch: 18/20... Step: 2480... Loss: 1.2446... Val Loss: 1.2990\nEpoch: 18/20... Step: 2500... Loss: 1.2305... Val Loss: 1.2955\nEpoch: 19/20... Step: 2520... Loss: 1.2529... Val Loss: 1.2933\nEpoch: 19/20... Step: 2540... Loss: 1.2641... Val Loss: 1.2893\nEpoch: 19/20... Step: 2560... Loss: 1.2408... Val Loss: 1.2868\nEpoch: 19/20... Step: 2580... Loss: 1.2594... Val Loss: 1.2942\nEpoch: 19/20... Step: 2600... Loss: 1.2209... Val Loss: 1.2898\nEpoch: 19/20... Step: 2620... Loss: 1.2154... Val Loss: 1.2854\nEpoch: 19/20... Step: 2640... Loss: 1.2285... Val Loss: 1.2812\nEpoch: 20/20... Step: 2660... Loss: 1.2309... Val Loss: 1.2862\nEpoch: 20/20... Step: 2680... Loss: 1.2355... Val Loss: 1.2777\nEpoch: 20/20... Step: 2700... Loss: 1.2338... Val Loss: 1.2938\nEpoch: 20/20... Step: 2720... Loss: 1.2080... Val Loss: 1.2781\nEpoch: 20/20... Step: 2740... Loss: 1.1992... Val Loss: 1.2762\nEpoch: 20/20... Step: 2760... Loss: 1.1839... Val Loss: 1.2845\nEpoch: 20/20... Step: 2780... Loss: 1.2650... Val Loss: 1.2739\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Checkpoint\n\nПосле обучения сохраним модель, чтобы можно было загрузить ее позже. Здесь сохраняются параметры, необходимые для создания той же архитектуры, гиперпараметры скрытого слоя и токены.","metadata":{"id":"ZfZxvNoDceEm"}},{"cell_type":"code","source":"# change the name, for saving multiple files\nmodel_name = 'lstm_20th_epoch.net'\n\ncheckpoint = {'n_hidden': net.n_hidden,\n              'n_layers': net.n_layers,\n              'state_dict': net.state_dict(),\n              'tokens': net.chars}\n\nwith open(model_name, 'wb') as f:\n    torch.save(checkpoint, f)","metadata":{"id":"q6RXl5VAceEm","execution":{"iopub.status.busy":"2023-02-14T12:51:58.003902Z","iopub.execute_input":"2023-02-14T12:51:58.004256Z","iopub.status.idle":"2023-02-14T12:51:58.046723Z","shell.execute_reply.started":"2023-02-14T12:51:58.004226Z","shell.execute_reply":"2023-02-14T12:51:58.045814Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"---\n## Делаем предсказания\n\nТеперь, когда мы обучили модель, сделаем предсказание следующих символов! Для предсказания мы передаем последний символ, и сеть предсказывает следующий символ, который мы потом передаем снова на вхол и получаем еще один предсказанный символ и так далее...\n\nНаши прогнозы основаны на категориальном распределении вероятностей по всем возможным символам. Мы можем ограничить число символов, чтобы сделать получаемый предсказанный текст более разумным, рассматривая только некоторые наиболее вероятные символы $K$. Это не позволит сети выдавать нам совершенно абсурдные прогнозы, а также позволит внести некоторый шум и случайность в выбранный текст. Узнать больше [можно здесь](https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk).","metadata":{"id":"K2sJhx5iceEm"}},{"cell_type":"code","source":"def predict(net, char, h=None, top_k=None):\n        ''' На вход подается символ, ф-я предсказания след символа\n            На выход - предсказанный символ и новое скрытое состояние\n        '''\n        \n        # переводим новый сивол в ohe-вектор\n        x = np.array([[net.char2int[char]]])\n        x = one_hot_encode(x, len(net.chars))\n        inputs = torch.from_numpy(x)\n        \n        if(train_on_gpu):\n            inputs = inputs.cuda()\n        \n        # detach hidden state from history\n        h = tuple([each.data for each in h])\n        # получаем выходы модели\n        out, h = net(inputs, h)\n\n        # находим вероятности по логитам\n        p = F.softmax(out, dim=1).data\n        if(train_on_gpu):\n            p = p.cpu() # move to cpu\n        \n        if top_k is None:\n            top_ch = np.arange(len(net.chars))\n        else:\n            p, top_ch = p.topk(top_k)\n            top_ch = top_ch.numpy().squeeze()\n        \n        # выбираем след элемент с некоторой степенью рандома\n        p = p.numpy().squeeze()\n        char = np.random.choice(top_ch, p=p/p.sum())\n        \n        # возвращаем предсказ символ и новое скрыт сост\n        return net.int2char[char], h","metadata":{"id":"QEIRW_B2ceEm","execution":{"iopub.status.busy":"2023-02-14T13:02:17.562880Z","iopub.execute_input":"2023-02-14T13:02:17.563357Z","iopub.status.idle":"2023-02-14T13:02:17.574298Z","shell.execute_reply.started":"2023-02-14T13:02:17.563313Z","shell.execute_reply":"2023-02-14T13:02:17.573117Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"### Priming и генерирование текста\n\nНужно задать скрытое состояние, чтобы сеть не генерировала произвольные символы. ","metadata":{"id":"OG38j3gQceEm"}},{"cell_type":"code","source":"def sample(net, size, prime='The', top_k=None):\n        \n    if(train_on_gpu):\n        net.cuda()\n    else:\n        net.cpu()\n    \n    net.eval() # режим предсказывания\n    \n    # для начало \"поглотим\" данный нам текст, чтобы логически оттталкиваться от него\n    chars = [ch for ch in prime]\n    h = net.init_hidden(1)\n    for ch in prime:\n        char, h = predict(net, ch, h, top_k=top_k)\n\n    chars.append(char)\n    \n    # теперь будем передавать последний символ и получать следующий\n    for ii in range(size):\n        char, h = predict(net, chars[-1], h, top_k=top_k)\n        chars.append(char)\n\n    return ''.join(chars)","metadata":{"id":"P9vpB5gRceEm","execution":{"iopub.status.busy":"2023-02-14T13:37:18.745809Z","iopub.execute_input":"2023-02-14T13:37:18.746287Z","iopub.status.idle":"2023-02-14T13:37:18.755991Z","shell.execute_reply.started":"2023-02-14T13:37:18.746237Z","shell.execute_reply":"2023-02-14T13:37:18.754822Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"print(sample(net, 1000, prime='Anna', top_k=5))","metadata":{"id":"BqmFA9eEceEm","execution":{"iopub.status.busy":"2023-02-14T13:37:18.951748Z","iopub.execute_input":"2023-02-14T13:37:18.952482Z","iopub.status.idle":"2023-02-14T13:37:19.522045Z","shell.execute_reply.started":"2023-02-14T13:37:18.952447Z","shell.execute_reply":"2023-02-14T13:37:19.521005Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"Anna\ntrying to talk about the most the thousand, and as that to be sure\nof that morning. And how to see him we consider to be, but he saw to\nand her husband his senticious cross to the position to him.\n\n\"What do you think that he won't come to me, but I'm going over them with\na long while, and there's not the first to set yourself,\" said\nStepan Arkadyevitch, walking into the carriage of such things\non the mother--to a little child only were answer to his hand.\n\nThe sorn on the carriase was not settled and still higher from his huge\nface, he was as he had at all that she said. After all astors, and a son of\nstarting it had taken the marshal, but ansters of corneds, true that\nthey were angroused her shoulders, and the mather, with a smothed\nsoulded hands. And all the complete people would be an harishing all the promate and sore\nof theirse or this conversation to him.\n\n\"Well, I'm all at our forever that you are thought in which I should hear that your husband\nsend him all the council to me fo\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Loading a checkpoint","metadata":{"id":"942mjdQHceEm"}},{"cell_type":"code","source":"# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\nwith open('lstm_20th_epoch.net', 'rb') as f:\n    checkpoint = torch.load(f)\n    \nloaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\nloaded.load_state_dict(checkpoint['state_dict'])","metadata":{"id":"Xt9ldUuSceEm","execution":{"iopub.status.busy":"2023-02-14T13:03:51.129390Z","iopub.execute_input":"2023-02-14T13:03:51.129869Z","iopub.status.idle":"2023-02-14T13:03:51.204629Z","shell.execute_reply.started":"2023-02-14T13:03:51.129824Z","shell.execute_reply":"2023-02-14T13:03:51.203804Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"# Sample using a loaded model\nprint(sample(loaded, 2000, top_k=5, prime=\"And Levin said\"))","metadata":{"id":"Ut6R3zDcceEm","execution":{"iopub.status.busy":"2023-02-14T13:04:14.359195Z","iopub.execute_input":"2023-02-14T13:04:14.359584Z","iopub.status.idle":"2023-02-14T13:04:15.440787Z","shell.execute_reply.started":"2023-02-14T13:04:14.359551Z","shell.execute_reply":"2023-02-14T13:04:15.439739Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"And Levin said:\n\nThese walked about. He had a good table wingon.\n\n\"That's in a minute,\" said Sergey Ivanovitch, with a smile, \"than\nthe world of his wife has not telling the most, bus silly and\nan implasting anything, at all, and simply so sorry as I'm ashamed.\"\n\n\"You're anyone to do?\" added, \"the sorts of a man, thonger of it\nto some tears of her. But that's as if the side of it.\n\n\"Well, I'd been such as all so to do all, you're all all over. We are\nnever coming. What do you true that I have a last times it was,\" she\nsaid to himself at the same time in the most dismiling, \"I have seen a long while,\nand he say, that I do not anyway, and wouldn't you any soul.\"\n\n\"Yes, yes; I'm asked that,\" said Vronsky.\n\nThe card at once so in the sorts and would not be calmed at the\nstretters which cared to the steps with the plate, and the\nsinger the farely was as he had stooped him. And that she had not\ncame out of his strange, some old man had been satisfaction\nthat we have been to be already she saw that he was to be in the pronection of\ntalents and happy and time with them with a change of senting his\nconsciousnity.\n\nAlexey Alexandrovitch had all before her, he had to give them.\n\n\"I don't know, that we have not already.\"\n\n\"It's a possible in the same that she went to begin to see her\nthere, and I was to go out from him, that I soon we go to\nsee anything,\" she said to\nherself, and said some time, and, with a mustress staring the\nfound of a most contrant of the condition of the same\nstrangeling to him. Had taken in the striggle those charming and attitude over\nseriousness.\n\n\"I have son to say it? If you, that I was a little son. That's\nthe meanance of he had always too, and was it all straightened as it's the carring\nhim with this stup the throates, and that itself is said,\nand that in the weakness of a celating saying what worrial sorred in something already. And\nthis is some suppress,\" answered Levin. \"Yes, you consider the conviction in\na\nchampagne, but I see a lutting off, and I\nwanted to s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Полезные ссылки:\n\n\n*   [Блог-пост Christopher'а Olah'а по LSTM](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)","metadata":{"id":"7rhp9qC8neCk"}},{"cell_type":"code","source":"","metadata":{"id":"A5Twt8oBnell"},"execution_count":null,"outputs":[]}]}