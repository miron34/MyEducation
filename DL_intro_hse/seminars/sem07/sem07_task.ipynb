{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYqqG4qcp7oB"
   },
   "source": [
    "# Эмбеддинги слов. Skip-gram Word2Vec\n",
    "\n",
    "## Хорошие ссылки для чтения\n",
    "\n",
    "\n",
    "* Полезный [общий взгляд](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) на Word2Vec\n",
    "* [Первые работы с Word2Vec](https://arxiv.org/pdf/1301.3781.pdf) \n",
    "* [Neural Information Processing Systems, paper](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) с улучшениями Word2Vec \n",
    "\n",
    "---\n",
    "## Word embeddings\n",
    "\n",
    "Для задач обработки текстов, в которых анализируемой единицей являются слова (part of speech tagging, named entity recognition, генерация текста, etc.) бывает полезно пользоваться готовыми признаковыми описаниями слов, заранее обученными за нас.\n",
    "Такие признаковые описания обычно обучают в форме векторов и называют эмбеддингами.\n",
    "Далее, при решении конкретной задачи, слова в текстах заменяют обученные эмбеддинги и поверх этого дела уже строят разные модели.\n",
    "\n",
    "<img src='https://github.com/udacity/deep-learning-v2-pytorch/blob/master/word2vec-embeddings/assets/one_hot_encoding.png?raw=1' width=50%>\n",
    "\n",
    "Конечно же, как мы это делали в предыдущем курсе, можно, использовать one-hot кодирование для каждого слова.\n",
    "One-hot векторы далее можно обрабатывать обычным линейныйм слоем, обучать его как часть модели и получать, в целом, похожую систему.\n",
    "Проблем в таком подходе, однако, хватает. \n",
    "Во-первых, умножение строки в виде one-hot вектора на матрицу линейного слоя можно заменить на простую индексацию строки матрицы этого слоя. \n",
    "Во-вторых, мы можем быть заинтересованы в хорошо обученных эмбеддингах на большом датасете (чтобы векторные представления хорошо отражали смысл слов), а в нашей конкретной задаче структура и/или количество данных могут отличаться.\n",
    "В-третьих, если у нас есть основания полагать, что готовые эмбеддинги хорошо подходят решаемой задачи, то мы можем немного сэкономить на обучении очень большого линейного слоя, ведь матрица эмбеддинигов имеет размеры `vocab_size x embed_size` и для стандартного словаря (десятки тысяч слов) может занимать больше 10 мегабайт памяти.\n",
    "\n",
    "<img src='https://github.com/udacity/deep-learning-v2-pytorch/blob/master/word2vec-embeddings/assets/lookup_matrix.png?raw=1' width=50%>\n",
    "\n",
    "Подведём небольшие итоги.\n",
    "Наша цель - получить векторные представления (одинакового размера) для каждого слова из словаря.\n",
    "При этом перед тем, как обрабатывать слова из текста моделью, мы хотим по номеру слова в словаре брать из таблицы эмбеддингов нужный вектор и далее работать уже с ним как с чем-то, что является признаковым описанием слова.\n",
    "\n",
    "<img src='https://github.com/udacity/deep-learning-v2-pytorch/blob/master/word2vec-embeddings/assets/tokenize_lookup.png?raw=1' width=50%>\n",
    " \n",
    "\n",
    "Конечно, эмбеддинги используются не только для слов. \n",
    "Эмбеддингом называют любую векторную репрезентацию дискретных объектов: слов (или частей слов), пользователей сервиса и чего только не."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrg11LAsp7oB"
   },
   "source": [
    "---\n",
    "## Word2Vec\n",
    "\n",
    "Сегодня мы реализуем один из самых известных подходов к построению эмбеддингов слов - Word2Vec.\n",
    "Этот алгоритм позволит нам не просто получить какие-то векторы чисел, соответствующих словам, а ещё и сохранит семантику слов в этих векторах.\n",
    "Занимательное свойство про семантику слов проявляется в том, что в полученном пространстве эмбеддингов векторы, соответствующие словам с близкими смысломи, будут иметь маленькое расстояние между друг другом.\n",
    "\n",
    "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/word2vec-embeddings/assets/context_drink.png?raw=1\" width=40%>\n",
    "\n",
    "Слова, которые появляются в похожих **контекстах**, таких как «кофе», «чай» и «вода», будут иметь векторы рядом друг с другом. Различные слова будут дальше друг от друга, а отношения могут быть представлены расстоянием в векторном пространстве.\n",
    "Построение подхода, удовлетворяющего такому свойству, отчасти отражает наше понимание слов, ведь при прочтении неизвестного нам слова в *достаточном* количестве конекстов, мы научаемся понимать его смысл.\n",
    "\n",
    "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/word2vec-embeddings/assets/vector_distance.png?raw=1\" width=40%>\n",
    "\n",
    "\n",
    "Для реализации Word2Vec существует два подхода:\n",
    "* **CBOW** (Continuous Bag-Of-Words). По контексту (словам вокруг) слова пытаемся предсказать центральное слово.\n",
    "* **Skip-gram**. По центральному слову контекста пытаемся предсказать слова из контекста.\n",
    "\n",
    "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/word2vec-embeddings/assets/word2vec_architectures.png?raw=1\" width=60%>\n",
    "\n",
    "На этом семинаре мы будем использовать **архитектуру skip-gram**, потому что она работает лучше, чем CBOW."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tn4RsC-rp7oB"
   },
   "source": [
    "---\n",
    "### Loading Data\n",
    "\n",
    "Для начала скачаем данные\n",
    "\n",
    "1. Скачаем [dataset](https://s3.amazonaws.com/video.udacity-data.com/topher/2018/October/5bbe6499_text8/text8.zip); файл очищенного *текста статьи в Википедии* от Мэтта Махони\n",
    "2. Расположим эти данные в  `data`\n",
    "3. Затем удалим скачанный файл для экономии памяти\n",
    "\n",
    "После этого мы оставим только один файл в репозитории: `data/text8`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YkuGJuoqAFKs",
    "outputId": "e202b134-1f85-4d20-a181-6cb6c1722d82"
   },
   "outputs": [],
   "source": [
    "# !wget https://s3.amazonaws.com/video.udacity-data.com/topher/2018/October/5bbe6499_text8/text8.zip \n",
    "# !unzip text8.zip\n",
    "# !rm text8.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o65mAtMVADv5",
    "outputId": "a6525cee-53e6-441a-95be-03fd82368b64"
   },
   "outputs": [],
   "source": [
    "# read in the extracted text file      \n",
    "with open('text8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# print out the first 100 characters\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ck6w80Q5p7oD"
   },
   "source": [
    "### Pre-processing\n",
    "\n",
    "Здесь мы чистим текст, чтобы облегчить обучение. \n",
    "За нас вся логика реализована в функции `preprocess` из файла `utils.py`. \n",
    "Функция делает несколько вещей:\n",
    "* Преобразует любые знаки препинания в токены, поэтому точка заменяется на `<PERIOD>`. В этом наборе данных нет никаких периодов, но это поможет при решении других задач. \n",
    "* Удаляет все слова, которые встречаются в наборе данных пять или меньше раз. Это значительно уменьшит проблемы, связанные с шумом в данных, и улучшит качество векторных представлений.\n",
    "* Она возвращает список слов в тексте.\n",
    "\n",
    "Это может занять несколько секунд, так как наш текстовый файл довольно большой. Если вы хотите написать свои собственные функции для этого материала, дерзайте!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aFUmJtPup7oD",
    "outputId": "86859698-2a99-4d3f-efdf-aaa9bdca7ea7"
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "\n",
    "words = utils.preprocess(text)\n",
    "print(words[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dsOItB4Ip7oD",
    "outputId": "7a05c906-c8bd-4894-d7b8-d72392d9e5a0"
   },
   "outputs": [],
   "source": [
    "# print some stats about this word data\n",
    "print(f\"Total words in text: {len(words)}\")\n",
    "print(f\"Unique words: {len(set(words))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yC2mLtwip7oD"
   },
   "source": [
    "#### Dictionaries\n",
    "\n",
    "Затем мы создаем два словаря для преобразования слов в целые числа и обратно (целые числа в слова). \n",
    "Это снова делается с помощью функции из файла `utils.py`: `create_lookup_tables` принимает список слов в тексте и возвращает два словаря.\n",
    "Целые числа присваиваются в порядке убывания частоты, поэтому самому частому слову («the») присваивается целое число 0, а следующему по частоте - 1 и так далее. \n",
    "\n",
    "Когда у нас есть словари, слова преобразуются в целые числа и сохраняются в списке `int_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "05eK3RZSp7oD",
    "outputId": "20aa95a5-92ee-4640-87ff-e647041c061b"
   },
   "outputs": [],
   "source": [
    "vocab_to_int, int_to_vocab = utils.create_lookup_tables(words)\n",
    "int_words = [vocab_to_int[word] for word in words]\n",
    "\n",
    "print(int_words[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5HDfby5p7oD"
   },
   "source": [
    "#### Subsampling\n",
    "\n",
    "Часто встречающиеся слова, такие как «the», «of» и «for», не обеспечивают особого контекста для близлежащих слов. \n",
    "Если мы отбросим некоторые из них, мы сможем удалить часть шума из наших данных и взамен получить более быстрое обучение и лучшее представление. \n",
    "Этот процесс иногда называют субдискретизацией. \n",
    "Для каждого слова $w_i$ в обучающем наборе мы отбрасываем его с вероятностью, равной\n",
    "\n",
    "$$ P(w_i) = 1 - \\sqrt{\\frac{t}{f(w_i)}} $$\n",
    "\n",
    "где $t$ - пороговый параметр, а $f(w_i)$ - частота слова $w_i$ в общем наборе данных.\n",
    "\n",
    "$$ P(0) = 1 - \\sqrt{\\frac{1*10^{-5}}{10^6/(16*10^6)}} = 0.98735 $$\n",
    "\n",
    "<br>\n",
    "\n",
    "**Упражнение:** Реализуйте подвыборку для слов в `int_words`. То есть пройдите через `int_words` и отбросьте каждое слово с вероятностью $ P (w_i) $, показанной выше. Обратите внимание, что $ P (w_i) $ - это вероятность того, что слово будет отброшено. Назначьте данные с подвыборкой для `train_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EenbWXACp7oD",
    "outputId": "11704915-5618-429b-9b32-b65c8f8fc93f"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "\n",
    "threshold = 1e-5\n",
    "word_counts = Counter(int_words)  # dictionary with number of appearances for each word\n",
    "print(f\"42-th word appears in the text {word_counts[42]} times\")  \n",
    "\n",
    "# discard some frequent words, according to the subsampling method\n",
    "# create a new list of words for training\n",
    "\n",
    "train_words = # <your code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(int_words[:15])\n",
    "print(train_words[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLCZzZVrp7oD"
   },
   "source": [
    "### Making batches\n",
    "\n",
    "Теперь, когда мы завершили препроцессинг данных, нам осталось правильно сформировать обучающие примеры, чтобы обучить модель. \n",
    "В архитектуре skip-gram для каждого слова в тексте мы хотим определить окружающий _context_ и захватить все слова в окне вокруг этого слова с размером $ C $. \n",
    "\n",
    "Из [статьи](https://arxiv.org/pdf/1301.3781.pdf):\n",
    "\n",
    "*Поскольку более далекие слова обычно меньше связаны с текущим словом, чем близкие к нему, мы придаем меньший вес удаленным словам, отбирая меньшее количество из этих слов в наших обучающих примерах ... Если мы выберем $ C = 5 $, то для каждого обучающего слова мы выбираем случайным образом число $ R $ в диапазоне $ [1: C] $, а затем используем слова $ R $ из предыдущих слов и слова $ R $ из будущего текущего слова в качестве правильных меток.*\n",
    "\n",
    "<br>\n",
    "\n",
    "**Упражнение:** Реализуйте функцию `get_target`, которая получает список слов, индекс и размер окна, а затем возвращает список слов в окне вокруг индекса. Обязательно используйте алгоритм, описанный выше, где вы выбрали случайное количество слов из окна.\n",
    "\n",
    "Скажем, у нас есть вход, и нас интересует токен `idx = 2`, `741`: \n",
    "```\n",
    "[5233, 58, 741, 10571, 27349, 0, 15067, 58112, 3580, 58, 10712]\n",
    "```\n",
    "\n",
    "Для R = 2 функция get_target должна возвращать список из четырех значений:\n",
    "```\n",
    "[5233, 58, 10571, 27349]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LmzEBIfdp7oD"
   },
   "outputs": [],
   "source": [
    "def get_target(words, idx, window_size=5):\n",
    "    \"\"\"\n",
    "    Get a list of words in a random-sized window around an index.\n",
    "    \"\"\"\n",
    "    # <your code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "259vdIDVp7oD",
    "outputId": "013606c4-2498-4a2d-e36e-27b82bedb2d5"
   },
   "outputs": [],
   "source": [
    "# test your code!\n",
    "\n",
    "# run this cell multiple times to check for random window selection\n",
    "int_text = [i for i in range(10)]\n",
    "print('Input: ', int_text)\n",
    "idx = 5 # word index of interest\n",
    "\n",
    "target = get_target(int_text, idx=idx, window_size=5)\n",
    "print('Target: ', target)  # you should get some indices around the idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucXKA7zxp7oD"
   },
   "source": [
    "#### Генерируем батчи \n",
    "\n",
    "Вот функция-генератор, которая возвращает пакеты входных и целевых данных для нашей модели, используя функцию get_target, описанную выше. Идея в том, что он берет слова `batch_size` из списка слов. Затем для каждого из этих пакетов слова отображаются в окне."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HghTPaSgp7oD"
   },
   "outputs": [],
   "source": [
    "def get_batches(words, batch_size, window_size=5):\n",
    "    \"\"\"\n",
    "    Create a generator of word batches as a tuple (inputs, targets)\n",
    "    \"\"\"\n",
    "    \n",
    "    n_batches = len(words) // batch_size\n",
    "    \n",
    "    # only full batches\n",
    "    words = words[:n_batches * batch_size]\n",
    "    \n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        x, y = [], []\n",
    "        batch = words[idx:idx + batch_size]\n",
    "        for ii in range(len(batch)):\n",
    "            batch_x = batch[ii]\n",
    "            batch_y = get_target(batch, ii, window_size)\n",
    "            y.extend(batch_y)\n",
    "            x.extend([batch_x] * len(batch_y))\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1TJ2c2XDp7oD",
    "outputId": "87ddcf50-f9ac-4cba-a614-12cb59c9d73c"
   },
   "outputs": [],
   "source": [
    "int_text = [i for i in range(20)]\n",
    "x, y = next(get_batches(int_text, batch_size=4, window_size=5))\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y: {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eeqYgmh2p7oD"
   },
   "source": [
    "### Строим модель\n",
    "\n",
    "Ниже представлена ​​примерная схема общей структуры нашей сети.\n",
    "\n",
    "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/word2vec-embeddings/assets/skip_gram_arch.png?raw=1\" width=60%>\n",
    "\n",
    "* Входные слова передаются как батчи индексов входных слов. Целевые переменные для каждого входного слова - номер слова из контекста.\n",
    "* Входные слова обрабатываются линейным слоем `vocab_size x embed_size`. \n",
    "* Затем полученные эмбеддинги обрабатываются выходным линейным слоем размера `embed_size x vocab_size` и к выходам применяется лосс для задачи классификации.\n",
    "\n",
    "Идея состоит в том, чтобы обучить матрицу весов слоя эмбеддингов и найти эффективные представления для наших слов. После обучения мы можем отбросить слой softmax, потому что нам не нужно делать прогнозы с помощью этой сети. Нам просто нужна матрица эмбеддингов, чтобы мы могли использовать ее в _других_ сетях, которые мы строим с использованием этого набора данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hS3IjZHDp7oD"
   },
   "source": [
    "### Validation\n",
    "\n",
    "Здесь нужно создать функцию, которая поможет нам наблюдать за нашей моделью в процессе обучения. Нужно выбрать несколько общих слов и несколько необычных слов. Затем мы распечатаем ближайшие к ним слова, используя косинусное сходство:\n",
    "\n",
    "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/word2vec-embeddings/assets/two_vectors.png?raw=1\" width=30%>\n",
    "\n",
    "$$\n",
    "\\mathrm{similarity} = \\cos(\\theta) = \\frac{\\vec{a} \\cdot \\vec{b}}{|\\vec{a}||\\vec{b}|}\n",
    "$$\n",
    "\n",
    "\n",
    "Мы можем закодировать слова проверки как векторы $\\vec{a}$, используя таблицу эмбеддингов, а затем вычислить сходство с каждым вектором слов $\\vec{b}$ в таблице эмбеддингов. Имея сходство, мы можем распечатать проверочные слова и слова в нашей матрице эмбеддингов, семантически похожие на эти слова. Это хороший способ проверить, объединяет ли наша таблица эмбеддингов слова с похожими семантическими значениями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GCEQG5RJp7oD"
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(embedding, valid_size=16, valid_window=100, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Returns the cosine similarity of validation words with words in the embedding matrix.\n",
    "    Here, embedding should be a PyTorch embedding module.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Here we're calculating the cosine similarity between some random words and \n",
    "    # our embedding vectors. With the similarities, we can look at what words are\n",
    "    # close to our random words.\n",
    "    \n",
    "    # sim = (a . b) / |a||b|\n",
    "    \n",
    "    embed_vectors = embedding.weight\n",
    "    \n",
    "    # magnitude of embedding vectors, |b|\n",
    "    magnitudes = embed_vectors.pow(2).sum(dim=1).sqrt().unsqueeze(0)\n",
    "    \n",
    "    # pick N words from our ranges (0,window) and (1000,1000+window). lower id implies more frequent \n",
    "    valid_examples = np.array(random.sample(range(valid_window), valid_size // 2))\n",
    "    valid_examples = np.append(valid_examples,\n",
    "                               random.sample(range(1000,1000+valid_window), valid_size // 2))\n",
    "    valid_examples = torch.LongTensor(valid_examples).to(device)\n",
    "    \n",
    "    valid_vectors = embedding(valid_examples)\n",
    "    similarities = torch.mm(valid_vectors, embed_vectors.t()) / magnitudes\n",
    "        \n",
    "    return valid_examples, similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "guJsEYdBp7oD"
   },
   "source": [
    "### SkipGram model\n",
    "\n",
    "Определите и обучите модель SkipGram:\n",
    "* Определите [embedding layer](https://pytorch.org/docs/stable/nn.html#embedding)\n",
    "* Определите выходной слой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nTemhFnMp7oD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W8h3wgY_p7oD"
   },
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super().__init__()\n",
    "        # complete this SkipGram model\n",
    "        # <your code here>\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # define the forward behavior\n",
    "        # <your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCVd0yaXp7oD"
   },
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fg9IEgfZp7oD",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "embedding_dim = 128 # you can change, if you want\n",
    "\n",
    "model = SkipGram(len(vocab_to_int), embedding_dim)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "print_every = 200\n",
    "steps = 0\n",
    "epochs = 5\n",
    "batch_size = 1024\n",
    "\n",
    "# train for some number of epochs\n",
    "for e in trange(epochs, leave=True, desc=\"Epoch number\"):\n",
    "    \n",
    "    pbar = tqdm(\n",
    "        get_batches(train_words, batch_size), \n",
    "        leave=False, \n",
    "        desc=\"Batch number\", \n",
    "        total=len(train_words) // batch_size\n",
    "    )\n",
    "    \n",
    "    # get input and target batches\n",
    "    for inputs, targets in pbar:\n",
    "        steps += 1\n",
    "        inputs, targets = torch.LongTensor(inputs), torch.LongTensor(targets)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        log_ps = model(inputs)\n",
    "        loss = criterion(log_ps, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if steps % print_every == 0:                  \n",
    "            # getting examples and similarities      \n",
    "            valid_examples, valid_similarities = cosine_similarity(model.embed, device=device)\n",
    "            _, closest_idxs = valid_similarities.topk(6) # topk highest similarities\n",
    "            \n",
    "            valid_examples, closest_idxs = valid_examples.to(\"cpu\"), closest_idxs.to(\"cpu\")\n",
    "            for ii, valid_idx in enumerate(valid_examples):\n",
    "                closest_words = [int_to_vocab[idx.item()] for idx in closest_idxs[ii]][1:]\n",
    "                print(int_to_vocab[valid_idx.item()] + \" | \" + \", \".join(closest_words))\n",
    "            print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkpRD4A8p7oD"
   },
   "source": [
    "## Visualizing the word vectors\n",
    "\n",
    "Ниже мы будем использовать T-SNE, чтобы визуализировать, как наши многомерные словесные векторы группируются вместе. Прочитать [эту статью](http://colah.github.io/posts/2014-10-Visualizing-MNIST/), чтобы узнать больше о T-SNE и других способах визуализации многомерных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JoYaca7xp7oD"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bs-ZhlbPp7oD"
   },
   "outputs": [],
   "source": [
    "# getting embeddings from the embedding layer of our model, by name\n",
    "embeddings = model.embed.weight.to(\"cpu\").data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "za8ne2ADp7oE"
   },
   "outputs": [],
   "source": [
    "viz_words = 600\n",
    "tsne = TSNE()\n",
    "embed_tsne = tsne.fit_transform(embeddings[:viz_words, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V1oitQiqp7oE"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "for idx in range(viz_words):\n",
    "    plt.scatter(*embed_tsne[idx, :], color=\"steelblue\")\n",
    "    plt.annotate(int_to_vocab[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zPtklc_2p7oE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Verteeva_Word2Vec.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
