{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../logo.png\" height=\"200\" width=\"900\"> \n",
    "\n",
    "# <center> Сбор данных: грязная работа своими руками </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Как выглядит хранилище мемов \n",
    "\n",
    "## 1.1. Что мы хотим получить\n",
    "\n",
    "\n",
    "Итак, мы хотим  написать код, который поможет нам скачать полезные данные с сайта [knowyourmeme.com:](http://knowyourmeme.com)\n",
    "\n",
    "- **Name** – название мема,\n",
    "- **Origin_year** – год его создания,\n",
    "- **Views** – число просмотров,\n",
    "- **About** – текстовое описание мема,\n",
    "- ** и многие другие** \n",
    "\n",
    "После скачивания и чистки данных, можно будет заняться оцениванием моделей. Например, можно попробовать предсказать популярность мема по его параметрам. Но это все позже, а сейчас познакомимся с парой определений:\n",
    "\n",
    "* **Парсер** — это скрипт, который собирает информацию с сайта\n",
    "* **Краулер** — это часть парсера, которая переходит по ссылкам\n",
    "* **Краулинг** — это переход по страницам и ссылкам\n",
    "* **Скрапинг** — это сбор данных со страниц\n",
    "* **Парсинг** — это сразу и краулинг и скрапинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.  Что такое HTML \n",
    "\n",
    "**HTML (HyperText Markup Language)**  — это язык разметки. Он является стандартным для написания сайтов. Команды в таком языке называются **тегами**. Если открыть абсолютно любой сайт, нажать на правую кнопку мышки, а после нажать `View page source`, то перед вами предстанет HTML скелет этого сайта. \n",
    "\n",
    "Можно увидеть, что HTML-страница это ни что иное, как набор вложенных тегов. Можно заметить, например, следующие теги:\n",
    "\n",
    "- `<title>` – заголовок страницы\n",
    "- `<h1>…<h6>` – заголовки разных уровней\n",
    "- `<p>` – абзац (paragraph)\n",
    "- `<div>` – выделения фрагмента документа с целью изменения вида содержимого\n",
    "- `<table>` – прорисовка таблицы \n",
    "- `<tr>` – разделитель для строк в таблице \n",
    "- `<td>` – разделитель для столбцов в таблице\n",
    "- `<b>` – устанавливает жирное начертание шрифта\n",
    "\n",
    "Обычно команда `<...>` открывает тег, а  `</...>` закрывает его. Все, что находится между этими двумя командами, подчиняется правилу, которое диктует тег. Например, все, что находится между `<p>` и  `</p>` — это отдельный абзац.   \n",
    "\n",
    "Теги образуют своеобразное дерево с корнем в теге `<html>` и разбивают страницу на разные логические кусочки. У каждого тега могут быть свои потомки (дети) — те теги, которые вложены в него, и свои родители. \n",
    "\n",
    "Например, HTML-древо страницы может выглядеть вот так:\n",
    "\n",
    "    <html>\n",
    "    <head> Заголовок </head>\n",
    "    <body>\n",
    "        <div> \n",
    "            Первый кусок текста со своими свойствами\n",
    "        </div>\n",
    "        <div>\n",
    "            Второй кусок текста\n",
    "                <b>\n",
    "                    Третий кусок с выделенным текстом\n",
    "                </b>\n",
    "        </div>\n",
    "        Четвёртый кусок текста        \n",
    "    </body>\n",
    "    </html>            \n",
    "    \n",
    "    \n",
    "<img align=\"center\" src=\"pictures/tree.png\" height=\"500\" width=\"500\"> \n",
    "\n",
    "Можно работать с этим html как с текстом, а можно как с деревом. Обход этого дерева и есть парсинг веб-страницы. Нам нужно находить нужные нам узлы среди всего этого разнообразия и забирать с них информацию.\n",
    "\n",
    "Вручную обходить эти деревья неудобно, поэтому есть специальные языки для обхода деревьев.\n",
    "\n",
    "- [CSS-селектор](https://ru.wikibooks.org/wiki/CSS/%D0%A1%D0%B5%D0%BB%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D1%8B) (это когда мы ищем элемент страницы по паре ключ, значение)\n",
    "- [XPath](https://ru.wikipedia.org/wiki/XPath) (это когда мы прописываем путь по дереву вот так: /html/body/div[1]/div[3]/div/div[2]/div)\n",
    "- Различные библиотеки, например, BeautifulSoup для питона. Именно эту библиотеку мы и будем использовать. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Наш первый запрос\n",
    "\n",
    "Доступ к веб-станицам позволяет получать модуль `requests`. Подгрузим его и ещё пару пакетов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests      # Библиотека для отправки запросов\n",
    "import numpy as np   # Библиотека для матриц, векторов и линала\n",
    "import pandas as pd  # Библиотека для табличек \n",
    "import time          # Библиотека для времени"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для наших благородных исследовательских целей нужно собрать данные по каждому мему с соответствующей ему страницы. Но для начала нужно получить адреса этих страниц. Поэтому открываем основную страницу со всеми выложенными мемами. Выглядит она следующим образом:\n",
    "\n",
    "<img align=\"center\" src=\"pictures/memes_main.png\" height=\"500\" width=\"500\"> \n",
    "\n",
    "Отсюда мы и будем собирать ссылки. Сохраним в переменную `page_link` адрес основной страницы и откроем её при помощи библиотеки `requests`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_link = 'http://knowyourmeme.com/memes/all/page/1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [403]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get(page_link)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А вот и первая проблема! Обращаемся к [главному источнику знаний](https://en.wikipedia.org/wiki/HTTP_403) и выясняем, что 403-я ошибка выдается сервером, если он доступен и способен обрабатывать запросы, но по некоторым личным причинам отказывается это делать. \n",
    "\n",
    "Попробуем выяснить, почему. Для этого проверим, как выглядел финальный запрос, отправленный нами на сервер."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-Agent: python-requests/2.21.0\n",
      "Accept-Encoding: gzip, deflate\n",
      "Accept: */*\n",
      "Connection: keep-alive\n"
     ]
    }
   ],
   "source": [
    "for key, value in response.request.headers.items():\n",
    "    print(key+\": \"+value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Похоже, мы недвусмысленно дали понять серверу, что мы используем python, а именно библиотеку requests версии 2.14.2. Скорее всего, это вызвало у сервера некоторые подозрения относительно наших благих намерений и он решил нас безжалостно отвергнуть. Для сравнения, можно посмотреть, как выглядят request-headers у запроса через браузер:\n",
    "\n",
    "<img align=\"center\" src=\"pictures/good_headers.png\" height=\"800\" width=\"800\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очевидно, что нашему скромному запросу не тягаться с таким обилием мета-информации, которое передается при запросе из обычного браузера. К счастью, никто нам не мешает притвориться человечными.\n",
    "\n",
    "Библиотек, которые справляются с такой задачей, существует очень много, мы воспользуемся [`fake-useragent`](https://pypi.python.org/pypi/fake-useragent). При вызове метода из различных кусочков будет генерироваться случайное сочетание операционной системы, спецификаций и версии браузера, которые можно передавать в запрос:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fake_useragent import UserAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML like Gecko) Chrome/44.0.2403.155 Safari/537.36'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UserAgent().chrome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get(page_link, headers={'User-Agent': UserAgent().chrome})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cоединение установлено и данные получены."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<!DOCTYPE html>\\n<html xmlns:fb=\\'https://www.facebook.com/2008/fbml\\' xmlns=\\'https://www.w3.org/1999/xhtml\\'>\\n<head>\\n<meta content=\\'text/html; charset=utf-8\\' http-equiv=\\'Content-Type\\'>\\n<script type=\"text/javascript\">window.NREUM||(NREUM={});NREUM.info={\"beacon\":\"bam.nr-data.net\",\"errorBeacon\":\"bam.nr-data.net\",\"licenseKey\":\"c1a6d52f38\",\"applicationID\":\"31165848\",\"transactionName\":\"dFdfRUpeWglTQB8GDUNKWFRLHlcJWg==\",\"queueTime\":0,\"applicationTime\":54,\"agent\":\"\"}</script>\\n<script type=\"text/javascript\">(window.NREUM||(NREUM={})).loader_config={licenseKey:\"c1a6d52f38\",applicationID:\"31165848\"};window.NREUM||(NREUM={}),__nr_require=function(e,n,t){function r(t){if(!n[t]){var i=n[t]={exports:{}};e[t][0].call(i.exports,function(n){var i=e[t][1][n];return r(i||n)},i,i.exports)}return n[t].exports}if(\"function\"==typeof __nr_require)return __nr_require;for(var i=0;i<t.length;i++)r(t[i]);return r}({1:[function(e,n,t){function r(){}function i(e,n,t){return function(){return o(e,[u.now()].concat(f(arg'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = response.content\n",
    "html[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Beautiful Soup\n",
    "\n",
    "<img align=\"center\" src=\"pictures/soup.jpg\" height=\"200\" width=\"200\"> \n",
    "\n",
    "Пакет **[bs4 , a.k.a BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/)** был назван в честь стиха про красивый суп из Алисы в стране чудес.\n",
    "\n",
    "BeautifulSoup — это библиотека, которая из необработанного HTML кода страницы создаёт структурированный массив данных, по которому очень удобно искать необходимые теги, классы, атрибуты, тексты и прочие элементы веб страниц.\n",
    "\n",
    "> Пакет под названием `BeautifulSoup` — скорее всего, не то, что нам нужно. Это третья версия (*Beautiful Soup 3*), а мы будем использовать четвертую. Нужно будет установить пакет `beautifulsoup4`. Чтобы было совсем весело, при импорте нужно указывать другое название пакета — `bs4`, а импортировать функцию под названием `BeautifulSoup`. В общем, сначала легко запутаться, но эти трудности нужно преодолеть.\n",
    "\n",
    "```\n",
    "pip install beautifulsoup4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Передадим функции `BeautifulSoup` текст веб-страницы, которую мы скачали выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html,'html.parser')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "\n",
    "<html xmlns=\"http://www.w3.org/1999/xhtml\" xmlns:fb=\"http://www.facebook.com/2008/fbml\">\n",
    "<head>\n",
    "<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n",
    "<script type=\"text/javascript\">window.NREUM||(NREUM={});NREUM.info={\"beacon\":\"bam.nr-data.net\",\"errorBeacon\":\"bam.nr-data.net\",\"licenseKey\":\"c1a6d52f38\",\"applicationID\":\"31165848\",\"transactionName\":\"dFdfRUpeWglTQB8GDUNKWFRLHkUNWUU=\",\"queueTime\":0,\"applicationTime\":24,\"agent\":\"\"}</script>\n",
    "<script type=\"text/javascript\">window.NREUM||(NREUM={}),__nr_require=function(e,n,t){function r(t){if(!n[t]){var o=n[t]={exports:{}};e[t][0].call(o.exports,function(n){var o=e[t][1][n];return r(o||n)},o,o.exports)}return n[t].exports}if(\"function\"==typeof __nr_require)return __nr_require;for(var o=0;o<t.length;o++)r(t[o]);return r}({1:[function(e,n,t){function r(){}function o(e,n,t){return function(){return i(e,[c.now()].concat(u(arguments)),n?null:this,t),n?void 0:this}}var i=e(\"handle\"),a=e(2),u=e(3),f=e(\"ee\").get(\"tracer\"),c=e(\"loader\"),s=NREUM;\"undefined\"==typeof window.newrelic&&(newrelic=s);var p="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим что лежит внутри переменной `soup`. Невнимательный пользователь, скорее всего, скажет,что ничего вообще не изменилось. Тем не менее, это не так. Теперь мы можем свободно перемещаться по HTML-дереву страницы и искать нужные нам теги."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>All Entries | Know Your Meme</title>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.html.head.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно извлечь из того места, где мы оказались, текст с помощью метода `text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'All Entries | Know Your Meme'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.html.head.title.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более того, зная адрес элемента, мы сразу можем найти его. Например, можно сделать это по классу. Следующая команда должна найти элемент, который лежит внутри тега `a` и имеет класс `photo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"photo left\" href=\"/memes/berniebruh\" target=\"_self\"><img alt=\"Screenshot of three participants in the #BernieBruh movement. This article explains the #berniebruh hashtag campaign. \" data-src=\"https://i.kym-cdn.com/featured_items/icons/wide/000/010/485/53b.jpg\" height=\"112\" src=\"https://s.kym-cdn.com/assets/blank-b3f96f160b75b1b49b426754ba188fe8.gif\" title='#BernieBruh Hashtag Fights Back Against \"Bernie Bro\" Stereotypes' width=\"198\"/> <div class=\"info abs\"> <div class=\"c\"> #BernieBruh Hashtag Fights Back Against \"Bernie Bro\" Stereotypes </div> </div> </a>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = soup.find('a', attrs = {'class':'photo'})\n",
    "obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако, вопреки нашим ожиданиям, извлечённый объект имеет класс `\"photo left\"`. Оказывается, `BeautifulSoup4` расценивает аттрибуты `class` как набор отдельных значений, поэтому `\"photo left\"` для библиотеки равносильно `[\"photo\", \"left\"]`, а указанное нами значение этого класса `\"photo\"` входит в этот список. Чтобы избежать такой неприятной ситуации, придется воспользоваться собственной функцией и задать строгое соответствие:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"photo\" href=\"/memes/people/jreg\"><img alt=\"Jreg\" data-src=\"https://i.kym-cdn.com/entries/icons/medium/000/033/034/jreg.jpg\" src=\"https://s.kym-cdn.com/assets/blank-b3f96f160b75b1b49b426754ba188fe8.gif\" title=\"Jreg\"/> <div class=\"entry-labels\"> <span class=\"label label-submission\"> Submission </span> <span class=\"label\" style=\"background: #d32f2e; color: white;\">Person</span> </div> </a>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = soup.find(lambda tag: tag.name == 'a' and tag.get('class') == ['photo'])\n",
    "obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полученный после поиска объект также обладает структурой bs4. Поэтому можно продолжить искать нужные нам объекты уже в нём. Извлечём ссылку на этот мем. Сделать это можно по атрибуту `href`, в котором лежит наша ссылка. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/memes/people/jreg'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.attrs['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.get('href')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что после преобразований у данных поменялся тип. Теперь он `str`. Это означет, что с ними можно работать как с текстом. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тип данных до вытаскивания ссылки: <class 'bs4.element.Tag'>\n",
      "Тип данных после вытаскивания ссылки: <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Тип данных до вытаскивания ссылки:\", type(obj))\n",
    "print(\"Тип данных после вытаскивания ссылки:\", type(obj.attrs['href']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если несколько элементов на странице обладают указанным адресом, то метод `find` вернёт только самый первый.  Чтобы найти все элементы с таким адресом, нужно использовать метод `findAll`, и на выход будет выдан список. Таким образом, мы можем получить одним поиском сразу все объекты, содержащие ссылки на страницы с мемами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"photo\" href=\"/memes/people/jreg\"><img alt=\"Jreg\" data-src=\"https://i.kym-cdn.com/entries/icons/medium/000/033/034/jreg.jpg\" src=\"https://s.kym-cdn.com/assets/blank-b3f96f160b75b1b49b426754ba188fe8.gif\" title=\"Jreg\"/> <div class=\"entry-labels\"> <span class=\"label label-submission\"> Submission </span> <span class=\"label\" style=\"background: #d32f2e; color: white;\">Person</span> </div> </a>,\n",
       " <a class=\"photo\" href=\"/memes/sometimes-maybe-good-sometimes-maybe-shit\"><img alt=\"Sometimes maybe good, sometimes maybe shit\" data-src=\"https://i.kym-cdn.com/entries/icons/medium/000/033/033/gattuso.jpg\" src=\"https://s.kym-cdn.com/assets/blank-b3f96f160b75b1b49b426754ba188fe8.gif\" title=\"Sometimes maybe good, sometimes maybe shit\"/> <div class=\"entry-labels\"> <span class=\"label label-deadpool\"> Deadpool </span> </div> </a>,\n",
       " <a class=\"photo\" href=\"/memes/events/indonesian-child-rights-official-statement-about-woman-can-get-pregnant-in-swimming-pool\"><img alt=\"Indonesian Child Rights Official Statement About Woman Can Get Pregnant in Swimming Pool\" data-src=\"https://i.kym-cdn.com/entries/icons/medium/000/033/032/woman_can_get_pregnant.jpg\" src=\"https://s.kym-cdn.com/assets/blank-b3f96f160b75b1b49b426754ba188fe8.gif\" title=\"Indonesian Child Rights Official Statement About Woman Can Get Pregnant in Swimming Pool\"/> <div class=\"entry-labels\"> <span class=\"label label-submission\"> Submission </span> <span class=\"label\" style=\"background: #0b6a40; color: white;\">Event</span> </div> </a>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meme_links = soup.findAll(lambda tag: tag.name == 'a' and tag.get('class') == ['photo'])\n",
    "meme_links[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось очистить полученный список от лишнего:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "meme_links = [link.attrs['href'] for link in meme_links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/memes/people/jreg',\n",
       " '/memes/sometimes-maybe-good-sometimes-maybe-shit',\n",
       " '/memes/events/indonesian-child-rights-official-statement-about-woman-can-get-pregnant-in-swimming-pool',\n",
       " '/memes/scottish-twitter-reactions-to-the-2019-20-wuhan-coronavirus-outbreak',\n",
       " '/memes/subcultures/baldurs-gate',\n",
       " '/memes/girls-in-class-looking-back',\n",
       " '/memes/people/liam-allen-miller',\n",
       " '/memes/dave-chappelle-reading-white-people-magazine',\n",
       " '/memes/the-smeeze',\n",
       " '/memes/observe-my-superior-strategic-mind-at-work']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meme_links[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(meme_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Готово, получили ровно 16 ссылок по числу мемов на одной странице поиска. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы легче было искать адрес элемента, можно установить для своего браузера специальную утилиту, позволяющую извлекать со страницы нужные теги, например, [selectorgadget.](http://selectorgadget.com/)\n",
    "\n",
    "Тем не менее, этот путь не подходит для истинного самурая. Есть другой способ — искать теги для каждого нужного нам элемента вручную. Для этого придётся жать правой кнопкой мышки по окну браузера и жать кнопку **Исследовать элемент (Inspect)**. После браузер будет выглядеть так: \n",
    "\n",
    "<img align=\"center\" src=\"pictures/memes_inspection.png\" height=\"800\" width=\"800\"> \n",
    "\n",
    "Полученный html, в котором находится адрес выбранного вами объекта, можно смело копировать в код."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Собираем ссылки\n",
    "\n",
    "После того, как мы скачали все ссылки с текущей страницы, нам нужно каким-то образом перейти на соседнюю и начать скачивать ссылки с неё. На сайте это можно сделать пролистав страницу с мемами вниз, javascript-функции подгрузят новые мемы.\n",
    "\n",
    "Обычно, все параметры, которые мы устанавливаем на сайте для поиска, отражаются на структуре ссылки. Если мы хотим получить первую страницу с мемами, мы должны будем обратиться к сайту по ссылке \n",
    "\n",
    "                `http://knowyourmeme.com/memes/all/page/1`\n",
    "\n",
    "\n",
    "Если мы захотим получить вторую страницу, нам придётся заменить номер страницы на 2\n",
    "\n",
    "\n",
    "                `http://knowyourmeme.com/memes/all/page/2`\n",
    " \n",
    "Таким образом мы сможем пройтись по всем страницам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPageLinks(page_number):\n",
    "    \"\"\"\n",
    "        Возвращает список ссылок на мемы, полученный с текущей страницы\n",
    "        \n",
    "        page_number: int/string\n",
    "            номер страницы для парсинга\n",
    "            \n",
    "    \"\"\"\n",
    "    # составляем ссылку на страницу поиска\n",
    "    page_link = 'http://knowyourmeme.com/memes/all/page/{}'.format(page_number)\n",
    "    \n",
    "    # запрашиваем данные по ней\n",
    "    response = requests.get(page_link, headers={'User-Agent': UserAgent().chrome})\n",
    "    \n",
    "    if not response.ok:\n",
    "        # если сервер нам отказал, вернем пустой лист для текущей страницы\n",
    "        return [] \n",
    "    \n",
    "    # получаем содержимое страницы\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    \n",
    "    # ищем ссылки на мемы и очищаем их от ненужных тэгов\n",
    "    meme_links = soup.findAll(lambda tag: tag.name == 'a' and tag.get('class') == ['photo'])\n",
    "    meme_links = ['http://knowyourmeme.com' + link.attrs['href'] for link in meme_links]\n",
    "    \n",
    "    return meme_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://knowyourmeme.com/memes/people/jreg',\n",
       " 'http://knowyourmeme.com/memes/sometimes-maybe-good-sometimes-maybe-shit']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meme_links = getPageLinks(1)\n",
    "meme_links[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://knowyourmeme.com/memes/why-must-you-hurt-me-in-this-way',\n",
       " 'http://knowyourmeme.com/memes/taj-mahal-comparison-with-slums']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meme_links = getPageLinks(2)\n",
    "meme_links[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично, функция работает и теперь мы теоретически можем достать все $18000$ ссылок, для чего нам нужно будет пройтись по $\\frac{18000}{16} \\approx 1125$ страницам. Прежде чем расстраивать сервер таким количеством запросов, посмотрим, как доставать всю необходимую информацию о конкретном меме. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Скачиваем информацию об одном меме\n",
    "\n",
    "По аналогии со ссылками можно извдечь что угодно. Для этого надо сделать несколько шагов: \n",
    "\n",
    "1. Открываем страничку с мемом\n",
    "2. Находим любым способом тег для нужной нам информации\n",
    "3. Вызываем Beautiful Soap\n",
    "4. ......\n",
    "5. Profit \n",
    "\n",
    "Извлечём число просмотров мема.\n",
    "\n",
    "<img align=\"center\" src=\"pictures/doge_main.png\" height=\"600\" width=\"600\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "meme_page = 'http://knowyourmeme.com/memes/doge'\n",
    "\n",
    "response = requests.get(meme_page, headers={'User-Agent': UserAgent().chrome})\n",
    "\n",
    "html = response.content\n",
    "soup = BeautifulSoup(html,'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как можно извлечь статистику просмотров, комментариев, а также числа загруженных видео и фото, связанных с нашим мемом. Всё это хранится справа вверху под тэгами `\"dd\"` и с классами  `\"views\"`, `\"videos\"`, `\"photos\"` и `\"comments\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dd class=\"views\" title=\"13,176,671 Views\">\n",
       "<a href=\"/memes/doge\" rel=\"nofollow\">13,176,671</a>\n",
       "</dd>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "views = soup.find('dd', attrs={'class':'views'})\n",
    "views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'13,176,671'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "views = views.find('a').text\n",
    "views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13176671"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "views = int(views.replace(',', ''))\n",
    "views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию для сбора этой статистики. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def getStats(soup, stats):\n",
    "    \"\"\"\n",
    "        Возвращает очищенное число просмотров/коментариев/...\n",
    "        \n",
    "        soup: объект bs4.BeautifulSoup \n",
    "            представление текущей страницы\n",
    "            \n",
    "        stats: string\n",
    "            views/videos/photos/comments\n",
    "            \n",
    "    \"\"\"\n",
    "    try:\n",
    "        obj = soup.find('dd', attrs={'class':stats})\n",
    "        obj = obj.find('a').text\n",
    "        obj = int(obj.replace(',', ''))\n",
    "    except:\n",
    "        obj=None\n",
    "    \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Просмотры: 13176671\n",
      "Видео: 64\n",
      "Фото: 1698\n",
      "Комментарии: 918\n"
     ]
    }
   ],
   "source": [
    "views = getStats(soup, stats='views')\n",
    "videos = getStats(soup, stats='videos')\n",
    "photos = getStats(soup, stats='photos')\n",
    "comments = getStats(soup, stats='comments')\n",
    "\n",
    "print(\"Просмотры: {}\\nВидео: {}\\nФото: {}\\nКомментарии: {}\".format(views, videos, photos, comments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще из интересного и исследовательского —  достанем дату и время добавления мема. Если посмотреть на страницу в браузере, можно подумать, что максимум информации, который мы можем извлечь - это число лет, прошедших с момента публикации —  `Added 4 years ago by NovaXP`. Однако мы так просто сдаваться не будем, посмотрим что в html-коде страницы отвечает за эту надпись:\n",
    "\n",
    "<img align=\"center\" src=\"pictures/html_time_ago.png\" height=\"600\" width=\"600\"> \n",
    "\n",
    "Ага! Вот и подробности по дате добавления, с точностью до минуты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-02-03T05:33:03-05:00'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date = soup.find('abbr', attrs={'class':'timeago'}).attrs['title']\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На самом деле, парсеры — дело непредсказуемое. Часто страницы, которые мы парсим, имеют очень неоднородну структуру. Например, если мы парсим мемы, на части страниц может быть указано описание, а на части нет.\n",
    "\n",
    "Как только код впервые встречается с отсутствием описания, он выдаёт ошибку и останавливается. Чтобы нормально собрать все данные, приходится [прописывать исключения.](https://pythonworld.ru/tipy-dannyx-v-python/isklyucheniya-v-python-konstrukciya-try-except-dlya-obrabotki-isklyuchenij.html) Для этого используют  конструкцию `try - except` \n",
    "\n",
    "Например, мы хотим извлечь статус мема, для этого найдем окружающие его тэги:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = soup.find('aside', attrs={'class':'left'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dd>\n",
       "Confirmed\n",
       "</dd>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meme_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дальше нужно извлечь из тэгов текст и убрать лишние пробелы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Confirmed'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meme_status.text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако, если неожиданно выяснится, что у мема нет статуса, метод `find` вернёт пустоту. Метод `text`, в свою очередь, не сможет найти в тэгах текст и выдаст ошибку. Чтобы обезопасить себя от таких пустот, можно прописать исключение или `if - else`. Так как в текущем меме статус все-таки есть, нарочно зададим его как пустой объект, чтобы проверить, что ошибка поймается в обоих случаях"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception\n",
      "Empty\n"
     ]
    }
   ],
   "source": [
    "# Делай раз! Ищем статус мема, но не находим его\n",
    "meme_status = None\n",
    "\n",
    "# Делай два! Пытаемся извлечь его...\n",
    "\n",
    "# ... с исключениями\n",
    "try:\n",
    "    print(meme_status.text.strip()) \n",
    "# Если возникает ошибка, статус не найден, выдаём пустоту.\n",
    "except:\n",
    "    print(\"Exception\")\n",
    "    \n",
    "    \n",
    "# ... с проверкой на пустой элемент\n",
    "if meme_status:\n",
    "    print(meme_status.text.strip())\n",
    "else:\n",
    "    print(\"Empty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такой код позволяет обезопасить себя от ошибок во время работы кода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confirmed\n"
     ]
    }
   ],
   "source": [
    "properties = soup.find('aside', attrs={'class':'left'})\n",
    "meme_status = properties.find(\"dd\")\n",
    "\n",
    "meme_status = \"Empty\" if not meme_status else meme_status.text.strip()\n",
    "print(meme_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По аналогии можно извлечь всю остальную информацию со страницы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def getProperties(soup):\n",
    "    \"\"\"\n",
    "        Возвращает список (tuple) с названием, статусом, типом, \n",
    "        годом и местом происхождения и тэгами\n",
    "        \n",
    "        soup: объект bs4.BeautifulSoup \n",
    "            представление текущей страницы\n",
    "    \n",
    "    \"\"\"\n",
    "    # название - идёт с самым большим заголовком h1, легко найти\n",
    "    meme_name = soup.find('section', attrs={'class':'info'}).find('h1').text.strip()\n",
    "    \n",
    "    # достаём все данные справа от картинки \n",
    "    properties = soup.find('aside', attrs={'class':'left'})\n",
    "    \n",
    "    # статус идет первым - можно не уточнять класс\n",
    "    meme_status = properties.find(\"dd\")\n",
    "    \n",
    "    # oneliner, заменяющий try-except: если тэга нет в properties, вернётся объект NoneType,\n",
    "    # у которого аттрибут text отсутствует, и в этом случае он заменится на пустую строку\n",
    "    meme_status = \"\" if not meme_status else meme_status.text.strip()\n",
    "    \n",
    "    # тип мема - обладает уникальным классом\n",
    "    meme_type = properties.find('a', attrs={'class':'entry-type-link'})\n",
    "    meme_type = \"\" if not meme_type else meme_type.text \n",
    "    \n",
    "    # год происхождения первоисточника можно найти после заголовка Year, \n",
    "    # находим заголовок, определяем родителя и ищем следущего ребенка - наш раздел\n",
    "    meme_origin_year = properties.find(text='\\nYear\\n')\n",
    "    meme_origin_year = \"\" if not meme_origin_year else meme_origin_year.parent.find_next()\n",
    "    meme_origin_year = meme_origin_year.text.strip()\n",
    "    \n",
    "    # сам первоисточник\n",
    "    meme_origin_place = properties.find('dd', attrs={'class':'entry_origin_link'})\n",
    "    meme_origin_place = \"\" if not meme_origin_place else meme_origin_place.text.strip()\n",
    "    \n",
    "    # тэги, связанные с мемом\n",
    "    meme_tags = properties.find('dl', attrs={'id':'entry_tags'}).find('dd')\n",
    "    meme_tags = \"\" if not meme_tags else meme_tags.text.strip()\n",
    "    \n",
    "    return meme_name, meme_status, meme_type, meme_origin_year, meme_origin_place, meme_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Doge',\n",
       " 'Confirmed',\n",
       " 'Animal',\n",
       " '2013',\n",
       " 'Tumblr',\n",
       " 'animal, dog, shiba inu, shibe, such doge, super shibe, japanese, super, tumblr, much, very, many, comic sans, photoshop meme, such, shiba, shibe doge, doges, dogges, reddit, comic sans ms, tumblr meme, hacked, bitcoin, dogecoin, shitposting, stare, canine')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getProperties(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Свойства мема собрали. Теперь собираем по аналогии его текстовое описание. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def getText(soup):\n",
    "    \"\"\"\n",
    "        Возвращает текстовые описания мема\n",
    "        \n",
    "        soup: объект bs4.BeautifulSoup \n",
    "            представление текущей страницы\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    # достаём все тексты под картинкой\n",
    "    body = soup.find('section', attrs={'class':'bodycopy'})\n",
    "    \n",
    "    # раздел about (если он есть), должен идти первым, берем его без уточнения класса\n",
    "    meme_about = body.find('p')\n",
    "    meme_about = \"\" if not meme_about else meme_about.text\n",
    "    \n",
    "    # раздел origin можно найти после заголовка Origin или History, \n",
    "    # находим заголовок, определяем родителя и ищем следущего ребенка - наш раздел\n",
    "    meme_origin = body.find(text='Origin') or body.find(text='History')\n",
    "    meme_origin = \"\" if not meme_origin else meme_origin.parent.find_next().text\n",
    "    \n",
    "    # весь остальной текст (если он есть) можно положить в одно текстовое поле\n",
    "    if body.text:\n",
    "        other_text = body.text.strip().split('\\n')[4:]\n",
    "        other_text = \" \".join(other_text).strip()\n",
    "    else:\n",
    "        other_text = \"\"\n",
    "        \n",
    "    return meme_about, meme_origin, other_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "О чем мем:\n",
      "Doge is a slang term for \"dog\" that is primarily associated with pictures of Shiba Inus (nicknamed \"Shibe\") and internal monologue captions on Tumblr. These photos may be photoshopped to change the dog's face or captioned with interior monologues in Comic Sans font. Starting in 2017, Ironic Doge formats gained prevalence over the original wholesome version.\n",
      "\n",
      "Происхождение:\n",
      "The use of the misspelled word \"doge\" to refer to a dog dates back to June 24th, 2005, when it was mentioned in an episode of Homestar Runner's puppet show. In the episode titled \"Biz Cas Fri 1\"[2], Homestar calls Strong Bad his \"d-o-g-e\" while trying to distract him from his work.\n",
      "\n",
      "Остальной текст:\n",
      "Identity On February 23rd, 2010, Japanese kindergarten teacher Atsuko Sato posted several photos of her rescue-adopted Shiba Inu dog Kabosu to her personal blog.[38] Among the photos included a peculi...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meme_about, meme_origin, other_text = getText(soup)\n",
    "\n",
    "print(\"О чем мем:\\n{}\\n\\nПроисхождение:\\n{}\\n\\nОстальной текст:\\n{}...\\n\"\\\n",
    "      .format(meme_about, meme_origin, other_text[:200]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, создадим функцию, возвращающую всю информацию по текущему мему"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def getMemeData(meme_page):\n",
    "    \"\"\"\n",
    "        Запрашивает данные по странице, возвращает обработанный словарь с данными\n",
    "        \n",
    "        meme_page: string\n",
    "            ссылка на страницу с мемом\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # запрашиваем данные по ссылке\n",
    "    response = requests.get(meme_page, headers={'User-Agent': UserAgent().chrome})\n",
    "    \n",
    "    if not response.ok:\n",
    "        # если сервер нам отказал, вернем статус ошибки \n",
    "        return response.status_code\n",
    "    \n",
    "    # получаем содержимое страницы\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "\n",
    "    # используя ранее написанные функции парсим информацию\n",
    "    views = getStats(soup=soup, stats='views')\n",
    "    videos = getStats(soup=soup, stats='videos')\n",
    "    photos = getStats(soup=soup, stats='photos')\n",
    "    comments = getStats(soup=soup, stats='comments')\n",
    "\n",
    "    # дата\n",
    "    date = soup.find('abbr', attrs={'class':'timeago'}).attrs['title']\n",
    "\n",
    "    # имя, статус, и т.д.\n",
    "    meme_name, meme_status, meme_type, meme_origin_year, meme_origin_place, meme_tags =\\\n",
    "    getProperties(soup=soup)\n",
    "\n",
    "    # текстовые поля\n",
    "    meme_about, meme_origin, other_text = getText(soup=soup)\n",
    "\n",
    "    # составляем словарь, в котором будут хранится все полученные и обработанные данные\n",
    "    data_row = {\"name\":meme_name, \"status\":meme_status, \n",
    "                \"type\":meme_type, \"origin_year\":meme_origin_year, \n",
    "                \"origin_place\":meme_origin_place,\n",
    "                \"date_added\":date, \"views\":views, \n",
    "                \"videos\":videos, \"photos\":photos, \"comments\":comments, \"tags\":meme_tags,\n",
    "                \"about\":meme_about, \"origin\":meme_origin, \"other_text\":other_text}\n",
    "\n",
    "    return data_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_row = getMemeData('http://knowyourmeme.com/memes/doge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь подготовим табличку, чтобы в неё записывать все данные, добавим в неё первую полученную строку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(columns=['name', 'status', 'type', 'origin_year', 'origin_place',\n",
    "                                 'date_added', 'views', 'videos', 'photos', 'comments', \n",
    "                                 'tags', 'about', 'origin', 'other_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.append(data_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>status</th>\n",
       "      <th>type</th>\n",
       "      <th>origin_year</th>\n",
       "      <th>origin_place</th>\n",
       "      <th>date_added</th>\n",
       "      <th>views</th>\n",
       "      <th>videos</th>\n",
       "      <th>photos</th>\n",
       "      <th>comments</th>\n",
       "      <th>tags</th>\n",
       "      <th>about</th>\n",
       "      <th>origin</th>\n",
       "      <th>other_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Doge</td>\n",
       "      <td>Confirmed</td>\n",
       "      <td>Animal</td>\n",
       "      <td>2013</td>\n",
       "      <td>Tumblr</td>\n",
       "      <td>2020-02-03T05:33:03-05:00</td>\n",
       "      <td>13176671</td>\n",
       "      <td>64</td>\n",
       "      <td>1698</td>\n",
       "      <td>918</td>\n",
       "      <td>animal, dog, shiba inu, shibe, such doge, supe...</td>\n",
       "      <td>Doge is a slang term for \"dog\" that is primari...</td>\n",
       "      <td>The use of the misspelled word \"doge\" to refer...</td>\n",
       "      <td>Identity On February 23rd, 2010, Japanese kind...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name     status    type origin_year origin_place  \\\n",
       "0  Doge  Confirmed  Animal        2013       Tumblr   \n",
       "\n",
       "                  date_added     views videos photos comments  \\\n",
       "0  2020-02-03T05:33:03-05:00  13176671     64   1698      918   \n",
       "\n",
       "                                                tags  \\\n",
       "0  animal, dog, shiba inu, shibe, such doge, supe...   \n",
       "\n",
       "                                               about  \\\n",
       "0  Doge is a slang term for \"dog\" that is primari...   \n",
       "\n",
       "                                              origin  \\\n",
       "0  The use of the misspelled word \"doge\" to refer...   \n",
       "\n",
       "                                          other_text  \n",
       "0  Identity On February 23rd, 2010, Japanese kind...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё раз убедимся что всё работает."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a911dd40b700412594368ea024cb5598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for meme_link in tqdm_notebook(meme_links):\n",
    "    data_row = getMemeData(meme_link)\n",
    "    final_df = final_df.append(data_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>status</th>\n",
       "      <th>type</th>\n",
       "      <th>origin_year</th>\n",
       "      <th>origin_place</th>\n",
       "      <th>date_added</th>\n",
       "      <th>views</th>\n",
       "      <th>videos</th>\n",
       "      <th>photos</th>\n",
       "      <th>comments</th>\n",
       "      <th>tags</th>\n",
       "      <th>about</th>\n",
       "      <th>origin</th>\n",
       "      <th>other_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Doge</td>\n",
       "      <td>Confirmed</td>\n",
       "      <td>Animal</td>\n",
       "      <td>2013</td>\n",
       "      <td>Tumblr</td>\n",
       "      <td>2020-02-03T05:33:03-05:00</td>\n",
       "      <td>13176671</td>\n",
       "      <td>64</td>\n",
       "      <td>1698</td>\n",
       "      <td>918</td>\n",
       "      <td>animal, dog, shiba inu, shibe, such doge, supe...</td>\n",
       "      <td>Doge is a slang term for \"dog\" that is primari...</td>\n",
       "      <td>The use of the misspelled word \"doge\" to refer...</td>\n",
       "      <td>Identity On February 23rd, 2010, Japanese kind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why Must You Hurt Me in This Way</td>\n",
       "      <td>Submission</td>\n",
       "      <td>Image Macro</td>\n",
       "      <td>2006</td>\n",
       "      <td>YouTube</td>\n",
       "      <td>2020-02-27T16:00:53-05:00</td>\n",
       "      <td>1178</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>puppet pals, potter puppet pals, wizard angst,...</td>\n",
       "      <td>\"Why Must You Hurt Me in This Way\" is a memora...</td>\n",
       "      <td>On November 2nd, 2006, YouTuber Neil Cicierega...</td>\n",
       "      <td>Spread On June 5th, 2014, Tumblr [1] user ohha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Taj Mahal Comparison With Slums</td>\n",
       "      <td>Submission</td>\n",
       "      <td>Exploitable</td>\n",
       "      <td>2019</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>2020-02-28T17:30:53-05:00</td>\n",
       "      <td>374</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>taj mahal, object labeling, social gap, jacob ...</td>\n",
       "      <td>Taj Mahal Comparison With Slums is an image ma...</td>\n",
       "      <td>On March, 21st 2019, Instagram [1] user @jacob...</td>\n",
       "      <td>Spread On August, 28th 2019, Reddit [2] user a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lore Olympus</td>\n",
       "      <td>Submission</td>\n",
       "      <td>Web Series</td>\n",
       "      <td>2018</td>\n",
       "      <td>Rachel Smythe</td>\n",
       "      <td>2020-02-27T15:40:19-05:00</td>\n",
       "      <td>1514</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>rachel smythe, jim henson, fantasy, greek myth...</td>\n",
       "      <td>Lore Olympus is a web series created by Rachel...</td>\n",
       "      <td>On March 4th, 2018, Rachel Smythe launched Lor...</td>\n",
       "      <td>“Witness what the gods do…after dark. The frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Roscoe The Cat Sees Himself</td>\n",
       "      <td>Submission</td>\n",
       "      <td></td>\n",
       "      <td>2019</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>2020-02-27T15:22:20-05:00</td>\n",
       "      <td>561</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>mirror, object label, tweet, kitty, cute, magn...</td>\n",
       "      <td>Roscoe The Cat Sees Himself is an image macro ...</td>\n",
       "      <td>On August 17th, 2019, Instagram user JustBeing...</td>\n",
       "      <td>Spread On February 17th, 2020, Twitter user @p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               name      status         type origin_year  \\\n",
       "0                              Doge   Confirmed       Animal        2013   \n",
       "1  Why Must You Hurt Me in This Way  Submission  Image Macro        2006   \n",
       "2   Taj Mahal Comparison With Slums  Submission  Exploitable        2019   \n",
       "3                      Lore Olympus  Submission   Web Series        2018   \n",
       "4       Roscoe The Cat Sees Himself  Submission                     2019   \n",
       "\n",
       "    origin_place                 date_added     views videos photos comments  \\\n",
       "0         Tumblr  2020-02-03T05:33:03-05:00  13176671     64   1698      918   \n",
       "1        YouTube  2020-02-27T16:00:53-05:00      1178      2     11        4   \n",
       "2      Instagram  2020-02-28T17:30:53-05:00       374      0     12        0   \n",
       "3  Rachel Smythe  2020-02-27T15:40:19-05:00      1514      0      3        7   \n",
       "4        Twitter  2020-02-27T15:22:20-05:00       561      0     10        0   \n",
       "\n",
       "                                                tags  \\\n",
       "0  animal, dog, shiba inu, shibe, such doge, supe...   \n",
       "1  puppet pals, potter puppet pals, wizard angst,...   \n",
       "2  taj mahal, object labeling, social gap, jacob ...   \n",
       "3  rachel smythe, jim henson, fantasy, greek myth...   \n",
       "4  mirror, object label, tweet, kitty, cute, magn...   \n",
       "\n",
       "                                               about  \\\n",
       "0  Doge is a slang term for \"dog\" that is primari...   \n",
       "1  \"Why Must You Hurt Me in This Way\" is a memora...   \n",
       "2  Taj Mahal Comparison With Slums is an image ma...   \n",
       "3  Lore Olympus is a web series created by Rachel...   \n",
       "4  Roscoe The Cat Sees Himself is an image macro ...   \n",
       "\n",
       "                                              origin  \\\n",
       "0  The use of the misspelled word \"doge\" to refer...   \n",
       "1  On November 2nd, 2006, YouTuber Neil Cicierega...   \n",
       "2  On March, 21st 2019, Instagram [1] user @jacob...   \n",
       "3  On March 4th, 2018, Rachel Smythe launched Lor...   \n",
       "4  On August 17th, 2019, Instagram user JustBeing...   \n",
       "\n",
       "                                          other_text  \n",
       "0  Identity On February 23rd, 2010, Japanese kind...  \n",
       "1  Spread On June 5th, 2014, Tumblr [1] user ohha...  \n",
       "2  Spread On August, 28th 2019, Reddit [2] user a...  \n",
       "3  “Witness what the gods do…after dark. The frie...  \n",
       "4  Spread On February 17th, 2020, Twitter user @p...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 14)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Итоговый цикл\n",
    "\n",
    "Осталось написать итоговый цикл. На всякий случай обернём его в `try-except`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "final_df = pd.DataFrame(columns=['name', 'status', 'type', 'origin_year', 'origin_place',\n",
    "                                 'date_added', 'views', 'videos', 'photos', 'comments', \n",
    "                                 'tags', 'about', 'origin', 'other_text'])\n",
    "\n",
    "for page_number in tqdm_notebook(range(1075), desc='Pages'):\n",
    "    \n",
    "    # собрали ссылки с текущей страницы\n",
    "    meme_links = getPageLinks(page_number)  \n",
    "    \n",
    "    for meme_link in tqdm_notebook(meme_links, desc='Memes', leave=False):\n",
    "        \n",
    "        # иногда с первого раза страничка не прогружается\n",
    "        for i in range(5):\n",
    "            try:\n",
    "                # пытаемся собрать данные\n",
    "                data_row = getMemeData(meme_link)           \n",
    "                final_df = final_df.append(data_row, ignore_index=True)  \n",
    "                # если всё получилось - выходим из внутреннего цикла\n",
    "                break\n",
    "            except:\n",
    "                # Иначе, пробуем еще несколько раз, пока не закончатся попытки\n",
    "                print('AHTUNG! parsing once again:', meme_link)\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Что делать, если сервер разозлился? \n",
    "\n",
    "\n",
    "* Вы решили собрать себе немного данных \n",
    "* Сервер не в восторге от ковровой бомбардировки автоматическими запросами \n",
    "* Error 403, 404, 504, $\\ldots$ \n",
    "* Капча, требования зарегистрироваться\n",
    "* Заботливые сообщения, что с вашего устройства обнаружен подозрительный трафик\n",
    "\n",
    "<center>\n",
    "<img src=\"pictures/doge.jpg\" width=\"450\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### а) быть терпеливым \n",
    "\n",
    "* Слишком частые запросы раздражают сервер\n",
    "* Ставьте между ними временные задержки \n",
    "* Сервер любит временные задержки, так как боится сломаться от перегрузок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(3) # и пусть весь мир подождёт 3 секунды"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### б) общаться через посредников\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/hse-econ-data-science/eds_spring_2020/master/sem05_parsing/image/proxy.jpeg\" width=\"400\"> "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "proxies = {\n",
    "    'http': '182.53.206.47:47592',\n",
    "    'https': '182.53.206.47:47592'\n",
    "}\n",
    "\n",
    "r = requests.get('https://httpbin.org/ip', proxies=proxies)\n",
    "\n",
    "print(r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запрос работал немного подольше, ip адрес сменился. Большая часть прокси-серверов, которые вы найдёте работают плохо. Иногда запрос идёт очень долго и выгоднее сбросить его. Это можно настроить опцией `timeout`.  Например, так если сервер не будет отвечать секунду, код перестанет работать. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "requests.get('http://www.google.com', timeout=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## в) уходить глубже \n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/hse-econ-data-science/eds_spring_2020/master/sem05_parsing/image/tor.jpg\" width=\"600\"> \n",
    "\n",
    "Можно попытаться обходить злые сервера через тор. Есть несколько способов, но мы про это говорить не будем. Лучше подробно почитать [в нашей статье на Хабре.](https://habr.com/ru/company/ods/blog/346632/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Совместить всё? \n",
    "\n",
    "1. Начните с простых приемов, например с `time.sleep`\n",
    "2. Пробуйте новые приёмы постепенно\n",
    "3. Каждый новый приём замедляет скорость работы\n",
    "4. [Разные продвинутые способы работы с библиотекой requests](http://docs.python-requests.org/en/v0.10.6/user/advanced/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Напоследок, хотелось бы сказать пару слов о парсинге вообще и при помощи Тора в частности. Собирать себе данные самостоятельно - это стильно, модно и в принципе интересно, можно получить наборы, которых еще никто никогда не обрабатывал, сделать что-то новое, посмотреть, наконец, на все мемы мира сразу. Однако не стоит забывать, что ограничения, введенные сервером, в том числе баны, появились не просто так, а в целях защиты сайта от DDoS-атак. К чужому труду стоит относится с уважением, и даже если у сервера никакой защиты нет, - это еще не повод неограниченно забрасывать его своими запросами, особенно если это может привести к его отключению - [уголовное наказание](http://sd-company.su/article/security/ddosataka-ugolovnaya-otvetstvennost) никто не отменял. Успешных и безопасных вам исследований!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Материалы\n",
    "\n",
    "* [Парсим мемы в python](https://habr.com/ru/company/ods/blog/346632/) - подробная статья на Хабре\n",
    "* [Продвинутое использование requests](https://2.python-requests.org/en/master/user/advanced/)\n",
    "* [Репозиторий](https://github.com/DmitrySerg/memology) с исследованием мемов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pictures/take_all.png\" height=\"200\" width=\"900\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
